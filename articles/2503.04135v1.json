{
  "id": "2503.04135v1",
  "title": "Biological Sequence with Language Model Prompting: A Survey",
  "url": "http://arxiv.org/abs/2503.04135v1",
  "pdf_url": "http://arxiv.org/pdf/2503.04135v1",
  "authors": [
    "Jiyue Jiang",
    "Zikang Wang",
    "Yuheng Shan",
    "Heyan Chai",
    "Jiayi Li",
    "Zixian Ma",
    "Xinrui Zhang",
    "Yu Li"
  ],
  "date": "2025-03-06",
  "summary": "Large Language models (LLMs) have emerged as powerful tools for addressing\nchallenges across diverse domains. Notably, recent studies have demonstrated\nthat large language models significantly enhance the efficiency of biomolecular\nanalysis and synthesis, attracting widespread attention from academics and\nmedicine. In this paper, we systematically investigate the application of\nprompt-based methods with LLMs to biological sequences, including DNA, RNA,\nproteins, and drug discovery tasks. Specifically, we focus on how prompt\nengineering enables LLMs to tackle domain-specific problems, such as promoter\nsequence prediction, protein structure modeling, and drug-target binding\naffinity prediction, often with limited labeled data. Furthermore, our\ndiscussion highlights the transformative potential of prompting in\nbioinformatics while addressing key challenges such as data scarcity,\nmultimodal fusion, and computational resource limitations. Our aim is for this\npaper to function both as a foundational primer for newcomers and a catalyst\nfor continued innovation within this dynamic field of study.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal"
  ],
  "attention_score": 2.32,
  "attention_components": {
    "base_score": 1.2,
    "recency_factor": 0.989041095890411,
    "source_weight": 1.0,
    "age_months": 0.1,
    "citation_velocity": 0
  }
}