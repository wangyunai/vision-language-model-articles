{
  "id": "2503_04724v1",
  "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
  "url": "http://arxiv.org/abs/2503.04724v1",
  "pdf_url": "http://arxiv.org/pdf/2503.04724v1",
  "authors": [
    "Sambal Shikhar",
    "Mohammed Irfan Kurpath",
    "Sahal Shaji Mullappilly",
    "Jean Lahoud",
    "Fahad Khan",
    "Rao Muhammad Anwer",
    "Salman Khan",
    "Hisham Cholakkal"
  ],
  "date": "2025-03-06",
  "summary": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal",
    "vision-language"
  ]
}