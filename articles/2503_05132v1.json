{
  "id": "2503_05132v1",
  "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
  "url": "http://arxiv.org/abs/2503.05132v1",
  "pdf_url": "http://arxiv.org/pdf/2503.05132v1",
  "authors": [
    "Hengguang Zhou",
    "Xirui Li",
    "Ruochen Wang",
    "Minhao Cheng",
    "Tianyi Zhou",
    "Cho-Jui Hsieh"
  ],
  "date": "2025-03-24",
  "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal",
    "visual reasoning"
  ]
}