{
  "id": "2503_03689v1",
  "title": "DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with\n  Reward Guidance",
  "url": "http://arxiv.org/abs/2503.03689v1",
  "pdf_url": "http://arxiv.org/pdf/2503.03689v1",
  "authors": [
    "Zhao Yang",
    "Zezhong Qian",
    "Xiaofan Li",
    "Weixiang Xu",
    "Gongpeng Zhao",
    "Ruohong Yu",
    "Lingsi Zhu",
    "Longjun Liu"
  ],
  "date": "2025-03-05",
  "summary": "Accurate and high-fidelity driving scene reconstruction demands the effective\nutilization of comprehensive scene information as conditional inputs. Existing\nmethods predominantly rely on 3D bounding boxes and BEV road maps for\nforeground and background control, which fail to capture the full complexity of\ndriving scenes and adequately integrate multimodal information. In this work,\nwe present DualDiff, a dual-branch conditional diffusion model designed to\nenhance driving scene generation across multiple views and video sequences.\nSpecifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional\ninput, offering rich foreground and background semantics alongside 3D spatial\ngeometry to precisely control the generation of both elements. To improve the\nsynthesis of fine-grained foreground objects, particularly complex and distant\nones, we propose a Foreground-Aware Mask (FGM) denoising loss function.\nAdditionally, we develop the Semantic Fusion Attention (SFA) mechanism to\ndynamically prioritize relevant information and suppress noise, enabling more\neffective multimodal fusion. Finally, to ensure high-quality image-to-video\ngeneration, we introduce the Reward-Guided Diffusion (RGD) framework, which\nmaintains global consistency and semantic coherence in generated videos.\nExtensive experiments demonstrate that DualDiff achieves state-of-the-art\n(SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff\nreduces the FID score by 4.09% compared to the best baseline. In downstream\ntasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and\nroad mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP\nincreases by 1.46%. Code will be made available at\nhttps://github.com/yangzhaojason/DualDiff.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal"
  ]
}