{
  "id": "2503_04504v1",
  "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
  "url": "http://arxiv.org/abs/2503.04504v1",
  "pdf_url": "http://arxiv.org/pdf/2503.04504v1",
  "authors": [
    "Sunghyun Ahn",
    "Youngwan Jo",
    "Kijung Lee",
    "Sein Kwon",
    "Inpyo Hong",
    "Sanghyun Park"
  ],
  "date": "2024-03-06",
  "summary": "Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "VLM",
    "visual question answering"
  ]
}