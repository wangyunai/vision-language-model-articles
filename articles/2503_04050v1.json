{
  "id": "2503_04050v1",
  "title": "Underlying Semantic Diffusion for Effective and Efficient In-Context\n  Learning",
  "url": "http://arxiv.org/abs/2503.04050v1",
  "pdf_url": "http://arxiv.org/pdf/2503.04050v1",
  "authors": [
    "Zhong Ji",
    "Weilong Cao",
    "Yan Zhang",
    "Yanwei Pang",
    "Jungong Han",
    "Xuelong Li"
  ],
  "date": "2025-03-06",
  "summary": "Diffusion models has emerged as a powerful framework for tasks like image\ncontrollable generation and dense prediction. However, existing models often\nstruggle to capture underlying semantics (e.g., edges, textures, shapes) and\neffectively utilize in-context learning, limiting their contextual\nunderstanding and image generation quality. Additionally, high computational\ncosts and slow inference speeds hinder their real-time applicability. To\naddress these challenges, we propose Underlying Semantic Diffusion\n(US-Diffusion), an enhanced diffusion model that boosts underlying semantics\nlearning, computational efficiency, and in-context learning capabilities on\nmulti-task scenarios. We introduce Separate & Gather Adapter (SGA), which\ndecouples input conditions for different tasks while sharing the architecture,\nenabling better in-context learning and generalization across diverse visual\ndomains. We also present a Feedback-Aided Learning (FAL) framework, which\nleverages feedback signals to guide the model in capturing semantic details and\ndynamically adapting to task-specific contextual cues. Furthermore, we propose\na plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time\nsteps with high-noise levels, which aims at optimizing training and inference\nefficiency while maintaining strong in-context learning performance.\nExperimental results demonstrate that US-Diffusion outperforms the\nstate-of-the-art method, achieving an average reduction of 7.47 in FID on\nMap2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks,\nwhile achieving approximately 9.45 times faster inference speed. Our method\nalso demonstrates superior training efficiency and in-context learning\ncapabilities, excelling in new datasets and tasks, highlighting its robustness\nand adaptability across diverse visual domains.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "image generation"
  ]
}