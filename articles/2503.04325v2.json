{
  "id": "2503.04325v2",
  "title": "GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain\n  tumour Segmentation on mp-MRI",
  "url": "http://arxiv.org/abs/2503.04325v2",
  "pdf_url": "http://arxiv.org/pdf/2503.04325v2",
  "authors": [
    "Cecilia Diana-Albelda",
    "Roberto Alcover-Couso",
    "\u00c1lvaro Garc\u00eda-Mart\u00edn",
    "Jesus Bescos",
    "Marcos Escudero-Vi\u00f1olo"
  ],
  "date": "2025-03-06",
  "summary": "Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "vision transformer",
    "ViT",
    "Segment Anything"
  ],
  "attention_score": 3.09,
  "attention_components": {
    "base_score": 1.6,
    "recency_factor": 0.989041095890411,
    "source_weight": 1.0,
    "age_months": 0.1,
    "citation_velocity": 0
  }
}