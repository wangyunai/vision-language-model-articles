[
  {
    "id": "2503_04724v1",
    "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
    "url": "http://arxiv.org/abs/2503.04724v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04724v1",
    "authors": [
      "Sambal Shikhar",
      "Mohammed Irfan Kurpath",
      "Sahal Shaji Mullappilly",
      "Jean Lahoud",
      "Fahad Khan",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Hisham Cholakkal"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_04643v1",
    "title": "Adaptive Prototype Learning for Multimodal Cancer Survival Analysis",
    "url": "http://arxiv.org/abs/2503.04643v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04643v1",
    "authors": [
      "Hong Liu",
      "Haosen Yang",
      "Federica Eduati",
      "Josien P. W. Pluim",
      "Mitko Veta"
    ],
    "date": "2025-03-06",
    "summary": "Leveraging multimodal data, particularly the integration of whole-slide\nhistology images (WSIs) and transcriptomic profiles, holds great promise for\nimproving cancer survival prediction. However, excessive redundancy in\nmultimodal data can degrade model performance. In this paper, we propose\nAdaptive Prototype Learning (APL), a novel and effective approach for\nmultimodal cancer survival analysis. APL adaptively learns representative\nprototypes in a data-driven manner, reducing redundancy while preserving\ncritical information. Our method employs two sets of learnable query vectors\nthat serve as a bridge between high-dimensional representations and survival\nprediction, capturing task-relevant features. Additionally, we introduce a\nmultimodal mixed self-attention mechanism to enable cross-modal interactions,\nfurther enhancing information fusion. Extensive experiments on five benchmark\ncancer datasets demonstrate the superiority of our approach over existing\nmethods. The code is available at https://github.com/HongLiuuuuu/APL.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04641v1",
    "title": "Simulating the Real World: A Unified Survey of Multimodal Generative\n  Models",
    "url": "http://arxiv.org/abs/2503.04641v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04641v1",
    "authors": [
      "Yuqi Hu",
      "Longguang Wang",
      "Xian Liu",
      "Ling-Hao Chen",
      "Yuwei Guo",
      "Yukai Shi",
      "Ce Liu",
      "Anyi Rao",
      "Zeyu Wang",
      "Hui Xiong"
    ],
    "date": "2025-03-06",
    "summary": "Understanding and replicating the real world is a critical challenge in\nArtificial General Intelligence (AGI) research. To achieve this, many existing\napproaches, such as world models, aim to capture the fundamental principles\ngoverning the physical world, enabling more accurate simulations and meaningful\ninteractions. However, current methods often treat different modalities,\nincluding 2D (images), videos, 3D, and 4D representations, as independent\ndomains, overlooking their interdependencies. Additionally, these methods\ntypically focus on isolated dimensions of reality without systematically\nintegrating their connections. In this survey, we present a unified survey for\nmultimodal generative models that investigate the progression of data\ndimensionality in real-world simulation. Specifically, this survey starts from\n2D generation (appearance), then moves to video (appearance+dynamics) and 3D\ngeneration (appearance+geometry), and finally culminates in 4D generation that\nintegrate all dimensions. To the best of our knowledge, this is the first\nattempt to systematically unify the study of 2D, video, 3D and 4D generation\nwithin a single framework. To guide future research, we provide a comprehensive\nreview of datasets, evaluation metrics and future directions, and fostering\ninsights for newcomers. This survey serves as a bridge to advance the study of\nmultimodal generative models and real-world simulation within a unified\nframework.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04639v1",
    "title": "Enhancing SAM with Efficient Prompting and Preference Optimization for\n  Semi-supervised Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2503.04639v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04639v1",
    "authors": [
      "Aishik Konwer",
      "Zhijian Yang",
      "Erhan Bas",
      "Cao Xiao",
      "Prateek Prasanna",
      "Parminder Bhatia",
      "Taha Kass-Hout"
    ],
    "date": "2025-03-06",
    "summary": "Foundational models such as the Segment Anything Model (SAM) are gaining\ntraction in medical imaging segmentation, supporting multiple downstream tasks.\nHowever, such models are supervised in nature, still relying on large annotated\ndatasets or prompts supplied by experts. Conventional techniques such as active\nlearning to alleviate such limitations are limited in scope and still\nnecessitate continuous human involvement and complex domain knowledge for label\nrefinement or establishing reward ground truth. To address these challenges, we\npropose an enhanced Segment Anything Model (SAM) framework that utilizes\nannotation-efficient prompts generated in a fully unsupervised fashion, while\nstill capturing essential semantic, location, and shape information through\ncontrastive language-image pretraining and visual question answering. We adopt\nthe direct preference optimization technique to design an optimal policy that\nenables the model to generate high-fidelity segmentations with simple ratings\nor rankings provided by a virtual annotator simulating the human annotation\nprocess. State-of-the-art performance of our framework in tasks such as lung\nsegmentation, breast tumor segmentation, and organ segmentation across various\nmodalities, including X-ray, ultrasound, and abdominal CT, justifies its\neffectiveness in low-annotation data scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual question answering"
    ]
  },
  {
    "id": "2503_04592v1",
    "title": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning",
    "url": "http://arxiv.org/abs/2503.04592v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04592v1",
    "authors": [
      "Qing Zhou",
      "Tao Yang",
      "Junyu Gao",
      "Weiping Ni",
      "Junzheng Wu",
      "Qi Wang"
    ],
    "date": "2025-03-06",
    "summary": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image captioning"
    ]
  },
  {
    "id": "2503_04545v1",
    "title": "ViT-VS: On the Applicability of Pretrained Vision Transformer Features\n  for Generalizable Visual Servoing",
    "url": "http://arxiv.org/abs/2503.04545v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04545v1",
    "authors": [
      "Alessandro Scherl",
      "Stefan Thalhammer",
      "Bernhard Neuberger",
      "Wilfried W\u00f6ber",
      "Jos\u00e9 Grac\u00eda-Rodr\u00edguez"
    ],
    "date": "2025-03-06",
    "summary": "Visual servoing enables robots to precisely position their end-effector\nrelative to a target object. While classical methods rely on hand-crafted\nfeatures and thus are universally applicable without task-specific training,\nthey often struggle with occlusions and environmental variations, whereas\nlearning-based approaches improve robustness but typically require extensive\ntraining. We present a visual servoing approach that leverages pretrained\nvision transformers for semantic feature extraction, combining the advantages\nof both paradigms while also being able to generalize beyond the provided\nsample. Our approach achieves full convergence in unperturbed scenarios and\nsurpasses classical image-based visual servoing by up to 31.2\\% relative\nimprovement in perturbed scenarios. Even the convergence rates of\nlearning-based methods are matched despite requiring no task- or\nobject-specific training. Real-world evaluations confirm robust performance in\nend-effector positioning, industrial box manipulation, and grasping of unseen\nobjects using only a reference from the same category. Our code and simulation\nenvironment are available at: https://alessandroscherl.github.io/ViT-VS/",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_04543v1",
    "title": "Keeping Yourself is Important in Downstream Tuning Multimodal Large\n  Language Model",
    "url": "http://arxiv.org/abs/2503.04543v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04543v1",
    "authors": [
      "Wenke Huang",
      "Jian Liang",
      "Xianda Guo",
      "Yiyang Fang",
      "Guancheng Wan",
      "Xuankun Rong",
      "Chi Wen",
      "Zekun Shi",
      "Qingyun Li",
      "Didi Zhu",
      "Yanbiao Ma",
      "Ke Liang",
      "Bin Yang",
      "He Li",
      "Jiawei Shao",
      "Mang Ye",
      "Bo Du"
    ],
    "date": "2025-03-06",
    "summary": "Multi-modal Large Language Models (MLLMs) integrate visual and linguistic\nreasoning to address complex tasks such as image captioning and visual question\nanswering. While MLLMs demonstrate remarkable versatility, MLLMs appears\nlimited performance on special applications. But tuning MLLMs for downstream\ntasks encounters two key challenges: Task-Expert Specialization, where\ndistribution shifts between pre-training and target datasets constrain target\nperformance, and Open-World Stabilization, where catastrophic forgetting erases\nthe model general knowledge. In this work, we systematically review recent\nadvancements in MLLM tuning methodologies, classifying them into three\nparadigms: (I) Selective Tuning, (II) Additive Tuning, and (III)\nReparameterization Tuning. Furthermore, we benchmark these tuning strategies\nacross popular MLLM architectures and diverse downstream tasks to establish\nstandardized evaluation analysis and systematic tuning principles. Finally, we\nhighlight several open challenges in this domain and propose future research\ndirections. To facilitate ongoing progress in this rapidly evolving field, we\nprovide a public repository that continuously tracks developments:\nhttps://github.com/WenkeHuang/Awesome-MLLM-Tuning.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image captioning"
    ]
  },
  {
    "id": "2503_04528v1",
    "title": "Federated Dynamic Modeling and Learning for Spatiotemporal Data\n  Forecasting",
    "url": "http://arxiv.org/abs/2503.04528v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04528v1",
    "authors": [
      "Thien Pham",
      "Angelo Furno",
      "Fa\u00efcel Chamroukhi",
      "Latifa Oukhellou"
    ],
    "date": "2025-03-06",
    "summary": "This paper presents an advanced Federated Learning (FL) framework for\nforecasting complex spatiotemporal data, improving upon recent state-of-the-art\nmodels. In the proposed approach, the original Gated Recurrent Unit (GRU)\nmodule within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent\nNetwork (DSTGCRN) modeling is first replaced with a Long Short-Term Memory\n(LSTM) network, enabling the resulting model to more effectively capture\nlong-term dependencies inherent to time series data. The resulting architecture\nsignificantly improves the model's capacity to handle complex temporal patterns\nin diverse forecasting applications. Furthermore, the proposed FL framework\nintegrates a novel Client-Side Validation (CSV) mechanism, introducing a\ncritical validation step at the client level before incorporating aggregated\nparameters from the central server into local models. This ensures that only\nthe most effective updates are adopted, improving both the robustness and\naccuracy of the forecasting model across clients. The efficiency of our\napproach is demonstrated through extensive experiments on real-world\napplications, including public datasets for multimodal transport demand\nforecasting and private datasets for Origin-Destination (OD) matrix forecasting\nin urban areas. The results demonstrate substantial improvements over\nconventional methods, highlighting the framework's ability to capture complex\nspatiotemporal dependencies while preserving data privacy. This work not only\nprovides a scalable and privacy-preserving solution for real-time,\nregion-specific forecasting and management but also underscores the potential\nof leveraging distributed data sources in a FL context. We provide our\nalgorithms as open-source on GitHub.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04506v1",
    "title": "Multi-modal Summarization in Model-Based Engineering: Automotive\n  Software Development Case Study",
    "url": "http://arxiv.org/abs/2503.04506v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04506v1",
    "authors": [
      "Nenad Petrovic",
      "Yurui Zhang",
      "Moaad Maaroufi",
      "Kuo-Yi Chao",
      "Lukasz Mazur",
      "Fengjunjie Pan",
      "Vahid Zolfaghari",
      "Alois Knoll"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal summarization integrating information from diverse data modalities\npresents a promising solution to aid the understanding of information within\nvarious processes. However, the application and advantages of multimodal\nsummarization have not received much attention in model-based engineering\n(MBE), where it has become a cornerstone in the design and development of\ncomplex systems, leveraging formal models to improve understanding, validation\nand automation throughout the engineering lifecycle. UML and EMF diagrams in\nmodel-based engineering contain a large amount of multimodal information and\nintricate relational data. Hence, our study explores the application of\nmultimodal large language models within the domain of model-based engineering\nto evaluate their capacity for understanding and identifying relationships,\nfeatures, and functionalities embedded in UML and EMF diagrams. We aim to\ndemonstrate the transformative potential benefits and limitations of multimodal\nsummarization in improving productivity and accuracy in MBE practices. The\nproposed approach is evaluated within the context of automotive software\ndevelopment, while many promising state-of-art models were taken into account.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04504v1",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "url": "http://arxiv.org/abs/2503.04504v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04504v1",
    "authors": [
      "Sunghyun Ahn",
      "Youngwan Jo",
      "Kijung Lee",
      "Sein Kwon",
      "Inpyo Hong",
      "Sanghyun Park"
    ],
    "date": "2025-03-06",
    "summary": "Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "visual question answering"
    ]
  },
  {
    "id": "2503_04490v1",
    "title": "Large Language Models in Bioinformatics: A Survey",
    "url": "http://arxiv.org/abs/2503.04490v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04490v1",
    "authors": [
      "Zhenyu Wang",
      "Zikang Wang",
      "Jiyue Jiang",
      "Pengan Chen",
      "Xiangyu Shi",
      "Yu Li"
    ],
    "date": "2025-03-06",
    "summary": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04478v1",
    "title": "Semantic Alignment of Unimodal Medical Text and Vision Representations",
    "url": "http://arxiv.org/abs/2503.04478v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04478v1",
    "authors": [
      "Maxime Di Folco",
      "Emily Chan",
      "Marta Hasny",
      "Cosmin I. Bercea",
      "Julia A. Schnabel"
    ],
    "date": "2025-03-06",
    "summary": "General-purpose AI models, particularly those designed for text and vision,\ndemonstrate impressive versatility across a wide range of deep-learning tasks.\nHowever, they often underperform in specialised domains like medical imaging,\nwhere domain-specific solutions or alternative knowledge transfer approaches\nare typically required. Recent studies have noted that general-purpose models\ncan exhibit similar latent spaces when processing semantically related data,\nalthough this alignment does not occur naturally. Building on this insight, it\nhas been shown that applying a simple transformation - at most affine -\nestimated from a subset of semantically corresponding samples, known as\nanchors, enables model stitching across diverse training paradigms,\narchitectures, and modalities. In this paper, we explore how semantic alignment\n- estimating transformations between anchors - can bridge general-purpose AI\nwith specialised medical knowledge. Using multiple public chest X-ray datasets,\nwe demonstrate that model stitching across model architectures allows general\nmodels to integrate domain-specific knowledge without additional training,\nleading to improved performance on medical tasks. Furthermore, we introduce a\nnovel zero-shot classification approach for unimodal vision encoders that\nleverages semantic alignment across modalities. Our results show that our\nmethod not only outperforms general multimodal models but also approaches the\nperformance levels of fully trained, medical-specific multimodal solutions",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04470v1",
    "title": "Gate-Shift-Pose: Enhancing Action Recognition in Sports with Skeleton\n  Information",
    "url": "http://arxiv.org/abs/2503.04470v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04470v1",
    "authors": [
      "Edoardo Bianchi",
      "Oswald Lanz"
    ],
    "date": "2025-03-06",
    "summary": "This paper introduces Gate-Shift-Pose, an enhanced version of Gate-Shift-Fuse\nnetworks, designed for athlete fall classification in figure skating by\nintegrating skeleton pose data alongside RGB frames. We evaluate two fusion\nstrategies: early-fusion, which combines RGB frames with Gaussian heatmaps of\npose keypoints at the input stage, and late-fusion, which employs a\nmulti-stream architecture with attention mechanisms to combine RGB and pose\nfeatures. Experiments on the FR-FS dataset demonstrate that Gate-Shift-Pose\nsignificantly outperforms the RGB-only baseline, improving accuracy by up to\n40% with ResNet18 and 20% with ResNet50. Early-fusion achieves the highest\naccuracy (98.08%) with ResNet50, leveraging the model's capacity for effective\nmultimodal integration, while late-fusion is better suited for lighter\nbackbones like ResNet18. These results highlight the potential of multimodal\narchitectures for sports action recognition and the critical role of skeleton\npose information in capturing complex motion patterns.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04459v2",
    "title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering",
    "url": "http://arxiv.org/abs/2503.04459v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04459v2",
    "authors": [
      "Hongyeob Kim",
      "Inyoung Jung",
      "Dayoon Suh",
      "Youjia Zhang",
      "Sangmin Lee",
      "Sungeun Hong"
    ],
    "date": "2025-03-06",
    "summary": "Audio-Visual Question Answering (AVQA) requires not only question-based\nmultimodal reasoning but also precise temporal grounding to capture subtle\ndynamics for accurate prediction. However, existing methods mainly use question\ninformation implicitly, limiting focus on question-specific details.\nFurthermore, most studies rely on uniform frame sampling, which can miss key\nquestion-relevant frames. Although recent Top-K frame selection methods aim to\naddress this, their discrete nature still overlooks fine-grained temporal\ndetails. This paper proposes QA-TIGER, a novel framework that explicitly\nincorporates question information and models continuous temporal dynamics. Our\nkey idea is to use Gaussian-based modeling to adaptively focus on both\nconsecutive and non-consecutive frames based on the question, while explicitly\ninjecting question information and applying progressive refinement. We leverage\na Mixture of Experts (MoE) to flexibly implement multiple Gaussian models,\nactivating temporal experts specifically tailored to the question. Extensive\nexperiments on multiple AVQA benchmarks show that QA-TIGER consistently\nachieves state-of-the-art performance. Code is available at\nhttps://aim-skku.github.io/QA-TIGER/",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA"
    ]
  },
  {
    "id": "2503_04457v1",
    "title": "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction",
    "url": "http://arxiv.org/abs/2503.04457v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04457v1",
    "authors": [
      "Chao Wang",
      "Weiwei Fu",
      "Yang Zhou"
    ],
    "date": "2025-03-06",
    "summary": "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_04444v1",
    "title": "ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task",
    "url": "http://arxiv.org/abs/2503.04444v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04444v1",
    "authors": [
      "Vittorio Pippi",
      "Matthieu Guillaumin",
      "Silvia Cascianelli",
      "Rita Cucchiara",
      "Maximilian Jaritz",
      "Loris Bazzani"
    ],
    "date": "2025-03-06",
    "summary": "Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04417v1",
    "title": "From Idea to CAD: A Language Model-Driven Multi-Agent System for\n  Collaborative Design",
    "url": "http://arxiv.org/abs/2503.04417v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04417v1",
    "authors": [
      "Felix Ocker",
      "Stefan Menzel",
      "Ahmed Sadik",
      "Thiago Rios"
    ],
    "date": "2025-03-06",
    "summary": "Creating digital models using Computer Aided Design (CAD) is a process that\nrequires in-depth expertise. In industrial product development, this process\ntypically involves entire teams of engineers, spanning requirements\nengineering, CAD itself, and quality assurance. We present an approach that\nmirrors this team structure with a Vision Language Model (VLM)-based Multi\nAgent System, with access to parametric CAD tooling and tool documentation.\nCombining agents for requirements engineering, CAD engineering, and\nvision-based quality assurance, a model is generated automatically from\nsketches and/ or textual descriptions. The resulting model can be refined\ncollaboratively in an iterative validation loop with the user. Our approach has\nthe potential to increase the effectiveness of design processes, both for\nindustry experts and for hobbyists who create models for 3D printing. We\ndemonstrate the potential of the architecture at the example of various design\ntasks and provide several ablations that show the benefits of the\narchitecture's individual components.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_04406v1",
    "title": "Training-Free Graph Filtering via Multimodal Feature Refinement for\n  Extremely Fast Multimodal Recommendation",
    "url": "http://arxiv.org/abs/2503.04406v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04406v1",
    "authors": [
      "Yu-Seung Roh",
      "Joo-Young Kim",
      "Jin-Duk Park",
      "Won-Yong Shin"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal recommender systems improve the performance of canonical\nrecommender systems with no item features by utilizing diverse content types\nsuch as text, images, and videos, while alleviating inherent sparsity of\nuser-item interactions and accelerating user engagement. However, current\nneural network-based models often incur significant computational overhead due\nto the complex training process required to learn and integrate information\nfrom multiple modalities. To overcome this limitation, we propose\nMultiModal-Graph Filtering (MM-GF), a training-free method based on the notion\nof graph filtering (GF) for efficient and accurate multimodal recommendations.\nSpecifically, MM-GF first constructs multiple similarity graphs through\nnontrivial multimodal feature refinement such as robust scaling and vector\nshifting by addressing the heterogeneous characteristics across modalities.\nThen, MM-GF optimally fuses multimodal information using linear low-pass\nfilters across different modalities. Extensive experiments on real-world\nbenchmark datasets demonstrate that MM-GF not only improves recommendation\naccuracy by up to 13.35% compared to the best competitor but also dramatically\nreduces computational costs by achieving the runtime of less than 10 seconds.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04353v1",
    "title": "ObjMST: An Object-Focused Multimodal Style Transfer Framework",
    "url": "http://arxiv.org/abs/2503.04353v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04353v1",
    "authors": [
      "Chanda Grover Kamra",
      "Indra Deep Mastan",
      "Debayan Gupta"
    ],
    "date": "2025-03-06",
    "summary": "We propose ObjMST, an object-focused multimodal style transfer framework that\nprovides separate style supervision for salient objects and surrounding\nelements while addressing alignment issues in multimodal representation\nlearning. Existing image-text multimodal style transfer methods face the\nfollowing challenges: (1) generating non-aligned and inconsistent multimodal\nstyle representations; and (2) content mismatch, where identical style patterns\nare applied to both salient objects and their surrounding elements. Our\napproach mitigates these issues by: (1) introducing a Style-Specific Masked\nDirectional CLIP Loss, which ensures consistent and aligned style\nrepresentations for both salient objects and their surroundings; and (2)\nincorporating a salient-to-key mapping mechanism for stylizing salient objects,\nfollowed by image harmonization to seamlessly blend the stylized objects with\ntheir environment. We validate the effectiveness of ObjMST through experiments,\nusing both quantitative metrics and qualitative visual evaluations of the\nstylized outputs. Our code is available at:\nhttps://github.com/chandagrover/ObjMST.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "CLIP"
    ]
  },
  {
    "id": "2503_04325v2",
    "title": "GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain\n  tumour Segmentation on mp-MRI",
    "url": "http://arxiv.org/abs/2503.04325v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04325v2",
    "authors": [
      "Cecilia Diana-Albelda",
      "Roberto Alcover-Couso",
      "\u00c1lvaro Garc\u00eda-Mart\u00edn",
      "Jesus Bescos",
      "Marcos Escudero-Vi\u00f1olo"
    ],
    "date": "2025-03-06",
    "summary": "Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_04280v2",
    "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models",
    "url": "http://arxiv.org/abs/2503.04280v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04280v2",
    "authors": [
      "Niccol\u00f2 Turcato",
      "Matteo Iovino",
      "Aris Synodinos",
      "Alberto Dalla Libera",
      "Ruggero Carli",
      "Pietro Falco"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ]
  },
  {
    "id": "2503_04252v1",
    "title": "RCRank: Multimodal Ranking of Root Causes of Slow Queries in Cloud\n  Database Systems",
    "url": "http://arxiv.org/abs/2503.04252v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04252v1",
    "authors": [
      "Biao Ouyang",
      "Yingying Zhang",
      "Hanyin Cheng",
      "Yang Shu",
      "Chenjuan Guo",
      "Bin Yang",
      "Qingsong Wen",
      "Lunting Fan",
      "Christian S. Jensen"
    ],
    "date": "2025-03-06",
    "summary": "With the continued migration of storage to cloud database systems,the impact\nof slow queries in such systems on services and user experience is increasing.\nRoot-cause diagnosis plays an indispensable role in facilitating slow-query\ndetection and revision. This paper proposes a method capable of both\nidentifying possible root cause types for slow queries and ranking these\naccording to their potential for accelerating slow queries. This enables\nprioritizing root causes with the highest impact, in turn improving slow-query\nrevision effectiveness. To enable more accurate and detailed diagnoses, we\npropose the multimodal Ranking for the Root Causes of slow queries (RCRank)\nframework, which formulates root cause analysis as a multimodal machine\nlearning problem and leverages multimodal information from query statements,\nexecution plans, execution logs, and key performance indicators. To obtain\nexpressive embeddings from its heterogeneous multimodal input, RCRank\nintegrates self-supervised pre-training that enhances cross-modal alignment and\ntask relevance. Next, the framework integrates root-cause-adaptive cross\nTransformers that enable adaptive fusion of multimodal features with varying\ncharacteristics. Finally, the framework offers a unified model that features an\nimpact-aware training objective for identifying and ranking root causes. We\nreport on experiments on real and synthetic datasets, finding that RCRank is\ncapable of consistently outperforming the state-of-the-art methods at root\ncause identification and ranking according to a range of metrics.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04250v1",
    "title": "An Egocentric Vision-Language Model based Portable Real-time Smart\n  Assistant",
    "url": "http://arxiv.org/abs/2503.04250v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04250v1",
    "authors": [
      "Yifei Huang",
      "Jilan Xu",
      "Baoqi Pei",
      "Yuping He",
      "Guo Chen",
      "Mingfang Zhang",
      "Lijin Yang",
      "Zheng Nie",
      "Jinyao Liu",
      "Guoshun Fan",
      "Dechen Lin",
      "Fang Fang",
      "Kunpeng Li",
      "Chang Yuan",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Yali Wang",
      "Yu Qiao",
      "Limin Wang"
    ],
    "date": "2025-03-06",
    "summary": "We present Vinci, a vision-language system designed to provide real-time,\ncomprehensive AI assistance on portable devices. At its core, Vinci leverages\nEgoVideo-VL, a novel model that integrates an egocentric vision foundation\nmodel with a large language model (LLM), enabling advanced functionalities such\nas scene understanding, temporal grounding, video summarization, and future\nplanning. To enhance its utility, Vinci incorporates a memory module for\nprocessing long video streams in real time while retaining contextual history,\na generation module for producing visual action demonstrations, and a retrieval\nmodule that bridges egocentric and third-person perspectives to provide\nrelevant how-to videos for skill acquisition. Unlike existing systems that\noften depend on specialized hardware, Vinci is hardware-agnostic, supporting\ndeployment across a wide range of devices, including smartphones and wearable\ncameras. In our experiments, we first demonstrate the superior performance of\nEgoVideo-VL on multiple public benchmarks, showcasing its vision-language\nreasoning and contextual understanding capabilities. We then conduct a series\nof user studies to evaluate the real-world effectiveness of Vinci, highlighting\nits adaptability and usability in diverse scenarios. We hope Vinci can\nestablish a new framework for portable, real-time egocentric AI systems,\nempowering users with contextual and actionable insights. Including the\nfrontend, backend, and models, all codes of Vinci are available at\nhttps://github.com/OpenGVLab/vinci.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ]
  },
  {
    "id": "2503_04229v1",
    "title": "Synthetic Data is an Elegant GIFT for Continual Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.04229v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04229v1",
    "authors": [
      "Bin Wu",
      "Wuxuan Shi",
      "Jinqiao Wang",
      "Mang Ye"
    ],
    "date": "2025-03-06",
    "summary": "Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to\nefficiently update their knowledge and adapt to various downstream tasks\nwithout retraining from scratch. However, for VLMs, in addition to the loss of\nknowledge previously learned from downstream tasks, pre-training knowledge is\nalso corrupted during continual fine-tuning. This issue is exacerbated by the\nunavailability of original pre-training data, leaving VLM's generalization\nability degrading. In this paper, we propose GIFT, a novel continual\nfine-tuning approach that utilizes synthetic data to overcome catastrophic\nforgetting in VLMs. Taking advantage of recent advances in text-to-image\nsynthesis, we employ a pre-trained diffusion model to recreate both\npre-training and learned downstream task data. In this way, the VLM can revisit\nprevious knowledge through distillation on matching diffusion-generated images\nand corresponding text prompts. Leveraging the broad distribution and high\nalignment between synthetic image-text pairs in VLM's feature space, we propose\na contrastive distillation loss along with an image-text alignment constraint.\nTo further combat in-distribution overfitting and enhance distillation\nperformance with limited amount of generated data, we incorporate adaptive\nweight consolidation, utilizing Fisher information from these synthetic\nimage-text pairs and achieving a better stability-plasticity balance. Extensive\nexperiments demonstrate that our method consistently outperforms previous\nstate-of-the-art approaches across various settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image-text",
      "text-to-image"
    ]
  },
  {
    "id": "2503_04205v1",
    "title": "Learning 3D Medical Image Models From Brain Functional Connectivity\n  Network Supervision For Mental Disorder Diagnosis",
    "url": "http://arxiv.org/abs/2503.04205v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04205v1",
    "authors": [
      "Xingcan Hu",
      "Wei Wang",
      "Li Xiao"
    ],
    "date": "2025-03-06",
    "summary": "In MRI-based mental disorder diagnosis, most previous studies focus on\nfunctional connectivity network (FCN) derived from functional MRI (fMRI).\nHowever, the small size of annotated fMRI datasets restricts its wide\napplication. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w)\nMRI, which are commonly used and readily accessible in clinical settings, are\noften overlooked. To integrate the complementary information from both function\nand structure for improved diagnostic accuracy, we propose CINP (Contrastive\nImage-Network Pre-training), a framework that employs contrastive learning\nbetween sMRI and FCN. During pre-training, we incorporate masked image modeling\nand network-image matching to enhance visual representation learning and\nmodality alignment. Since the CINP facilitates knowledge transfer from FCN to\nsMRI, we introduce network prompting. It utilizes only sMRI from suspected\npatients and a small amount of FCNs from different patient classes for\ndiagnosing mental disorders, which is practical in real-world clinical\nscenario. The competitive performance on three mental disorder diagnosis tasks\ndemonstrate the effectiveness of the CINP in integrating multimodal MRI\ninformation, as well as the potential of incorporating sMRI into clinical\ndiagnosis using network prompting.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04201v1",
    "title": "Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative\n  Approach to Few-shot Multimodal Dialogue Intention Recognition",
    "url": "http://arxiv.org/abs/2503.04201v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04201v1",
    "authors": [
      "Bin Chen",
      "Yu Zhang",
      "Hongfei Ye",
      "Ziyi Huang",
      "Hongyang Chen"
    ],
    "date": "2025-03-06",
    "summary": "Few-shot multimodal dialogue intention recognition is a critical challenge in\nthe e-commerce domainn. Previous methods have primarily enhanced model\nclassification capabilities through post-training techniques. However, our\nanalysis reveals that training for few-shot multimodal dialogue intention\nrecognition involves two interconnected tasks, leading to a seesaw effect in\nmulti-task learning. This phenomenon is attributed to knowledge interference\nstemming from the superposition of weight matrix updates during the training\nprocess. To address these challenges, we propose Knowledge-Decoupled Synergetic\nLearning (KDSL), which mitigates these issues by utilizing smaller models to\ntransform knowledge into interpretable rules, while applying the post-training\nof larger models. By facilitating collaboration between the large and small\nmultimodal large language models for prediction, our approach demonstrates\nsignificant improvements. Notably, we achieve outstanding results on two real\nTaobao datasets, with enhancements of 6.37\\% and 6.28\\% in online weighted F1\nscores compared to the state-of-the-art method, thereby validating the efficacy\nof our framework.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04199v1",
    "title": "MASTER: Multimodal Segmentation with Text Prompts",
    "url": "http://arxiv.org/abs/2503.04199v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04199v1",
    "authors": [
      "Fuyang Liu",
      "Shun Lu",
      "Jilin Mei",
      "Yu Hu"
    ],
    "date": "2025-03-06",
    "summary": "RGB-Thermal fusion is a potential solution for various weather and light\nconditions in challenging scenarios. However, plenty of studies focus on\ndesigning complex modules to fuse different modalities. With the widespread\napplication of large language models (LLMs), valuable information can be more\neffectively extracted from natural language. Therefore, we aim to leverage the\nadvantages of large language models to design a structurally simple and highly\nadaptable multimodal fusion model architecture. We proposed MultimodAl\nSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM into\nthe fusion of RGB-Thermal multimodal data and allows complex query text to\nparticipate in the fusion process. Our model utilizes a dual-path structure to\nextract information from different modalities of images. Additionally, we\nemploy LLM as the core module for multimodal fusion, enabling the model to\ngenerate learnable codebook tokens from RGB, thermal images, and textual\ninformation. A lightweight image decoder is used to obtain semantic\nsegmentation results. The proposed MASTER performs exceptionally well in\nbenchmark tests across various automated driving scenarios, yielding promising\nresults.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04167v1",
    "title": "The Role of Visual Modality in Multimodal Mathematical Reasoning:\n  Challenges and Insights",
    "url": "http://arxiv.org/abs/2503.04167v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04167v1",
    "authors": [
      "Yufang Liu",
      "Yao Du",
      "Tao Ji",
      "Jianing Wang",
      "Yang Liu",
      "Yuanbin Wu",
      "Aimin Zhou",
      "Mengdi Zhang",
      "Xunliang Cai"
    ],
    "date": "2025-03-06",
    "summary": "Recent research has increasingly focused on multimodal mathematical\nreasoning, particularly emphasizing the creation of relevant datasets and\nbenchmarks. Despite this, the role of visual information in reasoning has been\nunderexplored. Our findings show that existing multimodal mathematical models\nminimally leverage visual information, and model performance remains largely\nunaffected by changes to or removal of images in the dataset. We attribute this\nto the dominance of textual information and answer options that inadvertently\nguide the model to correct answers. To improve evaluation methods, we introduce\nthe HC-M3D dataset, specifically designed to require image reliance for\nproblem-solving and to challenge models with similar, yet distinct, images that\nchange the correct answer. In testing leading models, their failure to detect\nthese subtle visual differences suggests limitations in current visual\nperception capabilities. Additionally, we observe that the common approach of\nimproving general VQA capabilities by combining various types of image encoders\ndoes not contribute to math reasoning performance. This finding also presents a\nchallenge to enhancing visual reliance during math reasoning. Our benchmark and\ncode would be available at\n\\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\\_modality\\_role}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "VQA"
    ]
  },
  {
    "id": "2503_04144v1",
    "title": "DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person\n  Retrieval",
    "url": "http://arxiv.org/abs/2503.04144v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04144v1",
    "authors": [
      "Yating Liu",
      "Zimo Liu",
      "Xiangyuan Lan",
      "Wenming Yang",
      "Yaowei Li",
      "Qingmin Liao"
    ],
    "date": "2025-03-06",
    "summary": "Text-based person retrieval (TPR) has gained significant attention as a\nfine-grained and challenging task that closely aligns with practical\napplications. Tailoring CLIP to person domain is now a emerging research topic\ndue to the abundant knowledge of vision-language pretraining, but challenges\nstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is\ncomputationally expensive and prone to overfitting.(ii) Existing\nparameter-efficient transfer learning (PETL) for TPR lacks of fine-grained\nfeature extraction. To address these issues, we propose Domain-Aware\nMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and\nPETL to enhance fine-grained feature representations while maintaining\nefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to\nMLP layers in both vision and language branches, where different experts\nspecialize in distinct aspects of person knowledge to handle features more\nfinely. To promote the router to exploit domain information effectively and\nalleviate the routing imbalance, Domain-Aware Router is then developed by\nbuilding a novel gating function and injecting learnable domain-aware prompts.\nExtensive experiments show that our DM-Adapter achieves state-of-the-art\nperformance, outperforming previous methods by a significant margin.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP"
    ]
  },
  {
    "id": "2503_04135v1",
    "title": "Biological Sequence with Language Model Prompting: A Survey",
    "url": "http://arxiv.org/abs/2503.04135v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04135v1",
    "authors": [
      "Jiyue Jiang",
      "Zikang Wang",
      "Yuheng Shan",
      "Heyan Chai",
      "Jiayi Li",
      "Zixian Ma",
      "Xinrui Zhang",
      "Yu Li"
    ],
    "date": "2025-03-06",
    "summary": "Large Language models (LLMs) have emerged as powerful tools for addressing\nchallenges across diverse domains. Notably, recent studies have demonstrated\nthat large language models significantly enhance the efficiency of biomolecular\nanalysis and synthesis, attracting widespread attention from academics and\nmedicine. In this paper, we systematically investigate the application of\nprompt-based methods with LLMs to biological sequences, including DNA, RNA,\nproteins, and drug discovery tasks. Specifically, we focus on how prompt\nengineering enables LLMs to tackle domain-specific problems, such as promoter\nsequence prediction, protein structure modeling, and drug-target binding\naffinity prediction, often with limited labeled data. Furthermore, our\ndiscussion highlights the transformative potential of prompting in\nbioinformatics while addressing key challenges such as data scarcity,\nmultimodal fusion, and computational resource limitations. Our aim is for this\npaper to function both as a foundational primer for newcomers and a catalyst\nfor continued innovation within this dynamic field of study.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04130v1",
    "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.04130v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04130v1",
    "authors": [
      "Jindong Jiang",
      "Xiuyu Li",
      "Zhijian Liu",
      "Muyang Li",
      "Guo Chen",
      "Zhiqi Li",
      "De-An Huang",
      "Guilin Liu",
      "Zhiding Yu",
      "Kurt Keutzer",
      "Sungjin Ahn",
      "Jan Kautz",
      "Hongxu Yin",
      "Yao Lu",
      "Song Han",
      "Wonmin Byeon"
    ],
    "date": "2025-03-06",
    "summary": "Recent advances in video-based multimodal large language models (Video-LLMs)\nhave significantly improved video understanding by processing videos as\nsequences of image frames. However, many existing methods treat frames\nindependently in the vision backbone, lacking explicit temporal modeling, which\nlimits their ability to capture dynamic patterns and efficiently handle long\nvideos. To address these limitations, we introduce STORM\n(\\textbf{S}patiotemporal \\textbf{TO}ken \\textbf{R}eduction for\n\\textbf{M}ultimodal LLMs), a novel architecture incorporating a dedicated\ntemporal encoder between the image encoder and the LLM. Our temporal encoder\nleverages the Mamba State Space Model to integrate temporal information into\nimage tokens, generating enriched representations that preserve inter-frame\ndynamics across the entire video sequence. This enriched encoding not only\nenhances video reasoning capabilities but also enables effective token\nreduction strategies, including test-time sampling and training-based temporal\nand spatial pooling, substantially reducing computational demands on the LLM\nwithout sacrificing key temporal information. By integrating these techniques,\nour approach simultaneously reduces training and inference latency while\nimproving performance, enabling efficient and robust video understanding over\nextended temporal contexts. Extensive evaluations show that STORM achieves\nstate-of-the-art results across various long video understanding benchmarks\n(more than 5\\% improvement on MLVU and LongVideoBench) while reducing the\ncomputation costs by up to $8\\times$ and the decoding latency by\n2.4-2.9$\\times$ for the fixed numbers of input frames. Project page is\navailable at https://research.nvidia.com/labs/lpr/storm",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04121v1",
    "title": "Simple Self Organizing Map with Visual Transformer",
    "url": "http://arxiv.org/abs/2503.04121v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04121v1",
    "authors": [
      "Alan Luo",
      "Kaiwen Yuan"
    ],
    "date": "2025-03-06",
    "summary": "Vision Transformers (ViTs) have demonstrated exceptional performance in\nvarious vision tasks. However, they tend to underperform on smaller datasets\ndue to their inherent lack of inductive biases. Current approaches address this\nlimitation implicitly-often by pairing ViTs with pretext tasks or by distilling\nknowledge from convolutional neural networks (CNNs) to strengthen the prior. In\ncontrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised\nframework, are inherently structured to preserve topology and spatial\norganization, making them a promising candidate to directly address the\nlimitations of ViTs in limited or small training datasets. Despite this\npotential, equipping SOMs with modern deep learning architectures remains\nlargely unexplored. In this study, we conduct a novel exploration on how Vision\nTransformers (ViTs) and Self-Organizing Maps (SOMs) can empower each other,\naiming to bridge this critical research gap. Our findings demonstrate that\nthese architectures can synergistically enhance each other, leading to\nsignificantly improved performance in both unsupervised and supervised tasks.\nCode will be publicly available.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_04110v1",
    "title": "InterChat: Enhancing Generative Visual Analytics using Multimodal\n  Interactions",
    "url": "http://arxiv.org/abs/2503.04110v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04110v1",
    "authors": [
      "Juntong Chen",
      "Jiang Wu",
      "Jiajing Guo",
      "Vikram Mohanty",
      "Xueming Li",
      "Jorge Piazentin Ono",
      "Wenbin He",
      "Liu Ren",
      "Dongyu Liu"
    ],
    "date": "2025-03-06",
    "summary": "The rise of Large Language Models (LLMs) and generative visual analytics\nsystems has transformed data-driven insights, yet significant challenges\npersist in accurately interpreting users' analytical and interaction intents.\nWhile language inputs offer flexibility, they often lack precision, making the\nexpression of complex intents inefficient, error-prone, and time-intensive. To\naddress these limitations, we investigate the design space of multimodal\ninteractions for generative visual analytics through a literature review and\npilot brainstorming sessions. Building on these insights, we introduce a highly\nextensible workflow that integrates multiple LLM agents for intent inference\nand visualization generation. We develop InterChat, a generative visual\nanalytics system that combines direct manipulation of visual elements with\nnatural language inputs. This integration enables precise intent communication\nand supports progressive, visually driven exploratory data analyses. By\nemploying effective prompt engineering, and contextual interaction linking,\nalongside intuitive visualization and interaction designs, InterChat bridges\nthe gap between user interactions and LLM-driven visualizations, enhancing both\ninterpretability and usability. Extensive evaluations, including two usage\nscenarios, a user study, and expert feedback, demonstrate the effectiveness of\nInterChat. Results show significant improvements in the accuracy and efficiency\nof handling complex visual analytics tasks, highlighting the potential of\nmultimodal interactions to redefine user engagement and analytical depth in\ngenerative visual analytics.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04095v2",
    "title": "Chart-HQA: A Benchmark for Hypothetical Question Answering in Charts",
    "url": "http://arxiv.org/abs/2503.04095v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04095v2",
    "authors": [
      "Xiangnan Chen",
      "Yuancheng Fang",
      "Qian Xiao",
      "Juncheng Li",
      "Jun Lin",
      "Siliang Tang",
      "Yi Yang",
      "Yueting Zhuang"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal Large Language Models (MLLMs) have garnered significant attention\nfor their strong visual-semantic understanding. Most existing chart benchmarks\nevaluate MLLMs' ability to parse information from charts to answer questions.\nHowever, they overlook the inherent output biases of MLLMs, where models rely\non their parametric memory to answer questions rather than genuinely\nunderstanding the chart content. To address this limitation, we introduce a\nnovel Chart Hypothetical Question Answering (HQA) task, which imposes\nassumptions on the same question to compel models to engage in counterfactual\nreasoning based on the chart content. Furthermore, we introduce HAI, a human-AI\ninteractive data synthesis approach that leverages the efficient text-editing\ncapabilities of LLMs alongside human expert knowledge to generate diverse and\nhigh-quality HQA data at a low cost. Using HAI, we construct Chart-HQA, a\nchallenging benchmark synthesized from publicly available data sources.\nEvaluation results on 18 MLLMs of varying model sizes reveal that current\nmodels face significant generalization challenges and exhibit imbalanced\nreasoning performance on the HQA task.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04078v1",
    "title": "Spatial-Temporal Perception with Causal Inference for Naturalistic\n  Driving Action Recognition",
    "url": "http://arxiv.org/abs/2503.04078v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04078v1",
    "authors": [
      "Qing Chang",
      "Wei Dai",
      "Zhihao Shuai",
      "Limin Yu",
      "Yutao Yue"
    ],
    "date": "2025-03-06",
    "summary": "Naturalistic driving action recognition is essential for vehicle cabin\nmonitoring systems. However, the complexity of real-world backgrounds presents\nsignificant challenges for this task, and previous approaches have struggled\nwith practical implementation due to their limited ability to observe subtle\nbehavioral differences and effectively learn inter-frame features from video.\nIn this paper, we propose a novel Spatial-Temporal Perception (STP)\narchitecture that emphasizes both temporal information and spatial\nrelationships between key objects, incorporating a causal decoder to perform\nbehavior recognition and temporal action localization. Without requiring\nmultimodal input, STP directly extracts temporal and spatial distance features\nfrom RGB video clips. Subsequently, these dual features are jointly encoded by\nmaximizing the expected likelihood across all possible permutations of the\nfactorization order. By integrating temporal and spatial features at different\nscales, STP can perceive subtle behavioral changes in challenging scenarios.\nAdditionally, we introduce a causal-aware module to explore relationships\nbetween video frame features, significantly enhancing detection efficiency and\nperformance. We validate the effectiveness of our approach using two publicly\navailable driver distraction detection benchmarks. The results demonstrate that\nour framework achieves state-of-the-art performance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "CLIP"
    ]
  },
  {
    "id": "2503_04065v1",
    "title": "PP-DocBee: Improving Multimodal Document Understanding Through a Bag of\n  Tricks",
    "url": "http://arxiv.org/abs/2503.04065v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04065v1",
    "authors": [
      "Feng Ni",
      "Kui Huang",
      "Yao Lu",
      "Wenyu Lv",
      "Guanzhong Wang",
      "Zeyu Chen",
      "Yi Liu"
    ],
    "date": "2025-03-06",
    "summary": "With the rapid advancement of digitalization, various document images are\nbeing applied more extensively in production and daily life, and there is an\nincreasingly urgent need for fast and accurate parsing of the content in\ndocument images. Therefore, this report presents PP-DocBee, a novel multimodal\nlarge language model designed for end-to-end document image understanding.\nFirst, we develop a data synthesis strategy tailored to document scenarios in\nwhich we build a diverse dataset to improve the model generalization. Then, we\napply a few training techniques, including dynamic proportional sampling, data\npreprocessing, and OCR postprocessing strategies. Extensive evaluations\ndemonstrate the superior performance of PP-DocBee, achieving state-of-the-art\nresults on English document understanding benchmarks and even outperforming\nexisting open source and commercial models in Chinese document understanding.\nThe source code and pre-trained models are publicly available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04058v1",
    "title": "EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language\n  Models",
    "url": "http://arxiv.org/abs/2503.04058v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04058v1",
    "authors": [
      "Haiyang Yu",
      "Jinghui Lu",
      "Yanjie Wang",
      "Yang Li",
      "Han Wang",
      "Can Huang",
      "Bin Li"
    ],
    "date": "2025-03-06",
    "summary": "The advent of Large Vision-Language Models (LVLMs) has advanced the\nvideo-based tasks, such as video captioning and video understanding. Some\nprevious research indicates that taking texts in videos as input can further\nimprove the performance of video understanding. As a type of indispensable\ninformation in short videos or movies, subtitles can assist LVLMs to better\nunderstand videos. Most existing methods for video subtitle extraction are\nbased on a multi-stage framework, handling each frame independently. They can\nhardly exploit the temporal information of videos. Although some LVLMs exhibit\nthe robust OCR capability, predicting accurate timestamps for subtitle texts is\nstill challenging. In this paper, we propose an End-to-end Video Subtitle\nExtraction method, called EVE, which consists of three modules: a vision\nencoder, an adapter module, and a large language model. To effectively compress\nthe visual tokens from the vision encoder, we propose a novel adapter\nInterleavedVT to interleave two modalities. It contains a visual compressor and\na textual region compressor. The proposed InterleavedVT exploits both the\nmerits of average pooling and Q-Former in token compression. Taking the\ntemporal information of videos into account, we introduce a sliding-window\nmechanism in the textual region compressor. To benchmark the video subtitle\nextraction task, we propose a large dataset ViSa including 2.5M videos.\nExtensive experiments on ViSa demonstrate that the proposed EVE can outperform\nexisting open-sourced tools and LVLMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_04034v1",
    "title": "GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world\n  Scene Understanding",
    "url": "http://arxiv.org/abs/2503.04034v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04034v1",
    "authors": [
      "Xihan Wang",
      "Dianyi Yang",
      "Yu Gao",
      "Yufeng Yue",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in 3D Gaussian Splatting(3DGS) have significantly\nimproved semantic scene understanding, enabling natural language queries to\nlocalize objects within a scene. However, existing methods primarily focus on\nembedding compressed CLIP features to 3D Gaussians, suffering from low object\nsegmentation accuracy and lack spatial reasoning capabilities. To address these\nlimitations, we propose GaussianGraph, a novel framework that enhances\n3DGS-based scene understanding by integrating adaptive semantic clustering and\nscene graph generation. We introduce a \"Control-Follow\" clustering strategy,\nwhich dynamically adapts to scene scale and feature distribution, avoiding\nfeature compression and significantly improving segmentation accuracy.\nAdditionally, we enrich scene representation by integrating object attributes\nand spatial relations extracted from 2D foundation models. To address\ninaccuracies in spatial relationships, we propose 3D correction modules that\nfilter implausible relations through spatial consistency verification, ensuring\nreliable scene graph construction. Extensive experiments on three datasets\ndemonstrate that GaussianGraph outperforms state-of-the-art methods in both\nsemantic segmentation and object grounding tasks, providing a robust solution\nfor complex scene understanding and interaction.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_04006v1",
    "title": "DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for\n  Robust Few-Shot Segmentation",
    "url": "http://arxiv.org/abs/2503.04006v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04006v1",
    "authors": [
      "Amin Karimi",
      "Charalambos Poullis"
    ],
    "date": "2025-03-06",
    "summary": "Few-shot semantic segmentation (FSS) aims to enable models to segment\nnovel/unseen object classes using only a limited number of labeled examples.\nHowever, current FSS methods frequently struggle with generalization due to\nincomplete and biased feature representations, especially when support images\ndo not capture the full appearance variability of the target class. To improve\nthe FSS pipeline, we propose a novel framework that utilizes large language\nmodels (LLMs) to adapt general class semantic information to the query image.\nFurthermore, the framework employs dense pixel-wise matching to identify\nsimilarities between query and support images, resulting in enhanced FSS\nperformance. Inspired by reasoning-based segmentation frameworks, our method,\nnamed DSV-LFS, introduces an additional token into the LLM vocabulary, allowing\na multimodal LLM to generate a \"semantic prompt\" from class descriptions. In\nparallel, a dense matching module identifies visual similarities between the\nquery and support images, generating a \"visual prompt\". These prompts are then\njointly employed to guide the prompt-based decoder for accurate segmentation of\nthe query image. Comprehensive experiments on the benchmark datasets\nPascal-$5^{i}$ and COCO-$20^{i}$ demonstrate that our framework achieves\nstate-of-the-art performance-by a significant margin-demonstrating superior\ngeneralization to novel classes and robustness across diverse scenarios. The\nsource code is available at\n\\href{https://github.com/aminpdik/DSV-LFS}{https://github.com/aminpdik/DSV-LFS}",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03987v1",
    "title": "RetinalGPT: A Retinal Clinical Preference Conversational Assistant\n  Powered by Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.03987v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03987v1",
    "authors": [
      "Wenhui Zhu",
      "Xin Li",
      "Xiwen Chen",
      "Peijie Qiu",
      "Vamsi Krishna Vasa",
      "Xuanzhao Dong",
      "Yanxi Chen",
      "Natasha Lepore",
      "Oana Dumitrascu",
      "Yi Su",
      "Yalin Wang"
    ],
    "date": "2025-03-06",
    "summary": "Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention for their remarkable ability to process and analyze non-textual data,\nsuch as images, videos, and audio. Notably, several adaptations of\ngeneral-domain MLLMs to the medical field have been explored, including\nLLaVA-Med. However, these medical adaptations remain insufficiently advanced in\nunderstanding and interpreting retinal images. In contrast, medical experts\nemphasize the importance of quantitative analyses for disease detection and\ninterpretation. This underscores a gap between general-domain and\nmedical-domain MLLMs: while general-domain MLLMs excel in broad applications,\nthey lack the specialized knowledge necessary for precise diagnostic and\ninterpretative tasks in the medical field. To address these challenges, we\nintroduce \\textit{RetinalGPT}, a multimodal conversational assistant for\nclinically preferred quantitative analysis of retinal images. Specifically, we\nachieve this by compiling a large retinal image dataset, developing a novel\ndata pipeline, and employing customized visual instruction tuning to enhance\nboth retinal analysis and enrich medical knowledge. In particular, RetinalGPT\noutperforms MLLM in the generic domain by a large margin in the diagnosis of\nretinal diseases in 8 benchmark retinal datasets. Beyond disease diagnosis,\nRetinalGPT features quantitative analyses and lesion localization, representing\na pioneering step in leveraging LLMs for an interpretable and end-to-end\nclinical research framework. The code is available at\nhttps://github.com/Retinal-Research/RetinalGPT",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_03947v1",
    "title": "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.03947v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03947v1",
    "authors": [
      "Aurelio Noca",
      "Xianmei Lei",
      "Jonathan Becktor",
      "Jeffrey Edlund",
      "Anna Sabel",
      "Patrick Spieler",
      "Curtis Padgett",
      "Alexandre Alahi",
      "Deegan Atha"
    ],
    "date": "2025-03-05",
    "summary": "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_03935v1",
    "title": "GlucoLens: Explainable Postprandial Blood Glucose Prediction from Diet\n  and Physical Activity",
    "url": "http://arxiv.org/abs/2503.03935v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03935v1",
    "authors": [
      "Abdullah Mamun",
      "Asiful Arefeen",
      "Susan B. Racette",
      "Dorothy D. Sears",
      "Corrie M. Whisner",
      "Matthew P. Buman",
      "Hassan Ghasemzadeh"
    ],
    "date": "2025-03-05",
    "summary": "Postprandial hyperglycemia, marked by the blood glucose level exceeding the\nnormal range after meals, is a critical indicator of progression toward type 2\ndiabetes in prediabetic and healthy individuals. A key metric for understanding\nblood glucose dynamics after eating is the postprandial area under the curve\n(PAUC). Predicting PAUC in advance based on a person's diet and activity level\nand explaining what affects postprandial blood glucose could allow an\nindividual to adjust their lifestyle accordingly to maintain normal glucose\nlevels. In this paper, we propose GlucoLens, an explainable machine learning\napproach to predict PAUC and hyperglycemia from diet, activity, and recent\nglucose patterns. We conducted a five-week user study with 10 full-time working\nindividuals to develop and evaluate the computational model. Our machine\nlearning model takes multimodal data including fasting glucose, recent glucose,\nrecent activity, and macronutrient amounts, and provides an interpretable\nprediction of the postprandial glucose pattern. Our extensive analyses of the\ncollected data revealed that the trained model achieves a normalized root mean\nsquared error (NRMSE) of 0.123. On average, GlucoLense with a Random Forest\nbackbone provides a 16% better result than the baseline models. Additionally,\nGlucoLens predicts hyperglycemia with an accuracy of 74% and recommends\ndifferent options to help avoid hyperglycemia through diverse counterfactual\nexplanations. Code available: https://github.com/ab9mamun/GlucoLens.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03854v1",
    "title": "Vision-Language Models Struggle to Align Entities across Modalities",
    "url": "http://arxiv.org/abs/2503.03854v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03854v1",
    "authors": [
      "I\u00f1igo Alonso",
      "Ander Salaberria",
      "Gorka Azkune",
      "Jeremy Barnes",
      "Oier Lopez de Lacalle"
    ],
    "date": "2025-03-05",
    "summary": "Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_03848v1",
    "title": "Nexar Dashcam Collision Prediction Dataset and Challenge",
    "url": "http://arxiv.org/abs/2503.03848v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03848v1",
    "authors": [
      "Daniel C. Moura",
      "Shizhan Zhu",
      "Orly Zvitia"
    ],
    "date": "2025-03-05",
    "summary": "This paper presents the Nexar Dashcam Collision Prediction Dataset and\nChallenge, designed to support research in traffic event analysis, collision\nprediction, and autonomous vehicle safety. The dataset consists of 1,500\nannotated video clips, each approximately 40 seconds long, capturing a diverse\nrange of real-world traffic scenarios. Videos are labeled with event type\n(collision/near-collision vs. normal driving), environmental conditions\n(lighting conditions and weather), and scene type (urban, rural, highway,\netc.). For collision and near-collision cases, additional temporal labels are\nprovided, including the precise moment of the event and the alert time, marking\nwhen the collision first becomes predictable.\n  To advance research on accident prediction, we introduce the Nexar Dashcam\nCollision Prediction Challenge, a public competition on top of this dataset.\nParticipants are tasked with developing machine learning models that predict\nthe likelihood of an imminent collision, given an input video. Model\nperformance is evaluated using the average precision (AP) computed across\nmultiple intervals before the accident (i.e. 500 ms, 1000 ms, and 1500 ms prior\nto the event), emphasizing the importance of early and reliable predictions.\n  The dataset is released under an open license with restrictions on unethical\nuse, ensuring responsible research and innovation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_03840v1",
    "title": "Decoupling the components of geometric understanding in Vision Language\n  Models",
    "url": "http://arxiv.org/abs/2503.03840v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03840v1",
    "authors": [
      "Eliza Kosoy",
      "Annya Dahmani",
      "Andrew K. Lampinen",
      "Iulia M. Comsa",
      "Soojin Jeong",
      "Ishita Dasgupta",
      "Kelsey Allen"
    ],
    "date": "2025-03-05",
    "summary": "Understanding geometry relies heavily on vision. In this work, we evaluate\nwhether state-of-the-art vision language models (VLMs) can understand simple\ngeometric concepts. We use a paradigm from cognitive science that isolates\nvisual understanding of simple geometry from the many other capabilities it is\noften conflated with such as reasoning and world knowledge. We compare model\nperformance with human adults from the USA, as well as with prior research on\nhuman adults without formal education from an Amazonian indigenous group. We\nfind that VLMs consistently underperform both groups of human adults, although\nthey succeed with some concepts more than others. We also find that VLM\ngeometric understanding is more brittle than human understanding, and is not\nrobust when tasks require mental rotation. This work highlights interesting\ndifferences in the origin of geometric understanding in humans and machines --\ne.g. from printed materials used in formal education vs. interactions with the\nphysical world or a combination of the two -- and a small step toward\nunderstanding these differences.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_03743v1",
    "title": "CHOP: Mobile Operating Assistant with Constrained High-frequency\n  Optimized Subtask Planning",
    "url": "http://arxiv.org/abs/2503.03743v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03743v1",
    "authors": [
      "Yuqi Zhou",
      "Shuai Wang",
      "Sunhao Dai",
      "Qinglin Jia",
      "Zhaocheng Du",
      "Zhenhua Dong",
      "Jun Xu"
    ],
    "date": "2025-03-05",
    "summary": "The advancement of visual language models (VLMs) has enhanced mobile device\noperations, allowing simulated human-like actions to address user requirements.\nCurrent VLM-based mobile operating assistants can be structured into three\nlevels: task, subtask, and action. The subtask level, linking high-level goals\nwith low-level executable actions, is crucial for task completion but faces two\nchallenges: ineffective subtasks that lower-level agent cannot execute and\ninefficient subtasks that fail to contribute to the completion of the\nhigher-level task. These challenges stem from VLM's lack of experience in\ndecomposing subtasks within GUI scenarios in multi-agent architecture. To\naddress these, we propose a new mobile assistant architecture with constrained\nhigh-frequency o}ptimized planning (CHOP). Our approach overcomes the VLM's\ndeficiency in GUI scenarios planning by using human-planned subtasks as the\nbasis vector. We evaluate our architecture in both English and Chinese contexts\nacross 20 Apps, demonstrating significant improvements in both effectiveness\nand efficiency. Our dataset and code is available at\nhttps://github.com/Yuqi-Zhou/CHOP",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ]
  },
  {
    "id": "2503_03803v1",
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "url": "http://arxiv.org/abs/2503.03803v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03803v1",
    "authors": [
      "Jingkang Yang",
      "Shuai Liu",
      "Hongming Guo",
      "Yuhao Dong",
      "Xiamengwei Zhang",
      "Sicheng Zhang",
      "Pengyun Wang",
      "Zitang Zhou",
      "Binzhu Xie",
      "Ziyue Wang",
      "Bei Ouyang",
      "Zhengyu Lin",
      "Marco Cominelli",
      "Zhongang Cai",
      "Yuanhan Zhang",
      "Peiyuan Zhang",
      "Fangzhou Hong",
      "Joerg Widmer",
      "Francesco Gringoli",
      "Lei Yang",
      "Bo Li",
      "Ziwei Liu"
    ],
    "date": "2025-03-05",
    "summary": "We introduce EgoLife, a project to develop an egocentric life assistant that\naccompanies and enhances personal efficiency through AI-powered wearable\nglasses. To lay the foundation for this assistant, we conducted a comprehensive\ndata collection study where six participants lived together for one week,\ncontinuously recording their daily activities - including discussions,\nshopping, cooking, socializing, and entertainment - using AI glasses for\nmultimodal egocentric video capture, along with synchronized third-person-view\nvideo references. This effort resulted in the EgoLife Dataset, a comprehensive\n300-hour egocentric, interpersonal, multiview, and multimodal daily life\ndataset with intensive annotation. Leveraging this dataset, we introduce\nEgoLifeQA, a suite of long-context, life-oriented question-answering tasks\ndesigned to provide meaningful assistance in daily life by addressing practical\nquestions such as recalling past relevant events, monitoring health habits, and\noffering personalized recommendations. To address the key technical challenges\nof (1) developing robust visual-audio models for egocentric data, (2) enabling\nidentity recognition, and (3) facilitating long-context question answering over\nextensive temporal information, we introduce EgoButler, an integrated system\ncomprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on\negocentric datasets, achieving state-of-the-art performance on egocentric video\nunderstanding. EgoRAG is a retrieval-based component that supports answering\nultra-long-context questions. Our experimental studies verify their working\nmechanisms and reveal critical factors and bottlenecks, guiding future\nimprovements. By releasing our datasets, models, and benchmarks, we aim to\nstimulate further research in egocentric AI assistants.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03734v1",
    "title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction",
    "url": "http://arxiv.org/abs/2503.03734v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03734v1",
    "authors": [
      "Huang Huang",
      "Fangchen Liu",
      "Letian Fu",
      "Tingfan Wu",
      "Mustafa Mukadam",
      "Jitendra Malik",
      "Ken Goldberg",
      "Pieter Abbeel"
    ],
    "date": "2025-03-05",
    "summary": "Vision-Language-Action (VLA) models aim to predict robotic actions based on\nvisual observations and language instructions. Existing approaches require\nfine-tuning pre-trained visionlanguage models (VLMs) as visual and language\nfeatures are independently fed into downstream policies, degrading the\npre-trained semantic alignments. We propose OTTER, a novel VLA architecture\nthat leverages these existing alignments through explicit, text-aware visual\nfeature extraction. Instead of processing all visual features, OTTER\nselectively extracts and passes only task-relevant visual features that are\nsemantically aligned with the language instruction to the policy transformer.\nThis allows OTTER to keep the pre-trained vision-language encoders frozen.\nThereby, OTTER preserves and utilizes the rich semantic understanding learned\nfrom large-scale pre-training, enabling strong zero-shot generalization\ncapabilities. In simulation and real-world experiments, OTTER significantly\noutperforms existing VLA models, demonstrating strong zeroshot generalization\nto novel objects and environments. Video, code, checkpoints, and dataset:\nhttps://ottervla.github.io/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_03689v1",
    "title": "DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with\n  Reward Guidance",
    "url": "http://arxiv.org/abs/2503.03689v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03689v1",
    "authors": [
      "Zhao Yang",
      "Zezhong Qian",
      "Xiaofan Li",
      "Weixiang Xu",
      "Gongpeng Zhao",
      "Ruohong Yu",
      "Lingsi Zhu",
      "Longjun Liu"
    ],
    "date": "2025-03-05",
    "summary": "Accurate and high-fidelity driving scene reconstruction demands the effective\nutilization of comprehensive scene information as conditional inputs. Existing\nmethods predominantly rely on 3D bounding boxes and BEV road maps for\nforeground and background control, which fail to capture the full complexity of\ndriving scenes and adequately integrate multimodal information. In this work,\nwe present DualDiff, a dual-branch conditional diffusion model designed to\nenhance driving scene generation across multiple views and video sequences.\nSpecifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional\ninput, offering rich foreground and background semantics alongside 3D spatial\ngeometry to precisely control the generation of both elements. To improve the\nsynthesis of fine-grained foreground objects, particularly complex and distant\nones, we propose a Foreground-Aware Mask (FGM) denoising loss function.\nAdditionally, we develop the Semantic Fusion Attention (SFA) mechanism to\ndynamically prioritize relevant information and suppress noise, enabling more\neffective multimodal fusion. Finally, to ensure high-quality image-to-video\ngeneration, we introduce the Reward-Guided Diffusion (RGD) framework, which\nmaintains global consistency and semantic coherence in generated videos.\nExtensive experiments demonstrate that DualDiff achieves state-of-the-art\n(SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff\nreduces the FID score by 4.09% compared to the best baseline. In downstream\ntasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and\nroad mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP\nincreases by 1.46%. Code will be made available at\nhttps://github.com/yangzhaojason/DualDiff.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03664v1",
    "title": "A Generative Approach to High Fidelity 3D Reconstruction from Text Data",
    "url": "http://arxiv.org/abs/2503.03664v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03664v1",
    "authors": [
      "Venkat Kumar R",
      "Deepak Saravanan"
    ],
    "date": "2025-03-05",
    "summary": "The convergence of generative artificial intelligence and advanced computer\nvision technologies introduces a groundbreaking approach to transforming\ntextual descriptions into three-dimensional representations. This research\nproposes a fully automated pipeline that seamlessly integrates text-to-image\ngeneration, various image processing techniques, and deep learning methods for\nreflection removal and 3D reconstruction. By leveraging state-of-the-art\ngenerative models like Stable Diffusion, the methodology translates natural\nlanguage inputs into detailed 3D models through a multi-stage workflow.\n  The reconstruction process begins with the generation of high-quality images\nfrom textual prompts, followed by enhancement by a reinforcement learning agent\nand reflection removal using the Stable Delight model. Advanced image upscaling\nand background removal techniques are then applied to further enhance visual\nfidelity. These refined two-dimensional representations are subsequently\ntransformed into volumetric 3D models using sophisticated machine learning\nalgorithms, capturing intricate spatial relationships and geometric\ncharacteristics. This process achieves a highly structured and detailed output,\nensuring that the final 3D models reflect both semantic accuracy and geometric\nprecision.\n  This approach addresses key challenges in generative reconstruction, such as\nmaintaining semantic coherence, managing geometric complexity, and preserving\ndetailed visual information. Comprehensive experimental evaluations will assess\nreconstruction quality, semantic accuracy, and geometric fidelity across\ndiverse domains and varying levels of complexity. By demonstrating the\npotential of AI-driven 3D reconstruction techniques, this research offers\nsignificant implications for fields such as augmented reality (AR), virtual\nreality (VR), and digital content creation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion",
      "text-to-image"
    ]
  },
  {
    "id": "2503_03663v2",
    "title": "LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant",
    "url": "http://arxiv.org/abs/2503.03663v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03663v2",
    "authors": [
      "Wei Li",
      "Bing Hu",
      "Rui Shao",
      "Leyang Shen",
      "Liqiang Nie"
    ],
    "date": "2025-03-05",
    "summary": "First-person video assistants are highly anticipated to enhance our daily\nlives through online video dialogue. However, existing online video assistants\noften sacrifice assistant efficacy for real-time efficiency by processing\nlow-frame-rate videos with coarse-grained visual features.To overcome the\ntrade-off between efficacy and efficiency, we propose \"Fast & Slow\nVideo-Language Thinker\" as an onLIne videO assistaNt, LION-FS, achieving\nreal-time, proactive, temporally accurate, and contextually precise responses.\nLION-FS adopts a two-stage optimization strategy: 1)Fast Path: Routing-Based\nResponse Determination evaluates frame-by-frame whether an immediate response\nis necessary. To enhance response determination accuracy and handle higher\nframe-rate inputs efficiently, we employ Token Aggregation Routing to\ndynamically fuse spatiotemporal features without increasing token numbers,\nwhile utilizing Token Dropping Routing to eliminate redundant features. 2)Slow\nPath: Multi-granularity Keyframe Augmentation optimizes keyframes during\nresponse generation. To provide comprehensive and detailed responses beyond\natomic actions constrained by training data, fine-grained spatial features and\nhuman-environment interaction features are extracted through multi-granular\npooling. These features are further integrated into a meticulously designed\nmultimodal Thinking Template to guide more precise response generation.\nComprehensive evaluations on online video tasks demonstrate that LION-FS\nachieves state-of-the-art efficacy and efficiency.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03651v1",
    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in\n  Multimodal Cycles",
    "url": "http://arxiv.org/abs/2503.03651v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03651v1",
    "authors": [
      "Rui Zhao",
      "Weijia Mao",
      "Mike Zheng Shou"
    ],
    "date": "2025-03-05",
    "summary": "Adapting generative models to specific domains presents an effective solution\nfor satisfying specialized requirements. However, adapting to some complex\ndomains remains challenging, especially when these domains require substantial\npaired data to capture the targeted distributions. Since unpaired data from a\nsingle modality, such as vision or language, is more readily available, we\nutilize the bidirectional mappings between vision and language learned by the\nunified generative model to enable training on unpaired data for domain\nadaptation. Specifically, we propose DoraCycle, which integrates two multimodal\ncycles: text-to-image-to-text and image-to-text-to-image. The model is\noptimized through cross-entropy loss computed at the cycle endpoints, where\nboth endpoints share the same modality. This facilitates self-evolution of the\nmodel without reliance on annotated text-image pairs. Experimental results\ndemonstrate that for tasks independent of paired knowledge, such as\nstylization, DoraCycle can effectively adapt the unified model using only\nunpaired data. For tasks involving new paired knowledge, such as specific\nidentities, a combination of a small set of paired image-text examples and\nlarger-scale unpaired data is sufficient for effective domain-oriented\nadaptation. The code will be released at https://github.com/showlab/DoraCycle.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "text-to-image",
      "image-to-text"
    ]
  },
  {
    "id": "2503_03644v2",
    "title": "DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating\n  Semantic Understanding of Dongba Pictograms",
    "url": "http://arxiv.org/abs/2503.03644v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03644v2",
    "authors": [
      "Xiaojun Bi",
      "Shuo Li",
      "Ziyue Wang",
      "Fuwen Luo",
      "Weizheng Qiao",
      "Lu Han",
      "Ziwei Sun",
      "Peng Li",
      "Yang Liu"
    ],
    "date": "2025-03-05",
    "summary": "Dongba pictographs are the only pictographs still in use in the world. They\nhave pictorial ideographic features, and their symbols carry rich cultural and\ncontextual information. Due to the lack of relevant datasets, existing research\nhas difficulty in advancing the study of semantic understanding of Dongba\npictographs. To this end, we propose DongbaMIE, the first multimodal dataset\nfor semantic understanding and extraction of Dongba pictographs. The dataset\nconsists of Dongba pictograph images and their corresponding Chinese semantic\nannotations. It contains 23,530 sentence-level and 2,539 paragraph-level\nimages, covering four semantic dimensions: objects, actions, relations, and\nattributes. We systematically evaluate the GPT-4o, Gemini-2.0, and Qwen2-VL\nmodels. Experimental results show that the F1 scores of GPT-4o and Gemini in\nthe best object extraction are only 3.16 and 3.11 respectively. The F1 score of\nQwen2-VL after supervised fine-tuning is only 11.49. These results suggest that\ncurrent large multimodal models still face significant challenges in accurately\nrecognizing the diverse semantic information in Dongba pictographs. The dataset\ncan be obtained from this URL.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03613v1",
    "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards\n  Zero-shot Adversarial Robustness of CLIP",
    "url": "http://arxiv.org/abs/2503.03613v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03613v1",
    "authors": [
      "Songlong Xing",
      "Zhengyu Zhao",
      "Nicu Sebe"
    ],
    "date": "2025-03-05",
    "summary": "Despite its prevalent use in image-text matching tasks in a zero-shot manner,\nCLIP has been shown to be highly vulnerable to adversarial perturbations added\nonto images. Recent studies propose to finetune the vision encoder of CLIP with\nadversarial samples generated on the fly, and show improved robustness against\nadversarial attacks on a spectrum of downstream datasets, a property termed as\nzero-shot robustness. In this paper, we show that malicious perturbations that\nseek to maximise the classification loss lead to `falsely stable' images, and\npropose to leverage the pre-trained vision encoder of CLIP to counterattack\nsuch adversarial images during inference to achieve robustness. Our paradigm is\nsimple and training-free, providing the first method to defend CLIP from\nadversarial attacks at test time, which is orthogonal to existing methods\naiming to boost zero-shot adversarial robustness of CLIP. We conduct\nexperiments across 16 classification datasets, and demonstrate stable and\nconsistent gains compared to test-time defence methods adapted from existing\nadversarial robustness studies that do not rely on external networks, without\nnoticeably impairing performance on clean images. We also show that our\nparadigm can be employed on CLIP models that have been adversarially finetuned\nto further enhance their robustness at test time. Our code is available\n\\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image-text",
      "CLIP"
    ]
  },
  {
    "id": "2503_03579v1",
    "title": "A Generative System for Robot-to-Human Handovers: from Intent Inference\n  to Spatial Configuration Imagery",
    "url": "http://arxiv.org/abs/2503.03579v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03579v1",
    "authors": [
      "Hanxin Zhang",
      "Abdulqader Dhafer",
      "Zhou Daniel Hao",
      "Hongbiao Dong"
    ],
    "date": "2025-03-05",
    "summary": "We propose a novel system for robot-to-human object handover that emulates\nhuman coworker interactions. Unlike most existing studies that focus primarily\non grasping strategies and motion planning, our system focus on 1. inferring\nhuman handover intents, 2. imagining spatial handover configuration. The first\none integrates multimodal perception-combining visual and verbal cues-to infer\nhuman intent. The second one using a diffusion-based model to generate the\nhandover configuration, involving the spacial relationship among robot's\ngripper, the object, and the human hand, thereby mimicking the cognitive\nprocess of motor imagery. Experimental results demonstrate that our approach\neffectively interprets human cues and achieves fluent, human-like handovers,\noffering a promising solution for collaborative robotics. Code, videos, and\ndata are available at: https://i3handover.github.io.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03507v1",
    "title": "Mineral segmentation using electron microscope images and spectral\n  sampling through multimodal graph neural networks",
    "url": "http://arxiv.org/abs/2503.03507v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03507v1",
    "authors": [
      "Samuel Repka",
      "Bo\u0159ek Reich",
      "Fedor Zolotarev",
      "Tuomas Eerola",
      "Pavel Zem\u010d\u00edk"
    ],
    "date": "2025-03-05",
    "summary": "We propose a novel Graph Neural Network-based method for segmentation based\non data fusion of multimodal Scanning Electron Microscope (SEM) images. In most\ncases, Backscattered Electron (BSE) images obtained using SEM do not contain\nsufficient information for mineral segmentation. Therefore, imaging is often\ncomplemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS)\nspectral measurements that provide highly accurate information about the\nchemical composition but that are time-consuming to acquire. This motivates the\nuse of sparse spectral data in conjunction with BSE images for mineral\nsegmentation. The unstructured nature of the spectral data makes most\ntraditional image fusion techniques unsuitable for BSE-EDS fusion. We propose\nusing graph neural networks to fuse the two modalities and segment the mineral\nphases simultaneously. Our results demonstrate that providing EDS data for as\nfew as 1% of BSE pixels produces accurate segmentation, enabling rapid analysis\nof mineral samples. The proposed data fusion pipeline is versatile and can be\nadapted to other domains that involve image data and point-wise measurements.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03465v2",
    "title": "DTU-Net: A Multi-Scale Dilated Transformer Network for Nonlinear\n  Hyperspectral Unmixing",
    "url": "http://arxiv.org/abs/2503.03465v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03465v2",
    "authors": [
      "ChenTong Wang",
      "Jincheng Gao",
      "Fei Zhu",
      "Abderrahim Halimi",
      "C\u00e9dric Richard"
    ],
    "date": "2025-03-05",
    "summary": "Transformers have shown significant success in hyperspectral unmixing (HU).\nHowever, challenges remain. While multi-scale and long-range spatial\ncorrelations are essential in unmixing tasks, current Transformer-based\nunmixing networks, built on Vision Transformer (ViT) or Swin-Transformer,\nstruggle to capture them effectively. Additionally, current Transformer-based\nunmixing networks rely on the linear mixing model, which lacks the flexibility\nto accommodate scenarios where nonlinear effects are significant. To address\nthese limitations, we propose a multi-scale Dilated Transformer-based unmixing\nnetwork for nonlinear HU (DTU-Net). The encoder employs two branches. The first\none performs multi-scale spatial feature extraction using Multi-Scale Dilated\nAttention (MSDA) in the Dilated Transformer, which varies dilation rates across\nattention heads to capture long-range and multi-scale spatial correlations. The\nsecond one performs spectral feature extraction utilizing 3D-CNNs with channel\nattention. The outputs from both branches are then fused to integrate\nmulti-scale spatial and spectral information, which is subsequently transformed\nto estimate the abundances. The decoder is designed to accommodate both linear\nand nonlinear mixing scenarios. Its interpretability is enhanced by explicitly\nmodeling the relationships between endmembers, abundances, and nonlinear\ncoefficients in accordance with the polynomial post-nonlinear mixing model\n(PPNMM). Experiments on synthetic and real datasets validate the effectiveness\nof the proposed DTU-Net compared to PPNMM-derived methods and several advanced\nunmixing networks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_03335v1",
    "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective\n  Responses to News",
    "url": "http://arxiv.org/abs/2503.03335v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03335v1",
    "authors": [
      "Tiancheng Hu",
      "Nigel Collier"
    ],
    "date": "2025-03-05",
    "summary": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03321v1",
    "title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
    "url": "http://arxiv.org/abs/2503.03321v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03321v1",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ],
    "date": "2025-03-05",
    "summary": "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_03285v2",
    "title": "Enhancing Vietnamese VQA through Curriculum Learning on Raw and\n  Augmented Text Representations",
    "url": "http://arxiv.org/abs/2503.03285v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03285v2",
    "authors": [
      "Khoi Anh Nguyen",
      "Linh Yen Vu",
      "Thang Dinh Duong",
      "Thuan Nguyen Duong",
      "Huy Thanh Nguyen",
      "Vinh Quang Dinh"
    ],
    "date": "2025-03-05",
    "summary": "Visual Question Answering (VQA) is a multimodal task requiring reasoning\nacross textual and visual inputs, which becomes particularly challenging in\nlow-resource languages like Vietnamese due to linguistic variability and the\nlack of high-quality datasets. Traditional methods often rely heavily on\nextensive annotated datasets, computationally expensive pipelines, and large\npre-trained models, specifically in the domain of Vietnamese VQA, limiting\ntheir applicability in such scenarios. To address these limitations, we propose\na training framework that combines a paraphrase-based feature augmentation\nmodule with a dynamic curriculum learning strategy. Explicitly, augmented\nsamples are considered \"easy\" while raw samples are regarded as \"hard\". The\nframework then utilizes a mechanism that dynamically adjusts the ratio of easy\nto hard samples during training, progressively modifying the same dataset to\nincrease its difficulty level. By enabling gradual adaptation to task\ncomplexity, this approach helps the Vietnamese VQA model generalize well, thus\nimproving overall performance. Experimental results show consistent\nimprovements on the OpenViVQA dataset and mixed outcomes on the ViVQA dataset,\nhighlighting both the potential and challenges of our approach in advancing VQA\nfor Vietnamese language.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA"
    ]
  },
  {
    "id": "2503_03280v1",
    "title": "BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation",
    "url": "http://arxiv.org/abs/2503.03280v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03280v1",
    "authors": [
      "Hiep Truong Cong",
      "Ajay Kumar Sigatapu",
      "Arindam Das",
      "Yashwanth Sharma",
      "Venkatesh Satagopan",
      "Ganesh Sistu",
      "Ciaran Eising"
    ],
    "date": "2025-03-05",
    "summary": "Accurate motion understanding of the dynamic objects within the scene in\nbird's-eye-view (BEV) is critical to ensure a reliable obstacle avoidance\nsystem and smooth path planning for autonomous vehicles. However, this task has\nreceived relatively limited exploration when compared to object detection and\nsegmentation with only a few recent vision-based approaches presenting\npreliminary findings that significantly deteriorate in low-light, nighttime,\nand adverse weather conditions such as rain. Conversely, LiDAR and radar\nsensors remain almost unaffected in these scenarios, and radar provides key\nvelocity information of the objects. Therefore, we introduce BEVMOSNet, to our\nknowledge, the first end-to-end multimodal fusion leveraging cameras, LiDAR,\nand radar to precisely predict the moving objects in BEV. In addition, we\nperform a deeper analysis to find out the optimal strategy for deformable\ncross-attention-guided sensor fusion for cross-sensor knowledge sharing in BEV.\nWhile evaluating BEVMOSNet on the nuScenes dataset, we show an overall\nimprovement in IoU score of 36.59% compared to the vision-based unimodal\nbaseline BEV-MoSeg (Sigatapu et al., 2023), and 2.35% compared to the\nmultimodel SimpleBEV (Harley et al., 2022), extended for the motion\nsegmentation task, establishing this method as the state-of-the-art in BEV\nmotion segmentation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03278v1",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with\n  Knowledge Descriptions",
    "url": "http://arxiv.org/abs/2503.03278v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03278v1",
    "authors": [
      "Jun Li",
      "Che Liu",
      "Wenjia Bai",
      "Rossella Arcucci",
      "Cosmin I. Bercea",
      "Julia A. Schnabel"
    ],
    "date": "2025-03-05",
    "summary": "Visual Language Models (VLMs) have demonstrated impressive capabilities in\nvisual grounding tasks. However, their effectiveness in the medical domain,\nparticularly for abnormality detection and localization within medical images,\nremains underexplored. A major challenge is the complex and abstract nature of\nmedical terminology, which makes it difficult to directly associate\npathological anomaly terms with their corresponding visual features. In this\nwork, we introduce a novel approach to enhance VLM performance in medical\nabnormality detection and localization by leveraging decomposed medical\nknowledge. Instead of directly prompting models to recognize specific\nabnormalities, we focus on breaking down medical concepts into fundamental\nattributes and common visual patterns. This strategy promotes a stronger\nalignment between textual descriptions and visual features, improving both the\nrecognition and localization of abnormalities in medical images.We evaluate our\nmethod on the 0.23B Florence-2 base model and demonstrate that it achieves\ncomparable performance in abnormality grounding to significantly larger 7B\nLLaVA-based medical VLMs, despite being trained on only 1.5% of the data used\nfor such models. Experimental results also demonstrate the effectiveness of our\napproach in both known and previously unseen abnormalities, suggesting its\nstrong generalization capabilities.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_03792v1",
    "title": "Rebalanced Multimodal Learning with Data-aware Unimodal Sampling",
    "url": "http://arxiv.org/abs/2503.03792v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03792v1",
    "authors": [
      "Qingyuan Jiang",
      "Zhouyang Chi",
      "Xiao Ma",
      "Qirong Mao",
      "Yang Yang",
      "Jinhui Tang"
    ],
    "date": "2025-03-05",
    "summary": "To address the modality learning degeneration caused by modality imbalance,\nexisting multimodal learning~(MML) approaches primarily attempt to balance the\noptimization process of each modality from the perspective of model learning.\nHowever, almost all existing methods ignore the modality imbalance caused by\nunimodal data sampling, i.e., equal unimodal data sampling often results in\ndiscrepancies in informational content, leading to modality imbalance.\nTherefore, in this paper, we propose a novel MML approach called\n\\underline{D}ata-aware \\underline{U}nimodal \\underline{S}ampling~(\\method),\nwhich aims to dynamically alleviate the modality imbalance caused by sampling.\nSpecifically, we first propose a novel cumulative modality discrepancy to\nmonitor the multimodal learning process. Based on the learning status, we\npropose a heuristic and a reinforcement learning~(RL)-based data-aware unimodal\nsampling approaches to adaptively determine the quantity of sampled data at\neach iteration, thus alleviating the modality imbalance from the perspective of\nsampling. Meanwhile, our method can be seamlessly incorporated into almost all\nexisting multimodal learning approaches as a plugin. Experiments demonstrate\nthat \\method~can achieve the best performance by comparing with diverse\nstate-of-the-art~(SOTA) baselines.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03244v1",
    "title": "Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection\n  in Neonatal Care",
    "url": "http://arxiv.org/abs/2503.03244v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03244v1",
    "authors": [
      "Jorge Garc\u00eda-Torres",
      "\u00d8yvind Meinich-Bache",
      "Sara Brunner",
      "Siren Rettedal",
      "Vilde Kolstad",
      "Kjersti Engan"
    ],
    "date": "2025-03-05",
    "summary": "Around 10% of newborns require some help to initiate breathing, and 5\\% need\nventilation assistance. Accurate Time of Birth (ToB) documentation is essential\nfor optimizing neonatal care, as timely interventions are vital for proper\nresuscitation. However, current clinical methods for recording ToB often rely\non manual processes, which can be prone to inaccuracies. In this study, we\npresent a novel two-stream fusion system that combines the power of image and\nvideo analysis to accurately detect the ToB from thermal recordings in the\ndelivery room and operating theater. By integrating static and dynamic streams,\nour approach captures richer birth-related spatiotemporal features, leading to\nmore robust and precise ToB estimation. We demonstrate that this synergy\nbetween data modalities enhances performance over single-stream approaches. Our\nsystem achieves 95.7% precision and 84.8% recall in detecting birth within\nshort video clips. Additionally, with the help of a score aggregation module,\nit successfully identifies ToB in 100% of test cases, with a median absolute\nerror of 2 seconds and an absolute mean deviation of 4.5 seconds compared to\nmanual annotations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_03215v1",
    "title": "COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open\n  Source Intelligence",
    "url": "http://arxiv.org/abs/2503.03215v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03215v1",
    "authors": [
      "Wentao Li",
      "Congcong Wang",
      "Xiaoxiao Cui",
      "Zhi Liu",
      "Wei Guo",
      "Lizhen Cui"
    ],
    "date": "2025-03-05",
    "summary": "Open Source Intelligence (OSINT) requires the integration and reasoning of\ndiverse multimodal data, presenting significant challenges in deriving\nactionable insights. Traditional approaches, including multimodal large\nlanguage models (MLLMs), often struggle to infer complex contextual\nrelationships or deliver comprehensive intelligence from unstructured data\nsources. In this paper, we introduce COSINT-Agent, a knowledge-driven\nmultimodal agent tailored to address the challenges of OSINT in the Chinese\ndomain. COSINT-Agent seamlessly integrates the perceptual capabilities of\nfine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene\nKnowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match\nframework, which bridges COSINT-MLLM and EES-KG, enabling systematic\nextraction, reasoning, and contextualization of multimodal insights. This\nintegration facilitates precise entity recognition, event interpretation, and\ncontext retrieval, effectively transforming raw multimodal data into actionable\nintelligence. Extensive experiments validate the superior performance of\nCOSINT-Agent across core OSINT tasks, including entity recognition, EES\ngeneration, and context matching. These results underscore its potential as a\nrobust and scalable solution for advancing automated multimodal reasoning and\nenhancing the effectiveness of OSINT methodologies.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03202v1",
    "title": "Variance-Aware Loss Scheduling for Multimodal Alignment in Low-Data\n  Settings",
    "url": "http://arxiv.org/abs/2503.03202v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03202v1",
    "authors": [
      "Sneh Pillai"
    ],
    "date": "2025-03-05",
    "summary": "Training vision-language models for image-text alignment typically requires\nlarge datasets to achieve robust performance. In low-data scenarios, standard\ncontrastive learning can struggle to align modalities effectively due to\noverfitting and unstable training dynamics. In this paper, we propose a\nvariance-aware loss scheduling approach that dynamically adjusts the weighting\nof the contrastive loss based on the statistical variability (uncertainty) in\nthe model's alignment predictions. Using a subset of the Flickr8k image-caption\ndataset to simulate limited data conditions, we demonstrate that our approach\nimproves image-text retrieval accuracy compared to a fixed-weight baseline. We\nalso compare against other adaptive weighting strategies (using output entropy\nand cosine similarity spread) and find that variance-aware scheduling provides\nthe best overall trade-off. Qualitatively, our method yields more distinct\nmultimodal embeddings as shown by t-SNE visualizations. Moreover, in a stress\ntest with noise-injected captions and images, the variance-guided loss proves\nmore robust, maintaining higher recall when random perturbations are\nintroduced. These results highlight the benefit of adaptive loss weighting for\nmultimodal alignment in low-data regimes.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language",
      "image-text"
    ]
  },
  {
    "id": "2503_03196v1",
    "title": "SpiritSight Agent: Advanced GUI Agent with One Look",
    "url": "http://arxiv.org/abs/2503.03196v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03196v1",
    "authors": [
      "Zhiyuan Huang",
      "Ziming Cheng",
      "Junting Pan",
      "Zhaohui Hou",
      "Mingjie Zhan"
    ],
    "date": "2025-03-05",
    "summary": "Graphical User Interface (GUI) agents show amazing abilities in assisting\nhuman-computer interaction, automating human user's navigation on digital\ndevices. An ideal GUI agent is expected to achieve high accuracy, low latency,\nand compatibility for different GUI platforms. Recent vision-based approaches\nhave shown promise by leveraging advanced Vision Language Models (VLMs). While\nthey generally meet the requirements of compatibility and low latency, these\nvision-based GUI agents tend to have low accuracy due to their limitations in\nelement grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a\nvision-based, end-to-end GUI agent that excels in GUI navigation tasks across\nvarious GUI platforms. First, we create a multi-level, large-scale,\nhigh-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods,\nempowering SpiritSight with robust GUI understanding and grounding\ncapabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$\nmethod to resolve the ambiguity problem in dynamic high-resolution of visual\ninputs, further enhancing SpiritSight's ability to ground GUI objects. Through\nthese efforts, SpiritSight agent outperforms other advanced methods on diverse\nGUI benchmarks, demonstrating its superior capability and compatibility in GUI\nnavigation tasks. Models are available at\n$\\href{https://huggingface.co/SenseLLM/SpiritSight-Agent-8B}{this\\ URL}$.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_03190v2",
    "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
    "url": "http://arxiv.org/abs/2503.03190v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03190v2",
    "authors": [
      "Jingzhou Luo",
      "Yang Liu",
      "Weixing Chen",
      "Zhen Li",
      "Yaowei Wang",
      "Guanbin Li",
      "Liang Lin"
    ],
    "date": "2025-03-05",
    "summary": "3D Question Answering (3D QA) requires the model to comprehensively\nunderstand its situated 3D scene described by the text, then reason about its\nsurrounding environment and answer a question under that situation. However,\nexisting methods usually rely on global scene perception from pure 3D point\nclouds and overlook the importance of rich local texture details from\nmulti-view images. Moreover, due to the inherent noise in camera poses and\ncomplex occlusions, there exists significant feature degradation and reduced\nfeature robustness problems when aligning 3D point cloud with multi-view\nimages. In this paper, we propose a Dual-vision Scene Perception Network\n(DSPNet), to comprehensively integrate multi-view and point cloud features to\nimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) module\nprioritizes image views that closely match the semantic content of the text. To\nadaptively fuse back-projected multi-view images with point cloud features, we\ndesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scene\ncomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)\nmodule facilitates robust reasoning by integrating contextual information\nacross visual and linguistic modalities. Experimental results on SQA3D and\nScanQA datasets demonstrate the superiority of our DSPNet. Codes will be\navailable at https://github.com/LZ-CH/DSPNet.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03122v1",
    "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models",
    "url": "http://arxiv.org/abs/2503.03122v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03122v1",
    "authors": [
      "Zichao Li",
      "Xueru Wen",
      "Jie Lou",
      "Yuqiu Ji",
      "Yaojie Lu",
      "Xianpei Han",
      "Debing Zhang",
      "Le Sun"
    ],
    "date": "2025-03-05",
    "summary": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03112v1",
    "title": "A Multimodal Framework for Topic Propagation Classification in Social\n  Networks",
    "url": "http://arxiv.org/abs/2503.03112v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03112v1",
    "authors": [
      "Yuchuan Jiang",
      "Chaolong Jia",
      "Yunyi Qin",
      "Wei Cai",
      "Yongsen Qian"
    ],
    "date": "2025-03-05",
    "summary": "The rapid proliferation of the Internet and the widespread adoption of social\nnetworks have significantly accelerated information dissemination. However,\nthis transformation has introduced complexities in information capture and\nprocessing, posing substantial challenges for researchers and practitioners.\nPredicting the dissemination of topic-related information within social\nnetworks has thus become a critical research focus. This paper proposes a\npredictive model for topic dissemination in social networks by integrating\nmultidimensional features derived from key dissemination characteristics.\nSpecifically, we introduce two novel indicators, user relationship breadth and\nuser authority, into the PageRank algorithm to quantify user influence more\neffectively. Additionally, we employ a Text-CNN model for sentiment\nclassification, extracting sentiment features from textual content. Temporal\nembeddings of nodes are encoded using a Bi-LSTM model to capture temporal\ndynamics. Furthermore, we refine the measurement of user interaction traces\nwith topics, replacing traditional topic view metrics with a more precise\ncommunication characteristics measure. Finally, we integrate the extracted\nmultidimensional features using a Transformer model, significantly enhancing\npredictive performance. Experimental results demonstrate that our proposed\nmodel outperforms traditional machine learning and unimodal deep learning\nmodels in terms of FI-Score, AUC, and Recall, validating its effectiveness in\npredicting topic propagation within social networks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03107v1",
    "title": "External Reliable Information-enhanced Multimodal Contrastive Learning\n  for Fake News Detection",
    "url": "http://arxiv.org/abs/2503.03107v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03107v1",
    "authors": [
      "Biwei Cao",
      "Qihang Wu",
      "Jiuxin Cao",
      "Bo Liu",
      "Jie Gui"
    ],
    "date": "2025-03-05",
    "summary": "With the rapid development of the Internet, the information dissemination\nparadigm has changed and the efficiency has been improved greatly. While this\nalso brings the quick spread of fake news and leads to negative impacts on\ncyberspace. Currently, the information presentation formats have evolved\ngradually, with the news formats shifting from texts to multimodal contents. As\na result, detecting multimodal fake news has become one of the research\nhotspots. However, multimodal fake news detection research field still faces\ntwo main challenges: the inability to fully and effectively utilize multimodal\ninformation for detection, and the low credibility or static nature of the\nintroduced external information, which limits dynamic updates. To bridge the\ngaps, we propose ERIC-FND, an external reliable information-enhanced multimodal\ncontrastive learning framework for fake news detection. ERIC-FND strengthens\nthe representation of news contents by entity-enriched external information\nenhancement method. It also enriches the multimodal news information via\nmultimodal semantic interaction method where the multimodal constrative\nlearning is employed to make different modality representations learn from each\nother. Moreover, an adaptive fusion method is taken to integrate the news\nrepresentations from different dimensions for the eventual classification.\nExperiments are done on two commonly used datasets in different languages, X\n(Twitter) and Weibo. Experiment results demonstrate that our proposed model\nERIC-FND outperforms existing state-of-the-art fake news detection methods\nunder the same settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02950v1",
    "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
    "url": "http://arxiv.org/abs/2503.02950v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02950v1",
    "authors": [
      "Danqing Zhang",
      "Balaji Rama",
      "Jingyi Ni",
      "Shiying He",
      "Fu Zhao",
      "Kunyu Chen",
      "Arnold Chen",
      "Junyu Cao"
    ],
    "date": "2025-03-04",
    "summary": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ]
  },
  {
    "id": "2503_02876v1",
    "title": "SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and\n  Baseline Models",
    "url": "http://arxiv.org/abs/2503.02876v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02876v1",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "date": "2025-03-04",
    "summary": "Advancing AI in computational pathology requires large, high-quality, and\ndiverse datasets, yet existing public datasets are often limited in organ\ndiversity, class coverage, or annotation quality. To bridge this gap, we\nintroduce SPIDER (Supervised Pathology Image-DEscription Repository), the\nlargest publicly available patch-level dataset covering multiple organ types,\nincluding Skin, Colorectal, and Thorax, with comprehensive class coverage for\neach organ. SPIDER provides high-quality annotations verified by expert\npathologists and includes surrounding context patches, which enhance\nclassification performance by providing spatial context.\n  Alongside the dataset, we present baseline models trained on SPIDER using the\nHibou-L foundation model as a feature extractor combined with an\nattention-based classification head. The models achieve state-of-the-art\nperformance across multiple tissue categories and serve as strong benchmarks\nfor future digital pathology research. Beyond patch classification, the model\nenables rapid identification of significant areas, quantitative tissue metrics,\nand establishes a foundation for multimodal approaches.\n  Both the dataset and trained models are publicly available to advance\nresearch, reproducibility, and AI-driven pathology development. Access them at:\nhttps://github.com/HistAI/SPIDER",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02865v2",
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "url": "http://arxiv.org/abs/2503.02865v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02865v2",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "date": "2025-03-04",
    "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_02861v1",
    "title": "Evaluation of Architectural Synthesis Using Generative AI",
    "url": "http://arxiv.org/abs/2503.02861v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02861v1",
    "authors": [
      "Jingfei Huang",
      "Alexandros Haridis"
    ],
    "date": "2025-03-04",
    "summary": "Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02853v1",
    "title": "CADDI: An in-Class Activity Detection Dataset using IMU data from\n  low-cost sensors",
    "url": "http://arxiv.org/abs/2503.02853v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02853v1",
    "authors": [
      "Luis Marquez-Carpintero",
      "Sergio Suescun-Ferrandiz",
      "Monica Pina-Navarro",
      "Miguel Cazorla",
      "Francisco Gomez-Donoso"
    ],
    "date": "2025-03-04",
    "summary": "The monitoring and prediction of in-class student activities is of paramount\nimportance for the comprehension of engagement and the enhancement of\npedagogical efficacy. The accurate detection of these activities enables\neducators to modify their lessons in real time, thereby reducing negative\nemotional states and enhancing the overall learning experience. To this end,\nthe use of non-intrusive devices, such as inertial measurement units (IMUs)\nembedded in smartwatches, represents a viable solution. The development of\nreliable predictive systems has been limited by the lack of large, labeled\ndatasets in education. To bridge this gap, we present a novel dataset for\nin-class activity detection using affordable IMU sensors. The dataset comprises\n19 diverse activities, both instantaneous and continuous, performed by 12\nparticipants in typical classroom scenarios. It includes accelerometer,\ngyroscope, rotation vector data, and synchronized stereo images, offering a\ncomprehensive resource for developing multimodal algorithms using sensor and\nvisual data. This dataset represents a key step toward scalable solutions for\nactivity recognition in educational settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02849v1",
    "title": "Multimodal Deep Learning for Subtype Classification in Breast Cancer\n  Using Histopathological Images and Gene Expression Data",
    "url": "http://arxiv.org/abs/2503.02849v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02849v1",
    "authors": [
      "Amin Honarmandi Shandiz"
    ],
    "date": "2025-03-04",
    "summary": "Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02824v1",
    "title": "Developing a PET/CT Foundation Model for Cross-Modal Anatomical and\n  Functional Imaging",
    "url": "http://arxiv.org/abs/2503.02824v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02824v1",
    "authors": [
      "Yujin Oh",
      "Robert Seifert",
      "Yihan Cao",
      "Christoph Clement",
      "Justin Ferdinandus",
      "Constantin Lapa",
      "Alessandro Liebich",
      "Michelle Amon",
      "Johanna Enke",
      "Sifan Song",
      "Runqi Meng",
      "Fang Zeng",
      "Ning Guo",
      "Xiang Li",
      "Pedram Heidari",
      "Axel Rominger",
      "Kuangyu Shi",
      "Quanzheng Li"
    ],
    "date": "2025-03-04",
    "summary": "In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision transformer"
    ]
  },
  {
    "id": "2503_02823v1",
    "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
    "url": "http://arxiv.org/abs/2503.02823v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
    "authors": [
      "Matteo Spanio",
      "Massimiliano Zampini",
      "Antonio Rod\u00e0",
      "Franco Pierucci"
    ],
    "date": "2025-03-04",
    "summary": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02800v2",
    "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
    "url": "http://arxiv.org/abs/2503.02800v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02800v2",
    "authors": [
      "Alicia Russell-Gilbert",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Maria Seale",
      "Joseph Jabour",
      "Thomas Arnold",
      "Joshua Church"
    ],
    "date": "2025-03-04",
    "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02798v1",
    "title": "Spike-and-Slab Posterior Sampling in High Dimensions",
    "url": "http://arxiv.org/abs/2503.02798v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02798v1",
    "authors": [
      "Syamantak Kumar",
      "Purnamrita Sarkar",
      "Kevin Tian",
      "Yusong Zhu"
    ],
    "date": "2025-03-04",
    "summary": "Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal\ndistribution used to model uncertainty in variable selection, is considered the\ntheoretical gold standard method for Bayesian sparse linear regression [CPS09,\nRoc18]. However, designing provable algorithms for performing this sampling\ntask is notoriously challenging. Existing posterior samplers for Bayesian\nsparse variable selection tasks either require strong assumptions about the\nsignal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows\nat least linearly in the dimension [MW24], or rely on heuristic approximations\nto the posterior. We give the first provable algorithms for spike-and-slab\nposterior sampling that apply for any SNR, and use a measurement count\nsublinear in the problem dimension. Concretely, assume we are given a\nmeasurement matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ and noisy\nobservations $\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\star + \\mathbf{\\xi}$ of a\nsignal $\\mathbf{\\theta}^\\star$ drawn from a spike-and-slab prior $\\pi$ with a\nGaussian diffuse density and expected sparsity k, where $\\mathbf{\\xi} \\sim\n\\mathcal{N}(\\mathbb{0}_n, \\sigma^2\\mathbf{I}_n)$. We give a polynomial-time\nhigh-accuracy sampler for the posterior $\\pi(\\cdot \\mid \\mathbf{X},\n\\mathbf{y})$, for any SNR $\\sigma^{-1}$ > 0, as long as $n \\geq k^3 \\cdot\n\\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the\nrestricted isometry property. We further give a sampler that runs in\nnear-linear time $\\approx nd$ in the same setting, as long as $n \\geq k^5 \\cdot\n\\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend\nour result to spike-and-slab posterior sampling with Laplace diffuse densities,\nachieving similar guarantees when $\\sigma = O(\\frac{1}{k})$ is bounded.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02781v1",
    "title": "Multimodal AI predicts clinical outcomes of drug combinations from\n  preclinical data",
    "url": "http://arxiv.org/abs/2503.02781v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02781v1",
    "authors": [
      "Yepeng Huang",
      "Xiaorui Su",
      "Varun Ullanat",
      "Ivy Liang",
      "Lindsay Clegg",
      "Damilola Olabode",
      "Nicholas Ho",
      "Bino John",
      "Megan Gibbs",
      "Marinka Zitnik"
    ],
    "date": "2025-03-04",
    "summary": "Predicting clinical outcomes from preclinical data is essential for\nidentifying safe and effective drug combinations. Current models rely on\nstructural or target-based features to identify high-efficacy, low-toxicity\ndrug combinations. However, these approaches fail to incorporate the multimodal\ndata necessary for accurate, clinically-relevant predictions. Here, we\nintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,\ncell viability, and transcriptomic data to predict drug combination effects\nacross 953 clinical outcomes and 21842 compounds, including combinations of\napproved drugs and novel compounds in development. MADRIGAL uses a transformer\nbottleneck module to unify preclinical drug data modalities while handling\nmissing data during training and inference--a major challenge in multimodal\nlearning. It outperforms single-modality methods and state-of-the-art models in\npredicting adverse drug interactions. MADRIGAL performs virtual screening of\nanticancer drug combinations and supports polypharmacy management for type II\ndiabetes and metabolic dysfunction-associated steatohepatitis (MASH). It\nidentifies transporter-mediated drug interactions. MADRIGAL predicts\nresmetirom, the first and only FDA-approved drug for MASH, among therapies with\nthe most favorable safety profile. It supports personalized cancer therapy by\nintegrating genomic profiles from cancer patients. Using primary acute myeloid\nleukemia samples and patient-derived xenograft models, it predicts the efficacy\nof personalized drug combinations. Integrating MADRIGAL with a large language\nmodel allows users to describe clinical outcomes in natural language, improving\nsafety assessment by identifying potential adverse interactions and toxicity\nrisks. MADRIGAL provides a multimodal approach for designing combination\ntherapies with improved predictive accuracy and clinical relevance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02733v1",
    "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural\n  Video Compression",
    "url": "http://arxiv.org/abs/2503.02733v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02733v1",
    "authors": [
      "Jia Wang",
      "Xinfeng Zhang",
      "Gai Zhang",
      "Jun Zhu",
      "Lv Tang",
      "Li Zhang"
    ],
    "date": "2025-03-04",
    "summary": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_02616v1",
    "title": "Smoothing the Shift: Towards Stable Test-Time Adaptation under Complex\n  Multimodal Noises",
    "url": "http://arxiv.org/abs/2503.02616v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02616v1",
    "authors": [
      "Zirun Guo",
      "Tao Jin"
    ],
    "date": "2025-03-04",
    "summary": "Test-Time Adaptation (TTA) aims to tackle distribution shifts using unlabeled\ntest data without access to the source data. In the context of multimodal data,\nthere are more complex noise patterns than unimodal data such as simultaneous\ncorruptions for multiple modalities and missing modalities. Besides, in\nreal-world applications, corruptions from different distribution shifts are\nalways mixed. Existing TTA methods always fail in such multimodal scenario\nbecause the abrupt distribution shifts will destroy the prior knowledge from\nthe source model, thus leading to performance degradation. To this end, we\nreveal a new challenge named multimodal wild TTA. To address this challenging\nproblem, we propose two novel strategies: sample identification with\ninterquartile range Smoothing and unimodal assistance, and Mutual information\nsharing (SuMi). SuMi smooths the adaptation process by interquartile range\nwhich avoids the abrupt distribution shifts. Then, SuMi fully utilizes the\nunimodal features to select low-entropy samples with rich multimodal\ninformation for optimization. Furthermore, mutual information sharing is\nintroduced to align the information, reduce the discrepancies and enhance the\ninformation utilization across different modalities. Extensive experiments on\ntwo public datasets show the effectiveness and superiority over existing\nmethods under the complex noise patterns in multimodal data. Code is available\nat https://github.com/zrguo/SuMi.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02600v1",
    "title": "Resource-Efficient Affordance Grounding with Complementary Depth and\n  Semantic Prompts",
    "url": "http://arxiv.org/abs/2503.02600v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02600v1",
    "authors": [
      "Yizhou Huang",
      "Fan Yang",
      "Guoliang Zhu",
      "Gen Li",
      "Hao Shi",
      "Yukun Zuo",
      "Wenrui Chen",
      "Zhiyong Li",
      "Kailun Yang"
    ],
    "date": "2025-03-04",
    "summary": "Affordance refers to the functional properties that an agent perceives and\nutilizes from its environment, and is key perceptual information required for\nrobots to perform actions. This information is rich and multimodal in nature.\nExisting multimodal affordance methods face limitations in extracting useful\ninformation, mainly due to simple structural designs, basic fusion methods, and\nlarge model parameters, making it difficult to meet the performance\nrequirements for practical deployment. To address these issues, this paper\nproposes the BiT-Align image-depth-text affordance mapping framework. The\nframework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance\n(TFG) attention selection mechanism. BPM integrates the auxiliary modality\ndepth image directly as a prompt to the primary modality RGB image, embedding\nit into the primary modality encoder without introducing additional encoders.\nThis reduces the model's parameter count and effectively improves functional\nregion localization accuracy. The TFG mechanism guides the selection and\nenhancement of attention heads in the image encoder using textual features,\nimproving the understanding of affordance characteristics. Experimental results\ndemonstrate that the proposed method achieves significant performance\nimprovements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,\ncompared with the current state-of-the-art method, we achieve a 6.0%\nimprovement in the KLD metric, while reducing model parameters by 88.8%,\ndemonstrating practical application values. The source code will be made\npublicly available at https://github.com/DAWDSE/BiT-Align.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02597v1",
    "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.02597v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02597v1",
    "authors": [
      "Wei-Yao Wang",
      "Zhao Wang",
      "Helen Suzuki",
      "Yoshiyuki Kobayashi"
    ],
    "date": "2025-03-04",
    "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_02589v2",
    "title": "MCiteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs",
    "url": "http://arxiv.org/abs/2503.02589v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02589v2",
    "authors": [
      "Caiyu Hu",
      "Yikai Zhang",
      "Tinghui Zhu",
      "Yiwei Ye",
      "Yanghua Xiao"
    ],
    "date": "2025-03-04",
    "summary": "Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, overlooking the challenges and opportunities\nof multimodal contexts. To address this gap, we introduce MCiteBench, the first\nbenchmark designed to evaluate and analyze the multimodal citation text\ngeneration ability of MLLMs. Our benchmark comprises data derived from academic\npapers and review-rebuttal interactions, featuring diverse information sources\nand multimodal content. We comprehensively evaluate models from multiple\ndimensions, including citation quality, source reliability, and answer\naccuracy. Through extensive experiments, we observe that MLLMs struggle with\nmultimodal citation text generation. We also conduct deep analyses of models'\nperformance, revealing that the bottleneck lies in attributing the correct\nsources rather than understanding the multimodal content.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02579v1",
    "title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic\n  Understanding of High-Intensity Surgical Environments",
    "url": "http://arxiv.org/abs/2503.02579v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02579v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Chantal Pellegrini",
      "Tobias Czempiel",
      "Felix Tristram",
      "Kun Yuan",
      "David Bani-Harouni",
      "Ulrich Eck",
      "Benjamin Busam",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "date": "2025-03-04",
    "summary": "Operating rooms (ORs) are complex, high-stakes environments requiring precise\nunderstanding of interactions among medical staff, tools, and equipment for\nenhancing surgical assistance, situational awareness, and patient safety.\nCurrent datasets fall short in scale, realism and do not capture the multimodal\nnature of OR scenes, limiting progress in OR modeling. To this end, we\nintroduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR\ndataset, and the first dataset to enable multimodal scene graph generation.\nMM-OR captures comprehensive OR scenes containing RGB-D data, detail views,\naudio, speech transcripts, robotic logs, and tracking data and is annotated\nwith panoptic segmentations, semantic scene graphs, and downstream task labels.\nFurther, we propose MM2SG, the first multimodal large vision-language model for\nscene graph generation, and through extensive experiments, demonstrate its\nability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG\nestablish a new benchmark for holistic OR understanding, and open the path\ntowards multimodal scene analysis in complex, high-stakes environments. Our\ncode, and data is available at https://github.com/egeozsoy/MM-OR.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_02917v1",
    "title": "Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided\n  Prompting of Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.02917v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02917v1",
    "authors": [
      "Deval Mehta",
      "Yiwen Jiang",
      "Catherine L Jan",
      "Mingguang He",
      "Kshitij Jadhav",
      "Zongyuan Ge"
    ],
    "date": "2025-03-04",
    "summary": "Recent advancements in deep learning have shown significant potential for\nclassifying retinal diseases using color fundus images. However, existing works\npredominantly rely exclusively on image data, lack interpretability in their\ndiagnostic decisions, and treat medical professionals primarily as annotators\nfor ground truth labeling. To fill this gap, we implement two key strategies:\nextracting interpretable concepts of retinal diseases using the knowledge base\nof GPT models and incorporating these concepts as a language component in\nprompt-learning to train vision-language (VL) models with both fundus images\nand their associated concepts. Our method not only improves retinal disease\nclassification but also enriches few-shot and zero-shot detection (novel\ndisease detection), while offering the added benefit of concept-based model\ninterpretability. Our extensive evaluation across two diverse retinal fundus\nimage datasets illustrates substantial performance gains in VL-model based\nfew-shot methodologies through our concept integration approach, demonstrating\nan average improvement of approximately 5.8\\% and 2.7\\% mean average precision\nfor 16-shot learning and zero-shot (novel class) detection respectively. Our\nmethod marks a pivotal step towards interpretable and efficient retinal disease\nrecognition for real-world clinical applications.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ]
  },
  {
    "id": "2503_02511v1",
    "title": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place\n  Recognition",
    "url": "http://arxiv.org/abs/2503.02511v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02511v1",
    "authors": [
      "Oliver Grainge",
      "Michael Milford",
      "Indu Bodala",
      "Sarvapali D. Ramchurn",
      "Shoaib Ehsan"
    ],
    "date": "2025-03-04",
    "summary": "Visual Place Recognition (VPR) localizes a query image by matching it against\na database of geo-tagged reference images, making it essential for navigation\nand mapping in robotics. Although Vision Transformer (ViT) solutions deliver\nhigh accuracy, their large models often exceed the memory and compute budgets\nof resource-constrained platforms such as drones and mobile robots. To address\nthis issue, we propose TeTRA, a ternary transformer approach that progressively\nquantizes the ViT backbone to 2-bit precision and binarizes its final embedding\nlayer, offering substantial reductions in model size and latency. A carefully\ndesigned progressive distillation strategy preserves the representational power\nof a full-precision teacher, allowing TeTRA to retain or even surpass the\naccuracy of uncompressed convolutional counterparts, despite using fewer\nresources. Experiments on standard VPR benchmarks demonstrate that TeTRA\nreduces memory consumption by up to 69% compared to efficient baselines, while\nlowering inference latency by 35%, with either no loss or a slight improvement\nin recall@1. These gains enable high-accuracy VPR on power-constrained,\nmemory-limited robotic platforms, making TeTRA an appealing solution for\nreal-world deployment.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_02476v1",
    "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA",
    "url": "http://arxiv.org/abs/2503.02476v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02476v1",
    "authors": [
      "Zhengyang Ji",
      "Shang Gao",
      "Li Liu",
      "Yifan Jia",
      "Yutao Yue"
    ],
    "date": "2025-03-04",
    "summary": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "visual question answering",
      "VQA"
    ]
  },
  {
    "id": "2503_02459v1",
    "title": "Exploring Token-Level Augmentation in Vision Transformer for\n  Semi-Supervised Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.02459v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02459v1",
    "authors": [
      "Dengke Zhang",
      "Quan Tang",
      "Fagui Liu",
      "C. L. Philip Chen",
      "Haiqing Mei"
    ],
    "date": "2025-03-04",
    "summary": "Semi-supervised semantic segmentation has witnessed remarkable advancements\nin recent years. However, existing algorithms are based on convolutional neural\nnetworks and directly applying them to Vision Transformers poses certain\nlimitations due to conceptual disparities. To this end, we propose TokenMix, a\ndata augmentation technique specifically designed for semi-supervised semantic\nsegmentation with Vision Transformers. TokenMix aligns well with the global\nattention mechanism by mixing images at the token level, enhancing learning\ncapability for contexutual information among image patches. We further\nincorporate image augmentation and feature augmentation to promote the\ndiversity of augmentation. Moreover, to enhance consistency regularization, we\npropose a dual-branch framework where each branch applies both image\naugmentation and feature augmentation to the input image. We conduct extensive\nexperiments across multiple benchmark datasets, including Pascal VOC 2012,\nCityscapes, and COCO. Results suggest that the proposed method outperforms\nstate-of-the-art algorithms with notably observed accuracy improvement,\nespecially under the circumstance of limited fine annotations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_02420v1",
    "title": "Exploring Model Quantization in GenAI-based Image Inpainting and\n  Detection of Arable Plants",
    "url": "http://arxiv.org/abs/2503.02420v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02420v1",
    "authors": [
      "Sourav Modak",
      "Ahmet O\u011fuz Salt\u0131k",
      "Anthony Stein"
    ],
    "date": "2025-03-04",
    "summary": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion"
    ]
  },
  {
    "id": "2503_02394v3",
    "title": "BHViT: Binarized Hybrid Vision Transformer",
    "url": "http://arxiv.org/abs/2503.02394v3",
    "pdf_url": "http://arxiv.org/pdf/2503.02394v3",
    "authors": [
      "Tian Gao",
      "Zhiyuan Zhang",
      "Yu Zhang",
      "Huajun Liu",
      "Kaijie Yin",
      "Chengzhong Xu",
      "Hui Kong"
    ],
    "date": "2025-03-04",
    "summary": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_02393v1",
    "title": "Vision-Language Model IP Protection via Prompt-based Learning",
    "url": "http://arxiv.org/abs/2503.02393v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02393v1",
    "authors": [
      "Lianyu Wang",
      "Meng Wang",
      "Huazhu Fu",
      "Daoqiang Zhang"
    ],
    "date": "2025-03-04",
    "summary": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image\nPre-Training) have seen remarkable success in visual recognition, highlighting\nthe increasing need to safeguard the intellectual property (IP) of well-trained\nmodels. Effective IP protection extends beyond ensuring authorized usage; it\nalso necessitates restricting model deployment to authorized data domains,\nparticularly when the model is fine-tuned for specific target domains. However,\ncurrent IP protection methods often rely solely on the visual backbone, which\nmay lack sufficient semantic richness. To bridge this gap, we introduce\nIP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a\nprompt-based learning approach. By leveraging the frozen visual backbone of\nCLIP, we extract both image style and content information, incorporating them\ninto the learning of IP prompt. This strategy acts as a robust barrier,\neffectively preventing the unauthorized transfer of features from authorized\ndomains to unauthorized ones. Additionally, we propose a style-enhancement\nbranch that constructs feature banks for both authorized and unauthorized\ndomains. This branch integrates self-enhanced and cross-domain features,\nfurther strengthening IP-CLIP's capability to block features from unauthorized\ndomains. Finally, we present new three metrics designed to better balance the\nperformance degradation of authorized and unauthorized domains. Comprehensive\nexperiments in various scenarios demonstrate its promising potential for\napplication in IP protection tasks for VLMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "CLIP"
    ]
  },
  {
    "id": "2503_02379v1",
    "title": "Teaching Metric Distance to Autoregressive Multimodal Foundational\n  Models",
    "url": "http://arxiv.org/abs/2503.02379v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02379v1",
    "authors": [
      "Jiwan Chung",
      "Saejin Kim",
      "Yongrae Jo",
      "Jaewoo Park",
      "Dongjun Min",
      "Youngjae Yu"
    ],
    "date": "2025-03-04",
    "summary": "As large language models expand beyond natural language to domains such as\nmathematics, multimodal understanding, and embodied agents, tokens increasingly\nreflect metric relationships rather than purely linguistic meaning. We\nintroduce DIST2Loss, a distance-aware framework designed to train\nautoregressive discrete models by leveraging predefined distance relationships\namong output tokens. At its core, DIST2Loss transforms continuous exponential\nfamily distributions derived from inherent distance metrics into discrete,\ncategorical optimization targets compatible with the models' architectures.\nThis approach enables the models to learn and preserve meaningful distance\nrelationships during token generation while maintaining compatibility with\nexisting architectures. Empirical evaluations show consistent performance gains\nin diverse multimodal applications, including visual grounding, robotic\nmanipulation, generative reward modeling, and image generation using\nvector-quantized features. These improvements are pronounced in cases of\nlimited training data, highlighting DIST2Loss's effectiveness in\nresource-constrained settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02358v1",
    "title": "Are Large Vision Language Models Good Game Players?",
    "url": "http://arxiv.org/abs/2503.02358v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02358v1",
    "authors": [
      "Xinyu Wang",
      "Bohan Zhuang",
      "Qi Wu"
    ],
    "date": "2025-03-04",
    "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable abilities\nin understanding and reasoning about both visual and textual information.\nHowever, existing evaluation methods for LVLMs, primarily based on benchmarks\nlike Visual Question Answering and image captioning, often fail to capture the\nfull scope of LVLMs' capabilities. These benchmarks are limited by issues such\nas inadequate assessment of detailed visual perception, data contamination, and\na lack of focus on multi-turn reasoning. To address these challenges, we\npropose \\method{}, a game-based evaluation framework designed to provide a\ncomprehensive assessment of LVLMs' cognitive and reasoning skills in structured\nenvironments. \\method{} uses a set of games to evaluate LVLMs on four core\ntasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,\nwith each target task designed to assess specific abilities, including visual\nperception, reasoning, decision-making, etc. Based on this framework, we\nconduct extensive experiments that explore the limitations of current LVLMs,\nsuch as handling long structured outputs and perceiving detailed and dense\nelements. Code and data are publicly available at\nhttps://github.com/xinke-wang/LVLM-Playground.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "image captioning",
      "visual question answering"
    ]
  },
  {
    "id": "2503_02334v1",
    "title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language\n  Models",
    "url": "http://arxiv.org/abs/2503.02334v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02334v1",
    "authors": [
      "Sonnet Xu",
      "Joseph Janizek",
      "Yixing Jiang",
      "Roxana Daneshjou"
    ],
    "date": "2025-03-04",
    "summary": "Vision language models (VLMs) show promise in medical diagnosis, but their\nperformance across demographic subgroups when using in-context learning (ICL)\nremains poorly understood. We examine how the demographic composition of\ndemonstration examples affects VLM performance in two medical imaging tasks:\nskin lesion malignancy prediction and pneumothorax detection from chest\nradiographs. Our analysis reveals that ICL influences model predictions through\nmultiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease\nbase rates from prompts and (2) ICL leads VLMs to make predictions that perform\ndifferently across demographic groups, even after controlling for\nsubgroup-specific disease base rates. Our empirical results inform\nbest-practices for prompting current VLMs (specifically examining demographic\nsubgroup performance, and matching base rates of labels to target distribution\nat a bulk level and within subgroups), while also suggesting next steps for\nimproving our theoretical understanding of these models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_02330v1",
    "title": "Exploring Simple Siamese Network for High-Resolution Video Quality\n  Assessment",
    "url": "http://arxiv.org/abs/2503.02330v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02330v1",
    "authors": [
      "Guotao Shen",
      "Ziheng Yan",
      "Xin Jin",
      "Longhai Wu",
      "Jie Chen",
      "Ilhyun Cho",
      "Cheul-Hee Hahm"
    ],
    "date": "2025-03-04",
    "summary": "In the research of video quality assessment (VQA), two-branch network has\nemerged as a promising solution. It decouples VQA with separate technical and\naesthetic branches to measure the perception of low-level distortions and\nhigh-level semantics respectively. However, we argue that while technical and\naesthetic perspectives are complementary, the technical perspective itself\nshould be measured in semantic-aware manner. We hypothesize that existing\ntechnical branch struggles to perceive the semantics of high-resolution videos,\nas it is trained on local mini-patches sampled from videos. This issue can be\nhidden by apparently good results on low-resolution videos, but indeed becomes\ncritical for high-resolution VQA. This work introduces SiamVQA, a simple but\neffective Siamese network for highre-solution VQA. SiamVQA shares weights\nbetween technical and aesthetic branches, enhancing the semantic perception\nability of technical branch to facilitate technical-quality representation\nlearning. Furthermore, it integrates a dual cross-attention layer for fusing\ntechnical and aesthetic features. SiamVQA achieves state-of-the-art accuracy on\nhigh-resolution benchmarks, and competitive results on lower-resolution\nbenchmarks. Codes will be available at: https://github.com/srcn-ivl/SiamVQA",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VQA"
    ]
  }
]