[
  {
    "id": "paperswithcode_ArtVLM__Attribute_Recognition_Through_Vision_Based",
    "title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling",
    "url": "https://paperswithcode.com/paper/artvlm-attribute-recognition-through-vision",
    "authors": [],
    "date": "2025-03-28",
    "summary": "Recognizing and disentangling visual attributes from objects is a foundation to many computer vision applications.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null
  },
  {
    "id": "paperswithcode_ViLT__Vision_and_Language_Transformer_Without_Conv",
    "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
    "url": "https://paperswithcode.com/paper/vilt-vision-and-language-transformer-without",
    "authors": [],
    "date": "2025-03-27",
    "summary": "Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks.",
    "source": "Papers With Code",
    "keywords": [
      "vision-and-language"
    ],
    "code_url": null
  },
  {
    "id": "paperswithcode_BLIP__Bootstrapping_Language_Image_Pre_training_fo",
    "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
    "url": "https://paperswithcode.com/paper/blip-bootstrapping-language-image-pre",
    "authors": [],
    "date": "2025-03-26",
    "summary": "Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language",
      "image-text",
      "BLIP"
    ],
    "code_url": null
  },
  {
    "id": "2503_05093v1",
    "title": "Visual Cues of Gender and Race are Associated with Stereotyping in\n  Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.05093v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05093v1",
    "authors": [
      "Messi H. J. Lee",
      "Soyeon Jeon",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "date": "2025-03-25",
    "summary": "Current research on bias in Vision Language Models (VLMs) has important\nlimitations: it is focused exclusively on trait associations while ignoring\nother forms of stereotyping, it examines specific contexts where biases are\nexpected to appear, and it conceptualizes social categories like race and\ngender as binary, ignoring the multifaceted nature of these identities. Using\nstandardized facial images that vary in prototypicality, we test four VLMs for\nboth trait associations and homogeneity bias in open-ended contexts. We find\nthat VLMs consistently generate more uniform stories for women compared to men,\nwith people who are more gender prototypical in appearance being represented\nmore uniformly. By contrast, VLMs represent White Americans more uniformly than\nBlack Americans. Unlike with gender prototypicality, race prototypicality was\nnot related to stronger uniformity. In terms of trait associations, we find\nlimited evidence of stereotyping-Black Americans were consistently linked with\nbasketball across all models, while other racial associations (i.e., art,\nhealthcare, appearance) varied by specific VLM. These findings demonstrate that\nVLM stereotyping manifests in ways that go beyond simple group membership,\nsuggesting that conventional bias mitigation strategies may be insufficient to\naddress VLM stereotyping and that homogeneity bias persists even when trait\nassociations are less apparent in model outputs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_05132v1",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "url": "http://arxiv.org/abs/2503.05132v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05132v1",
    "authors": [
      "Hengguang Zhou",
      "Xirui Li",
      "Ruochen Wang",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ],
    "date": "2025-03-24",
    "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual reasoning"
    ]
  },
  {
    "id": "2503_05149v1",
    "title": "Development and Enhancement of Text-to-Image Diffusion Models",
    "url": "http://arxiv.org/abs/2503.05149v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05149v1",
    "authors": [
      "Rajdeep Roshan Sahu"
    ],
    "date": "2025-03-23",
    "summary": "This research focuses on the development and enhancement of text-to-image\ndenoising diffusion models, addressing key challenges such as limited sample\ndiversity and training instability. By incorporating Classifier-Free Guidance\n(CFG) and Exponential Moving Average (EMA) techniques, this study significantly\nimproves image quality, diversity, and stability. Utilizing Hugging Face's\nstate-of-the-art text-to-image generation model, the proposed enhancements\nestablish new benchmarks in generative AI. This work explores the underlying\nprinciples of diffusion models, implements advanced strategies to overcome\nexisting limitations, and presents a comprehensive evaluation of the\nimprovements achieved. Results demonstrate substantial progress in generating\nstable, diverse, and high-quality images from textual descriptions, advancing\nthe field of generative artificial intelligence and providing new foundations\nfor future applications.\n  Keywords: Text-to-image, Diffusion model, Classifier-free guidance,\nExponential moving average, Image generation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ]
  },
  {
    "id": "paperswithcode_LlavaGuard__An_Open_VLM_based_Framework_for_Safegu",
    "title": "LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models",
    "url": "https://paperswithcode.com/paper/llavaguard-vlm-based-safeguards-for-vision",
    "authors": [],
    "date": "2025-03-23",
    "summary": "This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that address the critical need for reliable guardrails in the era of large-scale data and models.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05179v1",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
    "url": "http://arxiv.org/abs/2503.05179v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05179v1",
    "authors": [
      "Simon A. Aytes",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ],
    "date": "2025-03-22",
    "summary": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_Open6DOR__Benchmarking_Open_instruction_6_DoF_Obje",
    "title": "Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach",
    "url": "https://paperswithcode.com/paper/open6dor-benchmarking-open-instruction-6-dof",
    "authors": [],
    "date": "2025-03-22",
    "summary": "In this work, we propel the pioneer construction of the benchmark and approach for table-top Open-instruction 6-DoF Object Rearrangement (Open6DOR).",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05186v1",
    "title": "Narrating the Video: Boosting Text-Video Retrieval via Comprehensive\n  Utilization of Frame-Level Captions",
    "url": "http://arxiv.org/abs/2503.05186v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05186v1",
    "authors": [
      "Chan hur",
      "Jeong-hun Hong",
      "Dong-hun Lee",
      "Dabin Kang",
      "Semin Myeong",
      "Sang-hyo Park",
      "Hyeyoung Park"
    ],
    "date": "2025-03-21",
    "summary": "In recent text-video retrieval, the use of additional captions from\nvision-language models has shown promising effects on the performance. However,\nexisting models using additional captions often have struggled to capture the\nrich semantics, including temporal changes, inherent in the video. In addition,\nincorrect information caused by generative models can lead to inaccurate\nretrieval. To address these issues, we propose a new framework, Narrating the\nVideo (NarVid), which strategically leverages the comprehensive information\navailable from frame-level captions, the narration. The proposed NarVid\nexploits narration in multiple ways: 1) feature enhancement through cross-modal\ninteractions between narration and video, 2) query-aware adaptive filtering to\nsuppress irrelevant or incorrect information, 3) dual-modal matching score by\nadding query-video similarity and query-narration similarity, and 4)\nhard-negative loss to learn discriminative features from multiple perspectives\nusing the two similarities from different views. Experimental results\ndemonstrate that NarVid achieves state-of-the-art performance on various\nbenchmark datasets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ]
  },
  {
    "id": "paperswithcode_RL_VLM_F__Reinforcement_Learning_from_Vision_Langu",
    "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
    "url": "https://paperswithcode.com/paper/rl-vlm-f-reinforcement-learning-from-vision",
    "authors": [],
    "date": "2025-03-21",
    "summary": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05204v1",
    "title": "Data-Efficient Generalization for Zero-shot Composed Image Retrieval",
    "url": "http://arxiv.org/abs/2503.05204v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05204v1",
    "authors": [
      "Zining Chen",
      "Zhicheng Zhao",
      "Fei Su",
      "Xiaoqin Zhang",
      "Shijian Lu"
    ],
    "date": "2025-03-20",
    "summary": "Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image\nbased on a reference image and a text description without requiring\nin-distribution triplets for training. One prevalent approach follows the\nvision-language pretraining paradigm that employs a mapping network to transfer\nthe image embedding to a pseudo-word token in the text embedding space.\nHowever, this approach tends to impede network generalization due to modality\ndiscrepancy and distribution shift between training and inference. To this end,\nwe propose a Data-efficient Generalization (DeG) framework, including two novel\ndesigns, namely, Textual Supplement (TS) module and Semantic-Set (S-Set). The\nTS module exploits compositional textual semantics during training, enhancing\nthe pseudo-word token with more linguistic semantics and thus mitigating the\nmodality discrepancy effectively. The S-Set exploits the zero-shot capability\nof pretrained Vision-Language Models (VLMs), alleviating the distribution shift\nand mitigating the overfitting issue from the redundancy of the large-scale\nimage-text data. Extensive experiments over four ZS-CIR benchmarks show that\nDeG outperforms the state-of-the-art (SOTA) methods with much less training\ndata, and saves substantial training and inference time for practical usage.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image-text"
    ]
  },
  {
    "id": "paperswithcode_An_Image_Grid_Can_Be_Worth_a_Video__Zero_shot_Vide",
    "title": "An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM",
    "url": "https://paperswithcode.com/paper/an-image-grid-can-be-worth-a-video-zero-shot",
    "authors": [],
    "date": "2025-03-20",
    "summary": "Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05214v1",
    "title": "Gaussian Random Fields as an Abstract Representation of Patient Metadata\n  for Multimodal Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2503.05214v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05214v1",
    "authors": [
      "Bill Cassidy",
      "Christian McBride",
      "Connah Kendrick",
      "Neil D. Reeves",
      "Joseph M. Pappachan",
      "Shaghayegh Raad",
      "Moi Hoon Yap"
    ],
    "date": "2025-03-19",
    "summary": "The growing rate of chronic wound occurrence, especially in patients with\ndiabetes, has become a concerning trend in recent years. Chronic wounds are\ndifficult and costly to treat, and have become a serious burden on health care\nsystems worldwide. Chronic wounds can have devastating consequences for the\npatient, with infection often leading to reduced quality of life and increased\nmortality risk. Innovative deep learning methods for the detection and\nmonitoring of such wounds have the potential to reduce the impact to both\npatient and clinician. We present a novel multimodal segmentation method which\nallows for the introduction of patient metadata into the training workflow\nwhereby the patient data are expressed as Gaussian random fields. Our results\nindicate that the proposed method improved performance when utilising multiple\nmodels, each trained on different metadata categories. Using the Diabetic Foot\nUlcer Challenge 2022 test set, when compared to the baseline results\n(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we\ndemonstrate improvements of +0.0220 and +0.0229 for intersection over union and\nDice similarity coefficient respectively. This paper presents the first study\nto focus on integrating patient data into a chronic wound segmentation\nworkflow. Our results show significant performance gains when training\nindividual models using specific metadata categories, followed by average\nmerging of prediction masks using distance transforms. All source code for this\nstudy is available at:\nhttps://github.com/mmu-dermatology-research/multimodal-grf",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_G_LLaVA__Solving_Geometric_Problem_with_Multi_Moda",
    "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
    "url": "https://paperswithcode.com/paper/g-llava-solving-geometric-problem-with-multi",
    "authors": [],
    "date": "2025-03-19",
    "summary": "We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ],
    "code_url": null
  },
  {
    "id": "2503_05228v1",
    "title": "RecipeGen: A Benchmark for Real-World Recipe Image Generation",
    "url": "http://arxiv.org/abs/2503.05228v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05228v1",
    "authors": [
      "Ruoxuan Zhang",
      "Hongxia Xie",
      "Yi Yao",
      "Jian-Yu Jiang-Lin",
      "Bin Wen",
      "Ling Lo",
      "Hong-Han Shuai",
      "Yung-Hui Li",
      "Wen-Huang Cheng"
    ],
    "date": "2025-03-18",
    "summary": "Recipe image generation is an important challenge in food computing, with\napplications from culinary education to interactive recipe platforms. However,\nthere is currently no real-world dataset that comprehensively connects recipe\ngoals, sequential steps, and corresponding images. To address this, we\nintroduce RecipeGen, the first real-world goal-step-image benchmark for recipe\ngeneration, featuring diverse ingredients, varied recipe steps, multiple\ncooking styles, and a broad collection of food categories. Data is in\nhttps://github.com/zhangdaxia22/RecipeGen.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ]
  },
  {
    "id": "paperswithcode_ConceptLab__Creative_Concept_Generation_using_VLM_",
    "title": "ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints",
    "url": "https://paperswithcode.com/paper/conceptlab-creative-generation-using",
    "authors": [],
    "date": "2025-03-18",
    "summary": "Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery.",
    "source": "Papers With Code",
    "keywords": [
      "VLM",
      "text-to-image"
    ],
    "code_url": null
  },
  {
    "id": "2503_05231v1",
    "title": "Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot\n  Learning and Human-Robot Interaction",
    "url": "http://arxiv.org/abs/2503.05231v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05231v1",
    "authors": [
      "Shuo Jiang",
      "Haonan Li",
      "Ruochen Ren",
      "Yanmin Zhou",
      "Zhipeng Wang",
      "Bin He"
    ],
    "date": "2025-03-17",
    "summary": "Cutting-edge robot learning techniques including foundation models and\nimitation learning from humans all pose huge demands on large-scale and\nhigh-quality datasets which constitute one of the bottleneck in the general\nintelligent robot fields. This paper presents the Kaiwu multimodal dataset to\naddress the missing real-world synchronized multimodal data problems in the\nsophisticated assembling scenario,especially with dynamics information and its\nfine-grained labelling. The dataset first provides an integration of\nhuman,environment and robot data collection framework with 20 subjects and 30\ninteraction objects resulting in totally 11,664 instances of integrated\nactions. For each of the demonstration,hand motions,operation pressures,sounds\nof the assembling process,multi-view videos, high-precision motion capture\ninformation,eye gaze with first-person videos,electromyography signals are all\nrecorded. Fine-grained multi-level annotation based on absolute timestamp,and\nsemantic segmentation labelling are performed. Kaiwu dataset aims to facilitate\nrobot learning,dexterous manipulation,human intention investigation and\nhuman-robot collaboration research.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_X__2__VLM__All_In_One_Pre_trained_Model_For_Vision",
    "title": "X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
    "url": "https://paperswithcode.com/paper/x-2-vlm-all-in-one-pre-trained-model-for",
    "authors": [],
    "date": "2025-03-17",
    "summary": "Vision language pre-training aims to learn alignments between vision and language from a large amount of data.",
    "source": "Papers With Code",
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "code_url": null
  },
  {
    "id": "2503_05236v1",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "url": "http://arxiv.org/abs/2503.05236v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05236v1",
    "authors": [
      "Yibin Wang",
      "Yuhang Zang",
      "Hao Li",
      "Cheng Jin",
      "Jiaqi Wang"
    ],
    "date": "2025-03-16",
    "summary": "Recent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guide preference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposes UnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling both pairwise ranking and pointwise scoring, which can be\nemployed for vision model preference alignment. Specifically, (1) we first\ndevelop UnifiedReward on our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment through Direct Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significantly improving the performance in each\ndomain.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_PaliGemma__A_versatile_3B_VLM_for_transfer",
    "title": "PaliGemma: A versatile 3B VLM for transfer",
    "url": "https://paperswithcode.com/paper/paligemma-a-versatile-3b-vlm-for-transfer",
    "authors": [],
    "date": "2025-03-16",
    "summary": "PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model.",
    "source": "Papers With Code",
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "code_url": null
  },
  {
    "id": "2503_05255v1",
    "title": "CMMCoT: Enhancing Complex Multi-Image Comprehension via Multi-Modal\n  Chain-of-Thought and Memory Augmentation",
    "url": "http://arxiv.org/abs/2503.05255v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05255v1",
    "authors": [
      "Guanghao Zhang",
      "Tao Zhong",
      "Yan Xia",
      "Zhelun Yu",
      "Haoyuan Li",
      "Wanggui He",
      "Fangxun Shu",
      "Mushui Liu",
      "Dong She",
      "Yi Wang",
      "Hao Jiang"
    ],
    "date": "2025-03-15",
    "summary": "While previous multimodal slow-thinking methods have demonstrated remarkable\nsuccess in single-image understanding scenarios, their effectiveness becomes\nfundamentally constrained when extended to more complex multi-image\ncomprehension tasks. This limitation stems from their predominant reliance on\ntext-based intermediate reasoning processes. While for human, when engaging in\nsophisticated multi-image analysis, they typically perform two complementary\ncognitive operations: (1) continuous cross-image visual comparison through\nregion-of-interest matching, and (2) dynamic memorization of critical visual\nconcepts throughout the reasoning chain. Motivated by these observations, we\npropose the Complex Multi-Modal Chain-of-Thought (CMMCoT) framework, a\nmulti-step reasoning framework that mimics human-like \"slow thinking\" for\nmulti-image understanding. Our approach incorporates two key innovations: 1.\nThe construction of interleaved multimodal multi-step reasoning chains, which\nutilize critical visual region tokens, extracted from intermediate reasoning\nsteps, as supervisory signals. This mechanism not only facilitates\ncomprehensive cross-modal understanding but also enhances model\ninterpretability. 2. The introduction of a test-time memory augmentation module\nthat expands the model reasoning capacity during inference while preserving\nparameter efficiency. Furthermore, to facilitate research in this direction, we\nhave curated a novel multi-image slow-thinking dataset. Extensive experiments\ndemonstrate the effectiveness of our model.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_VLM__Task_agnostic_Video_Language_Model_Pre_traini",
    "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
    "url": "https://paperswithcode.com/paper/vlm-task-agnostic-video-language-model-pre",
    "authors": [],
    "date": "2025-03-15",
    "summary": "We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05298v1",
    "title": "Coreference as an indicator of context scope in multimodal narrative",
    "url": "http://arxiv.org/abs/2503.05298v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05298v1",
    "authors": [
      "Nikolai Ilinykh",
      "Shalom Lappin",
      "Asad Sayeed",
      "Sharid Lo\u00e1iciga"
    ],
    "date": "2025-03-14",
    "summary": "We demonstrate that large multimodal language models differ substantially\nfrom humans in the distribution of coreferential expressions in a visual\nstorytelling task. We introduce a number of metrics to quantify the\ncharacteristics of coreferential patterns in both human- and machine-written\ntexts. Humans distribute coreferential expressions in a way that maintains\nconsistency across texts and images, interleaving references to different\nentities in a highly varied way. Machines are less able to track mixed\nreferences, despite achieving perceived improvements in generation quality.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_MME__A_Comprehensive_Evaluation_Benchmark_for_Mult",
    "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
    "url": "https://paperswithcode.com/paper/mme-a-comprehensive-evaluation-benchmark-for",
    "authors": [],
    "date": "2025-03-14",
    "summary": "Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "MLLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05305v1",
    "title": "Frequency Autoregressive Image Generation with Continuous Tokens",
    "url": "http://arxiv.org/abs/2503.05305v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05305v1",
    "authors": [
      "Hu Yu",
      "Hao Luo",
      "Hangjie Yuan",
      "Yu Rong",
      "Feng Zhao"
    ],
    "date": "2025-03-13",
    "summary": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ]
  },
  {
    "id": "paperswithcode_Kosmos_G__Generating_Images_in_Context_with_Multim",
    "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
    "url": "https://paperswithcode.com/paper/kosmos-g-generating-images-in-context-with",
    "authors": [],
    "date": "2025-03-13",
    "summary": "These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\"",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "image generation",
      "Kosmos"
    ],
    "code_url": null
  },
  {
    "id": "2503_05319v1",
    "title": "Robust Multimodal Learning for Ophthalmic Disease Grading via\n  Disentangled Representation",
    "url": "http://arxiv.org/abs/2503.05319v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05319v1",
    "authors": [
      "Xinkun Wang",
      "Yifang Wang",
      "Senwei Liang",
      "Feilong Tang",
      "Chengzhi Liu",
      "Ming Hu",
      "Chao Hu",
      "Junjun He",
      "Zongyuan Ge",
      "Imran Razzak"
    ],
    "date": "2025-03-12",
    "summary": "This paper discusses how ophthalmologists often rely on multimodal data to\nimprove diagnostic accuracy. However, complete multimodal data is rare in\nreal-world applications due to a lack of medical equipment and concerns about\ndata privacy. Traditional deep learning methods typically address these issues\nby learning representations in latent space. However, the paper highlights two\nkey limitations of these approaches: (i) Task-irrelevant redundant information\n(e.g., numerous slices) in complex modalities leads to significant redundancy\nin latent space representations. (ii) Overlapping multimodal representations\nmake it difficult to extract unique features for each modality. To overcome\nthese challenges, the authors propose the Essence-Point and Disentangle\nRepresentation Learning (EDRL) strategy, which integrates a self-distillation\nmechanism into an end-to-end framework to enhance feature selection and\ndisentanglement for more robust multimodal learning. Specifically, the\nEssence-Point Representation Learning module selects discriminative features\nthat improve disease grading performance. The Disentangled Representation\nLearning module separates multimodal data into modality-common and\nmodality-unique representations, reducing feature entanglement and enhancing\nboth robustness and interpretability in ophthalmic disease diagnosis.\nExperiments on multimodal ophthalmology datasets show that the proposed EDRL\nstrategy significantly outperforms current state-of-the-art methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_VATT__Transformers_for_Multimodal_Self_Supervised_",
    "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
    "url": "https://paperswithcode.com/paper/vatt-transformers-for-multimodal-self",
    "authors": [],
    "date": "2025-03-12",
    "summary": "We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "text-to-video"
    ],
    "code_url": null
  },
  {
    "id": "2503_05335v1",
    "title": "New multimodal similarity measure for image registration via modeling\n  local functional dependence with linear combination of learned basis\n  functions",
    "url": "http://arxiv.org/abs/2503.05335v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05335v1",
    "authors": [
      "Joel Honkamaa",
      "Pekka Marttinen"
    ],
    "date": "2025-03-11",
    "summary": "The deformable registration of images of different modalities, essential in\nmany medical imaging applications, remains challenging. The main challenge is\ndeveloping a robust measure for image overlap despite the compared images\ncapturing different aspects of the underlying tissue. Here, we explore\nsimilarity metrics based on functional dependence between intensity values of\nregistered images. Although functional dependence is too restrictive on the\nglobal scale, earlier work has shown competitive performance in deformable\nregistration when such measures are applied over small enough contexts. We\nconfirm this finding and further develop the idea by modeling local functional\ndependence via the linear basis function model with the basis functions learned\njointly with the deformation. The measure can be implemented via convolutions,\nmaking it efficient to compute on GPUs. We release the method as an easy-to-use\ntool and show good performance on three datasets compared to well-established\nbaseline and earlier functional dependence-based methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_MME_Survey__A_Comprehensive_Survey_on_Evaluation_o",
    "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
    "url": "https://paperswithcode.com/paper/mme-survey-a-comprehensive-survey-on",
    "authors": [],
    "date": "2025-03-11",
    "summary": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "multimodal LLM",
      "MLLM",
      "MLLMs"
    ],
    "code_url": null
  },
  {
    "id": "2503_05379v1",
    "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning",
    "url": "http://arxiv.org/abs/2503.05379v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05379v1",
    "authors": [
      "Jiaxing Zhao",
      "Xihan Wei",
      "Liefeng Bo"
    ],
    "date": "2025-03-10",
    "summary": "In this work, we present the first application of Reinforcement Learning with\nVerifiable Reward (RLVR) to an Omni-multimodal large language model in the\ncontext of emotion recognition, a task where both visual and audio modalities\nplay crucial roles. We leverage RLVR to optimize the Omni model, significantly\nenhancing its performance in three key aspects: reasoning capability, emotion\nrecognition accuracy, and generalization ability. The introduction of RLVR not\nonly improves the model's overall performance on in-distribution data but also\ndemonstrates superior robustness when evaluated on out-of-distribution\ndatasets. More importantly, the improved reasoning capability enables clear\nanalysis of the contributions of different modalities, particularly visual and\naudio information, in the emotion recognition process. This provides valuable\ninsights into the optimization of multimodal large language models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_A_Survey_on_Multimodal_Large_Language_Models",
    "title": "A Survey on Multimodal Large Language Models",
    "url": "https://paperswithcode.com/paper/a-survey-on-multimodal-large-language-models",
    "authors": [],
    "date": "2025-03-10",
    "summary": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "GPT-4V",
      "MLLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05383v1",
    "title": "VLMs Play StarCraft II: A Benchmark and Multimodal Decision Method",
    "url": "http://arxiv.org/abs/2503.05383v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05383v1",
    "authors": [
      "Weiyu Ma",
      "Yuqian Fu",
      "Zecheng Zhang",
      "Guohao Li"
    ],
    "date": "2025-03-09",
    "summary": "We introduce VLM-Attention, a multimodal StarCraft II environment that aligns\nartificial agent perception with the human gameplay experience. Traditional\nframeworks such as SMAC rely on abstract state representations that diverge\nsignificantly from human perception, limiting the ecological validity of agent\nbehavior. Our environment addresses this limitation by incorporating RGB visual\ninputs and natural language observations that more closely simulate human\ncognitive processes during gameplay. The VLM-Attention framework consists of\nthree integrated components: (1) a vision-language model enhanced with\nspecialized self-attention mechanisms for strategic unit targeting and\nbattlefield assessment, (2) a retrieval-augmented generation system that\nleverages domain-specific StarCraft II knowledge to inform tactical decisions,\nand (3) a dynamic role-based task distribution system that enables coordinated\nmulti-agent behavior. Our experimental evaluation across 21 custom scenarios\ndemonstrates that VLM-based agents powered by foundation models (specifically\nQwen-VL and GPT-4o) can execute complex tactical maneuvers without explicit\ntraining, achieving comparable performance to traditional MARL methods that\nrequire substantial training iterations. This work establishes a foundation for\ndeveloping human-aligned StarCraft II agents and advances the broader research\nagenda of multimodal game AI. Our implementation is available at\nhttps://github.com/camel-ai/VLM-Play-StarCraft2.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "paperswithcode_Screen2Words__Automatic_Mobile_UI_Summarization_wi",
    "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning",
    "url": "https://paperswithcode.com/paper/screen2words-automatic-mobile-ui",
    "authors": [],
    "date": "2025-03-09",
    "summary": "Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null
  },
  {
    "id": "2503_05424v1",
    "title": "Towards Locally Explaining Prediction Behavior via Gradual Interventions\n  and Measuring Property Gradients",
    "url": "http://arxiv.org/abs/2503.05424v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05424v1",
    "authors": [
      "Niklas Penzel",
      "Joachim Denzler"
    ],
    "date": "2025-03-08",
    "summary": "Deep learning models achieve high predictive performance but lack intrinsic\ninterpretability, hindering our understanding of the learned prediction\nbehavior. Existing local explainability methods focus on associations,\nneglecting the causal drivers of model predictions. Other approaches adopt a\ncausal perspective but primarily provide more general global explanations.\nHowever, for specific inputs, it's unclear whether globally identified factors\napply locally. To address this limitation, we introduce a novel framework for\nlocal interventional explanations by leveraging recent advances in\nimage-to-image editing models. Our approach performs gradual interventions on\nsemantic properties to quantify the corresponding impact on a model's\npredictions using a novel score, the expected property gradient magnitude. We\ndemonstrate the effectiveness of our approach through an extensive empirical\nevaluation on a wide range of architectures and tasks. First, we validate it in\na synthetic scenario and demonstrate its ability to locally identify biases.\nAfterward, we apply our approach to analyze network training dynamics,\ninvestigate medical skin lesion classifiers, and study a pre-trained CLIP model\nwith real-life interventional data. Our results highlight the potential of\ninterventional explanations on the property level to reveal new insights into\nthe behavior of deep models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "paperswithcode_Multimodal_Latent_Language_Modeling_with_Next_Toke",
    "title": "Multimodal Latent Language Modeling with Next-Token Diffusion",
    "url": "https://paperswithcode.com/paper/multimodal-latent-language-modeling-with-next",
    "authors": [],
    "date": "2025-03-08",
    "summary": "In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null
  },
  {
    "id": "2503_05534v1",
    "title": "S4M: Segment Anything with 4 Extreme Points",
    "url": "http://arxiv.org/abs/2503.05534v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05534v1",
    "authors": [
      "Adrien Meyer",
      "Lorenzo Arboit",
      "Giuseppe Massimiani",
      "Francesco Brucchi",
      "Luca Emanuele Amodio",
      "Didier Mutter",
      "Nicolas Padoy"
    ],
    "date": "2025-03-07",
    "summary": "The Segment Anything Model (SAM) has revolutionized open-set interactive\nimage segmentation, inspiring numerous adapters for the medical domain.\nHowever, SAM primarily relies on sparse prompts such as point or bounding box,\nwhich may be suboptimal for fine-grained instance segmentation, particularly in\nendoscopic imagery, where precise localization is critical and existing prompts\nstruggle to capture object boundaries effectively. To address this, we\nintroduce S4M (Segment Anything with 4 Extreme Points), which augments SAM by\nleveraging extreme points -- the top-, bottom-, left-, and right-most points of\nan instance -- prompts. These points are intuitive to identify and provide a\nfaster, structured alternative to box prompts. However, a na\\\"ive use of\nextreme points degrades performance, due to SAM's inability to interpret their\nsemantic roles. To resolve this, we introduce dedicated learnable embeddings,\nenabling the model to distinguish extreme points from generic free-form points\nand better reason about their spatial relationships. We further propose an\nauxiliary training task through the Canvas module, which operates solely on\nprompts -- without vision input -- to predict a coarse instance mask. This\nencourages the model to internalize the relationship between extreme points and\nmask distributions, leading to more robust segmentation. S4M outperforms other\nSAM-based approaches on three endoscopic surgical datasets, demonstrating its\neffectiveness in complex scenarios. Finally, we validate our approach through a\nhuman annotation study on surgical endoscopic videos, confirming that extreme\npoints are faster to acquire than bounding boxes.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Segment Anything"
    ]
  },
  {
    "id": "paperswithcode_Socratic_Models__Composing_Zero_Shot_Multimodal_Re",
    "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
    "url": "https://paperswithcode.com/paper/socratic-models-composing-zero-shot",
    "authors": [],
    "date": "2025-03-07",
    "summary": "Large pretrained (e. g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null
  },
  {
    "id": "2503_05064v1",
    "title": "Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided\n  Precision Robotic Manipulation",
    "url": "http://arxiv.org/abs/2503.05064v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05064v1",
    "authors": [
      "Qingxuan Jia",
      "Guoqin Tang",
      "Zeyuan Huang",
      "Zixuan Hao",
      "Ning Ji",
      "Shihang",
      "Yin",
      "Gang Chen"
    ],
    "date": "2025-03-07",
    "summary": "Vision-Language Models (VLMs) demonstrate remarkable potential in robotic\nmanipulation, yet challenges persist in executing complex fine manipulation\ntasks with high speed and precision. While excelling at high-level planning,\nexisting VLM methods struggle to guide robots through precise sequences of fine\nmotor actions. To address this limitation, we introduce a progressive VLM\nplanning algorithm that empowers robots to perform fast, precise, and\nerror-correctable fine manipulation. Our method decomposes complex tasks into\nsub-actions and maintains three key data structures: task memory structure, 2D\ntopology graphs, and 3D spatial networks, achieving high-precision\nspatial-semantic fusion. These three components collectively accumulate and\nstore critical information throughout task execution, providing rich context\nfor our task-oriented VLM interaction mechanism. This enables VLMs to\ndynamically adjust guidance based on real-time feedback, generating precise\naction plans and facilitating step-wise error correction. Experimental\nvalidation on complex assembly tasks demonstrates that our algorithm\neffectively guides robots to rapidly and precisely accomplish fine manipulation\nin challenging scenarios, significantly advancing robot intelligence for\nprecision tasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_04724v1",
    "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
    "url": "http://arxiv.org/abs/2503.04724v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04724v1",
    "authors": [
      "Sambal Shikhar",
      "Mohammed Irfan Kurpath",
      "Sahal Shaji Mullappilly",
      "Jean Lahoud",
      "Fahad Khan",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Hisham Cholakkal"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_04643v1",
    "title": "Adaptive Prototype Learning for Multimodal Cancer Survival Analysis",
    "url": "http://arxiv.org/abs/2503.04643v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04643v1",
    "authors": [
      "Hong Liu",
      "Haosen Yang",
      "Federica Eduati",
      "Josien P. W. Pluim",
      "Mitko Veta"
    ],
    "date": "2025-03-06",
    "summary": "Leveraging multimodal data, particularly the integration of whole-slide\nhistology images (WSIs) and transcriptomic profiles, holds great promise for\nimproving cancer survival prediction. However, excessive redundancy in\nmultimodal data can degrade model performance. In this paper, we propose\nAdaptive Prototype Learning (APL), a novel and effective approach for\nmultimodal cancer survival analysis. APL adaptively learns representative\nprototypes in a data-driven manner, reducing redundancy while preserving\ncritical information. Our method employs two sets of learnable query vectors\nthat serve as a bridge between high-dimensional representations and survival\nprediction, capturing task-relevant features. Additionally, we introduce a\nmultimodal mixed self-attention mechanism to enable cross-modal interactions,\nfurther enhancing information fusion. Extensive experiments on five benchmark\ncancer datasets demonstrate the superiority of our approach over existing\nmethods. The code is available at https://github.com/HongLiuuuuu/APL.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04641v1",
    "title": "Simulating the Real World: A Unified Survey of Multimodal Generative\n  Models",
    "url": "http://arxiv.org/abs/2503.04641v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04641v1",
    "authors": [
      "Yuqi Hu",
      "Longguang Wang",
      "Xian Liu",
      "Ling-Hao Chen",
      "Yuwei Guo",
      "Yukai Shi",
      "Ce Liu",
      "Anyi Rao",
      "Zeyu Wang",
      "Hui Xiong"
    ],
    "date": "2025-03-06",
    "summary": "Understanding and replicating the real world is a critical challenge in\nArtificial General Intelligence (AGI) research. To achieve this, many existing\napproaches, such as world models, aim to capture the fundamental principles\ngoverning the physical world, enabling more accurate simulations and meaningful\ninteractions. However, current methods often treat different modalities,\nincluding 2D (images), videos, 3D, and 4D representations, as independent\ndomains, overlooking their interdependencies. Additionally, these methods\ntypically focus on isolated dimensions of reality without systematically\nintegrating their connections. In this survey, we present a unified survey for\nmultimodal generative models that investigate the progression of data\ndimensionality in real-world simulation. Specifically, this survey starts from\n2D generation (appearance), then moves to video (appearance+dynamics) and 3D\ngeneration (appearance+geometry), and finally culminates in 4D generation that\nintegrate all dimensions. To the best of our knowledge, this is the first\nattempt to systematically unify the study of 2D, video, 3D and 4D generation\nwithin a single framework. To guide future research, we provide a comprehensive\nreview of datasets, evaluation metrics and future directions, and fostering\ninsights for newcomers. This survey serves as a bridge to advance the study of\nmultimodal generative models and real-world simulation within a unified\nframework.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04639v1",
    "title": "Enhancing SAM with Efficient Prompting and Preference Optimization for\n  Semi-supervised Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2503.04639v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04639v1",
    "authors": [
      "Aishik Konwer",
      "Zhijian Yang",
      "Erhan Bas",
      "Cao Xiao",
      "Prateek Prasanna",
      "Parminder Bhatia",
      "Taha Kass-Hout"
    ],
    "date": "2025-03-06",
    "summary": "Foundational models such as the Segment Anything Model (SAM) are gaining\ntraction in medical imaging segmentation, supporting multiple downstream tasks.\nHowever, such models are supervised in nature, still relying on large annotated\ndatasets or prompts supplied by experts. Conventional techniques such as active\nlearning to alleviate such limitations are limited in scope and still\nnecessitate continuous human involvement and complex domain knowledge for label\nrefinement or establishing reward ground truth. To address these challenges, we\npropose an enhanced Segment Anything Model (SAM) framework that utilizes\nannotation-efficient prompts generated in a fully unsupervised fashion, while\nstill capturing essential semantic, location, and shape information through\ncontrastive language-image pretraining and visual question answering. We adopt\nthe direct preference optimization technique to design an optimal policy that\nenables the model to generate high-fidelity segmentations with simple ratings\nor rankings provided by a virtual annotator simulating the human annotation\nprocess. State-of-the-art performance of our framework in tasks such as lung\nsegmentation, breast tumor segmentation, and organ segmentation across various\nmodalities, including X-ray, ultrasound, and abdominal CT, justifies its\neffectiveness in low-annotation data scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual question answering"
    ]
  },
  {
    "id": "2503_04592v1",
    "title": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning",
    "url": "http://arxiv.org/abs/2503.04592v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04592v1",
    "authors": [
      "Qing Zhou",
      "Tao Yang",
      "Junyu Gao",
      "Weiping Ni",
      "Junzheng Wu",
      "Qi Wang"
    ],
    "date": "2025-03-06",
    "summary": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image captioning"
    ]
  },
  {
    "id": "2503_04545v1",
    "title": "ViT-VS: On the Applicability of Pretrained Vision Transformer Features\n  for Generalizable Visual Servoing",
    "url": "http://arxiv.org/abs/2503.04545v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04545v1",
    "authors": [
      "Alessandro Scherl",
      "Stefan Thalhammer",
      "Bernhard Neuberger",
      "Wilfried W\u00f6ber",
      "Jos\u00e9 Grac\u00eda-Rodr\u00edguez"
    ],
    "date": "2025-03-06",
    "summary": "Visual servoing enables robots to precisely position their end-effector\nrelative to a target object. While classical methods rely on hand-crafted\nfeatures and thus are universally applicable without task-specific training,\nthey often struggle with occlusions and environmental variations, whereas\nlearning-based approaches improve robustness but typically require extensive\ntraining. We present a visual servoing approach that leverages pretrained\nvision transformers for semantic feature extraction, combining the advantages\nof both paradigms while also being able to generalize beyond the provided\nsample. Our approach achieves full convergence in unperturbed scenarios and\nsurpasses classical image-based visual servoing by up to 31.2\\% relative\nimprovement in perturbed scenarios. Even the convergence rates of\nlearning-based methods are matched despite requiring no task- or\nobject-specific training. Real-world evaluations confirm robust performance in\nend-effector positioning, industrial box manipulation, and grasping of unseen\nobjects using only a reference from the same category. Our code and simulation\nenvironment are available at: https://alessandroscherl.github.io/ViT-VS/",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_04543v1",
    "title": "Keeping Yourself is Important in Downstream Tuning Multimodal Large\n  Language Model",
    "url": "http://arxiv.org/abs/2503.04543v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04543v1",
    "authors": [
      "Wenke Huang",
      "Jian Liang",
      "Xianda Guo",
      "Yiyang Fang",
      "Guancheng Wan",
      "Xuankun Rong",
      "Chi Wen",
      "Zekun Shi",
      "Qingyun Li",
      "Didi Zhu",
      "Yanbiao Ma",
      "Ke Liang",
      "Bin Yang",
      "He Li",
      "Jiawei Shao",
      "Mang Ye",
      "Bo Du"
    ],
    "date": "2025-03-06",
    "summary": "Multi-modal Large Language Models (MLLMs) integrate visual and linguistic\nreasoning to address complex tasks such as image captioning and visual question\nanswering. While MLLMs demonstrate remarkable versatility, MLLMs appears\nlimited performance on special applications. But tuning MLLMs for downstream\ntasks encounters two key challenges: Task-Expert Specialization, where\ndistribution shifts between pre-training and target datasets constrain target\nperformance, and Open-World Stabilization, where catastrophic forgetting erases\nthe model general knowledge. In this work, we systematically review recent\nadvancements in MLLM tuning methodologies, classifying them into three\nparadigms: (I) Selective Tuning, (II) Additive Tuning, and (III)\nReparameterization Tuning. Furthermore, we benchmark these tuning strategies\nacross popular MLLM architectures and diverse downstream tasks to establish\nstandardized evaluation analysis and systematic tuning principles. Finally, we\nhighlight several open challenges in this domain and propose future research\ndirections. To facilitate ongoing progress in this rapidly evolving field, we\nprovide a public repository that continuously tracks developments:\nhttps://github.com/WenkeHuang/Awesome-MLLM-Tuning.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image captioning"
    ]
  },
  {
    "id": "2503_04528v1",
    "title": "Federated Dynamic Modeling and Learning for Spatiotemporal Data\n  Forecasting",
    "url": "http://arxiv.org/abs/2503.04528v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04528v1",
    "authors": [
      "Thien Pham",
      "Angelo Furno",
      "Fa\u00efcel Chamroukhi",
      "Latifa Oukhellou"
    ],
    "date": "2025-03-06",
    "summary": "This paper presents an advanced Federated Learning (FL) framework for\nforecasting complex spatiotemporal data, improving upon recent state-of-the-art\nmodels. In the proposed approach, the original Gated Recurrent Unit (GRU)\nmodule within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent\nNetwork (DSTGCRN) modeling is first replaced with a Long Short-Term Memory\n(LSTM) network, enabling the resulting model to more effectively capture\nlong-term dependencies inherent to time series data. The resulting architecture\nsignificantly improves the model's capacity to handle complex temporal patterns\nin diverse forecasting applications. Furthermore, the proposed FL framework\nintegrates a novel Client-Side Validation (CSV) mechanism, introducing a\ncritical validation step at the client level before incorporating aggregated\nparameters from the central server into local models. This ensures that only\nthe most effective updates are adopted, improving both the robustness and\naccuracy of the forecasting model across clients. The efficiency of our\napproach is demonstrated through extensive experiments on real-world\napplications, including public datasets for multimodal transport demand\nforecasting and private datasets for Origin-Destination (OD) matrix forecasting\nin urban areas. The results demonstrate substantial improvements over\nconventional methods, highlighting the framework's ability to capture complex\nspatiotemporal dependencies while preserving data privacy. This work not only\nprovides a scalable and privacy-preserving solution for real-time,\nregion-specific forecasting and management but also underscores the potential\nof leveraging distributed data sources in a FL context. We provide our\nalgorithms as open-source on GitHub.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04506v1",
    "title": "Multi-modal Summarization in Model-Based Engineering: Automotive\n  Software Development Case Study",
    "url": "http://arxiv.org/abs/2503.04506v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04506v1",
    "authors": [
      "Nenad Petrovic",
      "Yurui Zhang",
      "Moaad Maaroufi",
      "Kuo-Yi Chao",
      "Lukasz Mazur",
      "Fengjunjie Pan",
      "Vahid Zolfaghari",
      "Alois Knoll"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal summarization integrating information from diverse data modalities\npresents a promising solution to aid the understanding of information within\nvarious processes. However, the application and advantages of multimodal\nsummarization have not received much attention in model-based engineering\n(MBE), where it has become a cornerstone in the design and development of\ncomplex systems, leveraging formal models to improve understanding, validation\nand automation throughout the engineering lifecycle. UML and EMF diagrams in\nmodel-based engineering contain a large amount of multimodal information and\nintricate relational data. Hence, our study explores the application of\nmultimodal large language models within the domain of model-based engineering\nto evaluate their capacity for understanding and identifying relationships,\nfeatures, and functionalities embedded in UML and EMF diagrams. We aim to\ndemonstrate the transformative potential benefits and limitations of multimodal\nsummarization in improving productivity and accuracy in MBE practices. The\nproposed approach is evaluated within the context of automotive software\ndevelopment, while many promising state-of-art models were taken into account.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04504v1",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "url": "http://arxiv.org/abs/2503.04504v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04504v1",
    "authors": [
      "Sunghyun Ahn",
      "Youngwan Jo",
      "Kijung Lee",
      "Sein Kwon",
      "Inpyo Hong",
      "Sanghyun Park"
    ],
    "date": "2025-03-06",
    "summary": "Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "visual question answering"
    ]
  },
  {
    "id": "2503_04490v1",
    "title": "Large Language Models in Bioinformatics: A Survey",
    "url": "http://arxiv.org/abs/2503.04490v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04490v1",
    "authors": [
      "Zhenyu Wang",
      "Zikang Wang",
      "Jiyue Jiang",
      "Pengan Chen",
      "Xiangyu Shi",
      "Yu Li"
    ],
    "date": "2025-03-06",
    "summary": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04478v1",
    "title": "Semantic Alignment of Unimodal Medical Text and Vision Representations",
    "url": "http://arxiv.org/abs/2503.04478v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04478v1",
    "authors": [
      "Maxime Di Folco",
      "Emily Chan",
      "Marta Hasny",
      "Cosmin I. Bercea",
      "Julia A. Schnabel"
    ],
    "date": "2025-03-06",
    "summary": "General-purpose AI models, particularly those designed for text and vision,\ndemonstrate impressive versatility across a wide range of deep-learning tasks.\nHowever, they often underperform in specialised domains like medical imaging,\nwhere domain-specific solutions or alternative knowledge transfer approaches\nare typically required. Recent studies have noted that general-purpose models\ncan exhibit similar latent spaces when processing semantically related data,\nalthough this alignment does not occur naturally. Building on this insight, it\nhas been shown that applying a simple transformation - at most affine -\nestimated from a subset of semantically corresponding samples, known as\nanchors, enables model stitching across diverse training paradigms,\narchitectures, and modalities. In this paper, we explore how semantic alignment\n- estimating transformations between anchors - can bridge general-purpose AI\nwith specialised medical knowledge. Using multiple public chest X-ray datasets,\nwe demonstrate that model stitching across model architectures allows general\nmodels to integrate domain-specific knowledge without additional training,\nleading to improved performance on medical tasks. Furthermore, we introduce a\nnovel zero-shot classification approach for unimodal vision encoders that\nleverages semantic alignment across modalities. Our results show that our\nmethod not only outperforms general multimodal models but also approaches the\nperformance levels of fully trained, medical-specific multimodal solutions",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04470v1",
    "title": "Gate-Shift-Pose: Enhancing Action Recognition in Sports with Skeleton\n  Information",
    "url": "http://arxiv.org/abs/2503.04470v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04470v1",
    "authors": [
      "Edoardo Bianchi",
      "Oswald Lanz"
    ],
    "date": "2025-03-06",
    "summary": "This paper introduces Gate-Shift-Pose, an enhanced version of Gate-Shift-Fuse\nnetworks, designed for athlete fall classification in figure skating by\nintegrating skeleton pose data alongside RGB frames. We evaluate two fusion\nstrategies: early-fusion, which combines RGB frames with Gaussian heatmaps of\npose keypoints at the input stage, and late-fusion, which employs a\nmulti-stream architecture with attention mechanisms to combine RGB and pose\nfeatures. Experiments on the FR-FS dataset demonstrate that Gate-Shift-Pose\nsignificantly outperforms the RGB-only baseline, improving accuracy by up to\n40% with ResNet18 and 20% with ResNet50. Early-fusion achieves the highest\naccuracy (98.08%) with ResNet50, leveraging the model's capacity for effective\nmultimodal integration, while late-fusion is better suited for lighter\nbackbones like ResNet18. These results highlight the potential of multimodal\narchitectures for sports action recognition and the critical role of skeleton\npose information in capturing complex motion patterns.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04459v2",
    "title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering",
    "url": "http://arxiv.org/abs/2503.04459v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04459v2",
    "authors": [
      "Hongyeob Kim",
      "Inyoung Jung",
      "Dayoon Suh",
      "Youjia Zhang",
      "Sangmin Lee",
      "Sungeun Hong"
    ],
    "date": "2025-03-06",
    "summary": "Audio-Visual Question Answering (AVQA) requires not only question-based\nmultimodal reasoning but also precise temporal grounding to capture subtle\ndynamics for accurate prediction. However, existing methods mainly use question\ninformation implicitly, limiting focus on question-specific details.\nFurthermore, most studies rely on uniform frame sampling, which can miss key\nquestion-relevant frames. Although recent Top-K frame selection methods aim to\naddress this, their discrete nature still overlooks fine-grained temporal\ndetails. This paper proposes QA-TIGER, a novel framework that explicitly\nincorporates question information and models continuous temporal dynamics. Our\nkey idea is to use Gaussian-based modeling to adaptively focus on both\nconsecutive and non-consecutive frames based on the question, while explicitly\ninjecting question information and applying progressive refinement. We leverage\na Mixture of Experts (MoE) to flexibly implement multiple Gaussian models,\nactivating temporal experts specifically tailored to the question. Extensive\nexperiments on multiple AVQA benchmarks show that QA-TIGER consistently\nachieves state-of-the-art performance. Code is available at\nhttps://aim-skku.github.io/QA-TIGER/",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA"
    ]
  },
  {
    "id": "2503_04457v1",
    "title": "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction",
    "url": "http://arxiv.org/abs/2503.04457v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04457v1",
    "authors": [
      "Chao Wang",
      "Weiwei Fu",
      "Yang Zhou"
    ],
    "date": "2025-03-06",
    "summary": "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_04444v1",
    "title": "ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task",
    "url": "http://arxiv.org/abs/2503.04444v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04444v1",
    "authors": [
      "Vittorio Pippi",
      "Matthieu Guillaumin",
      "Silvia Cascianelli",
      "Rita Cucchiara",
      "Maximilian Jaritz",
      "Loris Bazzani"
    ],
    "date": "2025-03-06",
    "summary": "Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04417v1",
    "title": "From Idea to CAD: A Language Model-Driven Multi-Agent System for\n  Collaborative Design",
    "url": "http://arxiv.org/abs/2503.04417v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04417v1",
    "authors": [
      "Felix Ocker",
      "Stefan Menzel",
      "Ahmed Sadik",
      "Thiago Rios"
    ],
    "date": "2025-03-06",
    "summary": "Creating digital models using Computer Aided Design (CAD) is a process that\nrequires in-depth expertise. In industrial product development, this process\ntypically involves entire teams of engineers, spanning requirements\nengineering, CAD itself, and quality assurance. We present an approach that\nmirrors this team structure with a Vision Language Model (VLM)-based Multi\nAgent System, with access to parametric CAD tooling and tool documentation.\nCombining agents for requirements engineering, CAD engineering, and\nvision-based quality assurance, a model is generated automatically from\nsketches and/ or textual descriptions. The resulting model can be refined\ncollaboratively in an iterative validation loop with the user. Our approach has\nthe potential to increase the effectiveness of design processes, both for\nindustry experts and for hobbyists who create models for 3D printing. We\ndemonstrate the potential of the architecture at the example of various design\ntasks and provide several ablations that show the benefits of the\narchitecture's individual components.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_04406v1",
    "title": "Training-Free Graph Filtering via Multimodal Feature Refinement for\n  Extremely Fast Multimodal Recommendation",
    "url": "http://arxiv.org/abs/2503.04406v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04406v1",
    "authors": [
      "Yu-Seung Roh",
      "Joo-Young Kim",
      "Jin-Duk Park",
      "Won-Yong Shin"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal recommender systems improve the performance of canonical\nrecommender systems with no item features by utilizing diverse content types\nsuch as text, images, and videos, while alleviating inherent sparsity of\nuser-item interactions and accelerating user engagement. However, current\nneural network-based models often incur significant computational overhead due\nto the complex training process required to learn and integrate information\nfrom multiple modalities. To overcome this limitation, we propose\nMultiModal-Graph Filtering (MM-GF), a training-free method based on the notion\nof graph filtering (GF) for efficient and accurate multimodal recommendations.\nSpecifically, MM-GF first constructs multiple similarity graphs through\nnontrivial multimodal feature refinement such as robust scaling and vector\nshifting by addressing the heterogeneous characteristics across modalities.\nThen, MM-GF optimally fuses multimodal information using linear low-pass\nfilters across different modalities. Extensive experiments on real-world\nbenchmark datasets demonstrate that MM-GF not only improves recommendation\naccuracy by up to 13.35% compared to the best competitor but also dramatically\nreduces computational costs by achieving the runtime of less than 10 seconds.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04353v1",
    "title": "ObjMST: An Object-Focused Multimodal Style Transfer Framework",
    "url": "http://arxiv.org/abs/2503.04353v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04353v1",
    "authors": [
      "Chanda Grover Kamra",
      "Indra Deep Mastan",
      "Debayan Gupta"
    ],
    "date": "2025-03-06",
    "summary": "We propose ObjMST, an object-focused multimodal style transfer framework that\nprovides separate style supervision for salient objects and surrounding\nelements while addressing alignment issues in multimodal representation\nlearning. Existing image-text multimodal style transfer methods face the\nfollowing challenges: (1) generating non-aligned and inconsistent multimodal\nstyle representations; and (2) content mismatch, where identical style patterns\nare applied to both salient objects and their surrounding elements. Our\napproach mitigates these issues by: (1) introducing a Style-Specific Masked\nDirectional CLIP Loss, which ensures consistent and aligned style\nrepresentations for both salient objects and their surroundings; and (2)\nincorporating a salient-to-key mapping mechanism for stylizing salient objects,\nfollowed by image harmonization to seamlessly blend the stylized objects with\ntheir environment. We validate the effectiveness of ObjMST through experiments,\nusing both quantitative metrics and qualitative visual evaluations of the\nstylized outputs. Our code is available at:\nhttps://github.com/chandagrover/ObjMST.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "CLIP"
    ]
  },
  {
    "id": "2503_04325v2",
    "title": "GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain\n  tumour Segmentation on mp-MRI",
    "url": "http://arxiv.org/abs/2503.04325v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04325v2",
    "authors": [
      "Cecilia Diana-Albelda",
      "Roberto Alcover-Couso",
      "\u00c1lvaro Garc\u00eda-Mart\u00edn",
      "Jesus Bescos",
      "Marcos Escudero-Vi\u00f1olo"
    ],
    "date": "2025-03-06",
    "summary": "Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_04280v2",
    "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models",
    "url": "http://arxiv.org/abs/2503.04280v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04280v2",
    "authors": [
      "Niccol\u00f2 Turcato",
      "Matteo Iovino",
      "Aris Synodinos",
      "Alberto Dalla Libera",
      "Ruggero Carli",
      "Pietro Falco"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ]
  },
  {
    "id": "2503_04252v1",
    "title": "RCRank: Multimodal Ranking of Root Causes of Slow Queries in Cloud\n  Database Systems",
    "url": "http://arxiv.org/abs/2503.04252v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04252v1",
    "authors": [
      "Biao Ouyang",
      "Yingying Zhang",
      "Hanyin Cheng",
      "Yang Shu",
      "Chenjuan Guo",
      "Bin Yang",
      "Qingsong Wen",
      "Lunting Fan",
      "Christian S. Jensen"
    ],
    "date": "2025-03-06",
    "summary": "With the continued migration of storage to cloud database systems,the impact\nof slow queries in such systems on services and user experience is increasing.\nRoot-cause diagnosis plays an indispensable role in facilitating slow-query\ndetection and revision. This paper proposes a method capable of both\nidentifying possible root cause types for slow queries and ranking these\naccording to their potential for accelerating slow queries. This enables\nprioritizing root causes with the highest impact, in turn improving slow-query\nrevision effectiveness. To enable more accurate and detailed diagnoses, we\npropose the multimodal Ranking for the Root Causes of slow queries (RCRank)\nframework, which formulates root cause analysis as a multimodal machine\nlearning problem and leverages multimodal information from query statements,\nexecution plans, execution logs, and key performance indicators. To obtain\nexpressive embeddings from its heterogeneous multimodal input, RCRank\nintegrates self-supervised pre-training that enhances cross-modal alignment and\ntask relevance. Next, the framework integrates root-cause-adaptive cross\nTransformers that enable adaptive fusion of multimodal features with varying\ncharacteristics. Finally, the framework offers a unified model that features an\nimpact-aware training objective for identifying and ranking root causes. We\nreport on experiments on real and synthetic datasets, finding that RCRank is\ncapable of consistently outperforming the state-of-the-art methods at root\ncause identification and ranking according to a range of metrics.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04250v1",
    "title": "An Egocentric Vision-Language Model based Portable Real-time Smart\n  Assistant",
    "url": "http://arxiv.org/abs/2503.04250v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04250v1",
    "authors": [
      "Yifei Huang",
      "Jilan Xu",
      "Baoqi Pei",
      "Yuping He",
      "Guo Chen",
      "Mingfang Zhang",
      "Lijin Yang",
      "Zheng Nie",
      "Jinyao Liu",
      "Guoshun Fan",
      "Dechen Lin",
      "Fang Fang",
      "Kunpeng Li",
      "Chang Yuan",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Yali Wang",
      "Yu Qiao",
      "Limin Wang"
    ],
    "date": "2025-03-06",
    "summary": "We present Vinci, a vision-language system designed to provide real-time,\ncomprehensive AI assistance on portable devices. At its core, Vinci leverages\nEgoVideo-VL, a novel model that integrates an egocentric vision foundation\nmodel with a large language model (LLM), enabling advanced functionalities such\nas scene understanding, temporal grounding, video summarization, and future\nplanning. To enhance its utility, Vinci incorporates a memory module for\nprocessing long video streams in real time while retaining contextual history,\na generation module for producing visual action demonstrations, and a retrieval\nmodule that bridges egocentric and third-person perspectives to provide\nrelevant how-to videos for skill acquisition. Unlike existing systems that\noften depend on specialized hardware, Vinci is hardware-agnostic, supporting\ndeployment across a wide range of devices, including smartphones and wearable\ncameras. In our experiments, we first demonstrate the superior performance of\nEgoVideo-VL on multiple public benchmarks, showcasing its vision-language\nreasoning and contextual understanding capabilities. We then conduct a series\nof user studies to evaluate the real-world effectiveness of Vinci, highlighting\nits adaptability and usability in diverse scenarios. We hope Vinci can\nestablish a new framework for portable, real-time egocentric AI systems,\nempowering users with contextual and actionable insights. Including the\nfrontend, backend, and models, all codes of Vinci are available at\nhttps://github.com/OpenGVLab/vinci.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ]
  },
  {
    "id": "2503_04229v1",
    "title": "Synthetic Data is an Elegant GIFT for Continual Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.04229v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04229v1",
    "authors": [
      "Bin Wu",
      "Wuxuan Shi",
      "Jinqiao Wang",
      "Mang Ye"
    ],
    "date": "2025-03-06",
    "summary": "Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to\nefficiently update their knowledge and adapt to various downstream tasks\nwithout retraining from scratch. However, for VLMs, in addition to the loss of\nknowledge previously learned from downstream tasks, pre-training knowledge is\nalso corrupted during continual fine-tuning. This issue is exacerbated by the\nunavailability of original pre-training data, leaving VLM's generalization\nability degrading. In this paper, we propose GIFT, a novel continual\nfine-tuning approach that utilizes synthetic data to overcome catastrophic\nforgetting in VLMs. Taking advantage of recent advances in text-to-image\nsynthesis, we employ a pre-trained diffusion model to recreate both\npre-training and learned downstream task data. In this way, the VLM can revisit\nprevious knowledge through distillation on matching diffusion-generated images\nand corresponding text prompts. Leveraging the broad distribution and high\nalignment between synthetic image-text pairs in VLM's feature space, we propose\na contrastive distillation loss along with an image-text alignment constraint.\nTo further combat in-distribution overfitting and enhance distillation\nperformance with limited amount of generated data, we incorporate adaptive\nweight consolidation, utilizing Fisher information from these synthetic\nimage-text pairs and achieving a better stability-plasticity balance. Extensive\nexperiments demonstrate that our method consistently outperforms previous\nstate-of-the-art approaches across various settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image-text",
      "text-to-image"
    ]
  },
  {
    "id": "2503_04205v1",
    "title": "Learning 3D Medical Image Models From Brain Functional Connectivity\n  Network Supervision For Mental Disorder Diagnosis",
    "url": "http://arxiv.org/abs/2503.04205v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04205v1",
    "authors": [
      "Xingcan Hu",
      "Wei Wang",
      "Li Xiao"
    ],
    "date": "2025-03-06",
    "summary": "In MRI-based mental disorder diagnosis, most previous studies focus on\nfunctional connectivity network (FCN) derived from functional MRI (fMRI).\nHowever, the small size of annotated fMRI datasets restricts its wide\napplication. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w)\nMRI, which are commonly used and readily accessible in clinical settings, are\noften overlooked. To integrate the complementary information from both function\nand structure for improved diagnostic accuracy, we propose CINP (Contrastive\nImage-Network Pre-training), a framework that employs contrastive learning\nbetween sMRI and FCN. During pre-training, we incorporate masked image modeling\nand network-image matching to enhance visual representation learning and\nmodality alignment. Since the CINP facilitates knowledge transfer from FCN to\nsMRI, we introduce network prompting. It utilizes only sMRI from suspected\npatients and a small amount of FCNs from different patient classes for\ndiagnosing mental disorders, which is practical in real-world clinical\nscenario. The competitive performance on three mental disorder diagnosis tasks\ndemonstrate the effectiveness of the CINP in integrating multimodal MRI\ninformation, as well as the potential of incorporating sMRI into clinical\ndiagnosis using network prompting.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04201v1",
    "title": "Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative\n  Approach to Few-shot Multimodal Dialogue Intention Recognition",
    "url": "http://arxiv.org/abs/2503.04201v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04201v1",
    "authors": [
      "Bin Chen",
      "Yu Zhang",
      "Hongfei Ye",
      "Ziyi Huang",
      "Hongyang Chen"
    ],
    "date": "2025-03-06",
    "summary": "Few-shot multimodal dialogue intention recognition is a critical challenge in\nthe e-commerce domainn. Previous methods have primarily enhanced model\nclassification capabilities through post-training techniques. However, our\nanalysis reveals that training for few-shot multimodal dialogue intention\nrecognition involves two interconnected tasks, leading to a seesaw effect in\nmulti-task learning. This phenomenon is attributed to knowledge interference\nstemming from the superposition of weight matrix updates during the training\nprocess. To address these challenges, we propose Knowledge-Decoupled Synergetic\nLearning (KDSL), which mitigates these issues by utilizing smaller models to\ntransform knowledge into interpretable rules, while applying the post-training\nof larger models. By facilitating collaboration between the large and small\nmultimodal large language models for prediction, our approach demonstrates\nsignificant improvements. Notably, we achieve outstanding results on two real\nTaobao datasets, with enhancements of 6.37\\% and 6.28\\% in online weighted F1\nscores compared to the state-of-the-art method, thereby validating the efficacy\nof our framework.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04199v1",
    "title": "MASTER: Multimodal Segmentation with Text Prompts",
    "url": "http://arxiv.org/abs/2503.04199v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04199v1",
    "authors": [
      "Fuyang Liu",
      "Shun Lu",
      "Jilin Mei",
      "Yu Hu"
    ],
    "date": "2025-03-06",
    "summary": "RGB-Thermal fusion is a potential solution for various weather and light\nconditions in challenging scenarios. However, plenty of studies focus on\ndesigning complex modules to fuse different modalities. With the widespread\napplication of large language models (LLMs), valuable information can be more\neffectively extracted from natural language. Therefore, we aim to leverage the\nadvantages of large language models to design a structurally simple and highly\nadaptable multimodal fusion model architecture. We proposed MultimodAl\nSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM into\nthe fusion of RGB-Thermal multimodal data and allows complex query text to\nparticipate in the fusion process. Our model utilizes a dual-path structure to\nextract information from different modalities of images. Additionally, we\nemploy LLM as the core module for multimodal fusion, enabling the model to\ngenerate learnable codebook tokens from RGB, thermal images, and textual\ninformation. A lightweight image decoder is used to obtain semantic\nsegmentation results. The proposed MASTER performs exceptionally well in\nbenchmark tests across various automated driving scenarios, yielding promising\nresults.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04167v1",
    "title": "The Role of Visual Modality in Multimodal Mathematical Reasoning:\n  Challenges and Insights",
    "url": "http://arxiv.org/abs/2503.04167v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04167v1",
    "authors": [
      "Yufang Liu",
      "Yao Du",
      "Tao Ji",
      "Jianing Wang",
      "Yang Liu",
      "Yuanbin Wu",
      "Aimin Zhou",
      "Mengdi Zhang",
      "Xunliang Cai"
    ],
    "date": "2025-03-06",
    "summary": "Recent research has increasingly focused on multimodal mathematical\nreasoning, particularly emphasizing the creation of relevant datasets and\nbenchmarks. Despite this, the role of visual information in reasoning has been\nunderexplored. Our findings show that existing multimodal mathematical models\nminimally leverage visual information, and model performance remains largely\nunaffected by changes to or removal of images in the dataset. We attribute this\nto the dominance of textual information and answer options that inadvertently\nguide the model to correct answers. To improve evaluation methods, we introduce\nthe HC-M3D dataset, specifically designed to require image reliance for\nproblem-solving and to challenge models with similar, yet distinct, images that\nchange the correct answer. In testing leading models, their failure to detect\nthese subtle visual differences suggests limitations in current visual\nperception capabilities. Additionally, we observe that the common approach of\nimproving general VQA capabilities by combining various types of image encoders\ndoes not contribute to math reasoning performance. This finding also presents a\nchallenge to enhancing visual reliance during math reasoning. Our benchmark and\ncode would be available at\n\\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\\_modality\\_role}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "VQA"
    ]
  },
  {
    "id": "2503_04144v1",
    "title": "DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person\n  Retrieval",
    "url": "http://arxiv.org/abs/2503.04144v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04144v1",
    "authors": [
      "Yating Liu",
      "Zimo Liu",
      "Xiangyuan Lan",
      "Wenming Yang",
      "Yaowei Li",
      "Qingmin Liao"
    ],
    "date": "2025-03-06",
    "summary": "Text-based person retrieval (TPR) has gained significant attention as a\nfine-grained and challenging task that closely aligns with practical\napplications. Tailoring CLIP to person domain is now a emerging research topic\ndue to the abundant knowledge of vision-language pretraining, but challenges\nstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is\ncomputationally expensive and prone to overfitting.(ii) Existing\nparameter-efficient transfer learning (PETL) for TPR lacks of fine-grained\nfeature extraction. To address these issues, we propose Domain-Aware\nMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and\nPETL to enhance fine-grained feature representations while maintaining\nefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to\nMLP layers in both vision and language branches, where different experts\nspecialize in distinct aspects of person knowledge to handle features more\nfinely. To promote the router to exploit domain information effectively and\nalleviate the routing imbalance, Domain-Aware Router is then developed by\nbuilding a novel gating function and injecting learnable domain-aware prompts.\nExtensive experiments show that our DM-Adapter achieves state-of-the-art\nperformance, outperforming previous methods by a significant margin.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP"
    ]
  },
  {
    "id": "2503_04135v1",
    "title": "Biological Sequence with Language Model Prompting: A Survey",
    "url": "http://arxiv.org/abs/2503.04135v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04135v1",
    "authors": [
      "Jiyue Jiang",
      "Zikang Wang",
      "Yuheng Shan",
      "Heyan Chai",
      "Jiayi Li",
      "Zixian Ma",
      "Xinrui Zhang",
      "Yu Li"
    ],
    "date": "2025-03-06",
    "summary": "Large Language models (LLMs) have emerged as powerful tools for addressing\nchallenges across diverse domains. Notably, recent studies have demonstrated\nthat large language models significantly enhance the efficiency of biomolecular\nanalysis and synthesis, attracting widespread attention from academics and\nmedicine. In this paper, we systematically investigate the application of\nprompt-based methods with LLMs to biological sequences, including DNA, RNA,\nproteins, and drug discovery tasks. Specifically, we focus on how prompt\nengineering enables LLMs to tackle domain-specific problems, such as promoter\nsequence prediction, protein structure modeling, and drug-target binding\naffinity prediction, often with limited labeled data. Furthermore, our\ndiscussion highlights the transformative potential of prompting in\nbioinformatics while addressing key challenges such as data scarcity,\nmultimodal fusion, and computational resource limitations. Our aim is for this\npaper to function both as a foundational primer for newcomers and a catalyst\nfor continued innovation within this dynamic field of study.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04130v1",
    "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.04130v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04130v1",
    "authors": [
      "Jindong Jiang",
      "Xiuyu Li",
      "Zhijian Liu",
      "Muyang Li",
      "Guo Chen",
      "Zhiqi Li",
      "De-An Huang",
      "Guilin Liu",
      "Zhiding Yu",
      "Kurt Keutzer",
      "Sungjin Ahn",
      "Jan Kautz",
      "Hongxu Yin",
      "Yao Lu",
      "Song Han",
      "Wonmin Byeon"
    ],
    "date": "2025-03-06",
    "summary": "Recent advances in video-based multimodal large language models (Video-LLMs)\nhave significantly improved video understanding by processing videos as\nsequences of image frames. However, many existing methods treat frames\nindependently in the vision backbone, lacking explicit temporal modeling, which\nlimits their ability to capture dynamic patterns and efficiently handle long\nvideos. To address these limitations, we introduce STORM\n(\\textbf{S}patiotemporal \\textbf{TO}ken \\textbf{R}eduction for\n\\textbf{M}ultimodal LLMs), a novel architecture incorporating a dedicated\ntemporal encoder between the image encoder and the LLM. Our temporal encoder\nleverages the Mamba State Space Model to integrate temporal information into\nimage tokens, generating enriched representations that preserve inter-frame\ndynamics across the entire video sequence. This enriched encoding not only\nenhances video reasoning capabilities but also enables effective token\nreduction strategies, including test-time sampling and training-based temporal\nand spatial pooling, substantially reducing computational demands on the LLM\nwithout sacrificing key temporal information. By integrating these techniques,\nour approach simultaneously reduces training and inference latency while\nimproving performance, enabling efficient and robust video understanding over\nextended temporal contexts. Extensive evaluations show that STORM achieves\nstate-of-the-art results across various long video understanding benchmarks\n(more than 5\\% improvement on MLVU and LongVideoBench) while reducing the\ncomputation costs by up to $8\\times$ and the decoding latency by\n2.4-2.9$\\times$ for the fixed numbers of input frames. Project page is\navailable at https://research.nvidia.com/labs/lpr/storm",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04121v1",
    "title": "Simple Self Organizing Map with Visual Transformer",
    "url": "http://arxiv.org/abs/2503.04121v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04121v1",
    "authors": [
      "Alan Luo",
      "Kaiwen Yuan"
    ],
    "date": "2025-03-06",
    "summary": "Vision Transformers (ViTs) have demonstrated exceptional performance in\nvarious vision tasks. However, they tend to underperform on smaller datasets\ndue to their inherent lack of inductive biases. Current approaches address this\nlimitation implicitly-often by pairing ViTs with pretext tasks or by distilling\nknowledge from convolutional neural networks (CNNs) to strengthen the prior. In\ncontrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised\nframework, are inherently structured to preserve topology and spatial\norganization, making them a promising candidate to directly address the\nlimitations of ViTs in limited or small training datasets. Despite this\npotential, equipping SOMs with modern deep learning architectures remains\nlargely unexplored. In this study, we conduct a novel exploration on how Vision\nTransformers (ViTs) and Self-Organizing Maps (SOMs) can empower each other,\naiming to bridge this critical research gap. Our findings demonstrate that\nthese architectures can synergistically enhance each other, leading to\nsignificantly improved performance in both unsupervised and supervised tasks.\nCode will be publicly available.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_04110v1",
    "title": "InterChat: Enhancing Generative Visual Analytics using Multimodal\n  Interactions",
    "url": "http://arxiv.org/abs/2503.04110v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04110v1",
    "authors": [
      "Juntong Chen",
      "Jiang Wu",
      "Jiajing Guo",
      "Vikram Mohanty",
      "Xueming Li",
      "Jorge Piazentin Ono",
      "Wenbin He",
      "Liu Ren",
      "Dongyu Liu"
    ],
    "date": "2025-03-06",
    "summary": "The rise of Large Language Models (LLMs) and generative visual analytics\nsystems has transformed data-driven insights, yet significant challenges\npersist in accurately interpreting users' analytical and interaction intents.\nWhile language inputs offer flexibility, they often lack precision, making the\nexpression of complex intents inefficient, error-prone, and time-intensive. To\naddress these limitations, we investigate the design space of multimodal\ninteractions for generative visual analytics through a literature review and\npilot brainstorming sessions. Building on these insights, we introduce a highly\nextensible workflow that integrates multiple LLM agents for intent inference\nand visualization generation. We develop InterChat, a generative visual\nanalytics system that combines direct manipulation of visual elements with\nnatural language inputs. This integration enables precise intent communication\nand supports progressive, visually driven exploratory data analyses. By\nemploying effective prompt engineering, and contextual interaction linking,\nalongside intuitive visualization and interaction designs, InterChat bridges\nthe gap between user interactions and LLM-driven visualizations, enhancing both\ninterpretability and usability. Extensive evaluations, including two usage\nscenarios, a user study, and expert feedback, demonstrate the effectiveness of\nInterChat. Results show significant improvements in the accuracy and efficiency\nof handling complex visual analytics tasks, highlighting the potential of\nmultimodal interactions to redefine user engagement and analytical depth in\ngenerative visual analytics.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04095v2",
    "title": "Chart-HQA: A Benchmark for Hypothetical Question Answering in Charts",
    "url": "http://arxiv.org/abs/2503.04095v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04095v2",
    "authors": [
      "Xiangnan Chen",
      "Yuancheng Fang",
      "Qian Xiao",
      "Juncheng Li",
      "Jun Lin",
      "Siliang Tang",
      "Yi Yang",
      "Yueting Zhuang"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal Large Language Models (MLLMs) have garnered significant attention\nfor their strong visual-semantic understanding. Most existing chart benchmarks\nevaluate MLLMs' ability to parse information from charts to answer questions.\nHowever, they overlook the inherent output biases of MLLMs, where models rely\non their parametric memory to answer questions rather than genuinely\nunderstanding the chart content. To address this limitation, we introduce a\nnovel Chart Hypothetical Question Answering (HQA) task, which imposes\nassumptions on the same question to compel models to engage in counterfactual\nreasoning based on the chart content. Furthermore, we introduce HAI, a human-AI\ninteractive data synthesis approach that leverages the efficient text-editing\ncapabilities of LLMs alongside human expert knowledge to generate diverse and\nhigh-quality HQA data at a low cost. Using HAI, we construct Chart-HQA, a\nchallenging benchmark synthesized from publicly available data sources.\nEvaluation results on 18 MLLMs of varying model sizes reveal that current\nmodels face significant generalization challenges and exhibit imbalanced\nreasoning performance on the HQA task.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04078v1",
    "title": "Spatial-Temporal Perception with Causal Inference for Naturalistic\n  Driving Action Recognition",
    "url": "http://arxiv.org/abs/2503.04078v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04078v1",
    "authors": [
      "Qing Chang",
      "Wei Dai",
      "Zhihao Shuai",
      "Limin Yu",
      "Yutao Yue"
    ],
    "date": "2025-03-06",
    "summary": "Naturalistic driving action recognition is essential for vehicle cabin\nmonitoring systems. However, the complexity of real-world backgrounds presents\nsignificant challenges for this task, and previous approaches have struggled\nwith practical implementation due to their limited ability to observe subtle\nbehavioral differences and effectively learn inter-frame features from video.\nIn this paper, we propose a novel Spatial-Temporal Perception (STP)\narchitecture that emphasizes both temporal information and spatial\nrelationships between key objects, incorporating a causal decoder to perform\nbehavior recognition and temporal action localization. Without requiring\nmultimodal input, STP directly extracts temporal and spatial distance features\nfrom RGB video clips. Subsequently, these dual features are jointly encoded by\nmaximizing the expected likelihood across all possible permutations of the\nfactorization order. By integrating temporal and spatial features at different\nscales, STP can perceive subtle behavioral changes in challenging scenarios.\nAdditionally, we introduce a causal-aware module to explore relationships\nbetween video frame features, significantly enhancing detection efficiency and\nperformance. We validate the effectiveness of our approach using two publicly\navailable driver distraction detection benchmarks. The results demonstrate that\nour framework achieves state-of-the-art performance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "CLIP"
    ]
  },
  {
    "id": "2503_04065v1",
    "title": "PP-DocBee: Improving Multimodal Document Understanding Through a Bag of\n  Tricks",
    "url": "http://arxiv.org/abs/2503.04065v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04065v1",
    "authors": [
      "Feng Ni",
      "Kui Huang",
      "Yao Lu",
      "Wenyu Lv",
      "Guanzhong Wang",
      "Zeyu Chen",
      "Yi Liu"
    ],
    "date": "2025-03-06",
    "summary": "With the rapid advancement of digitalization, various document images are\nbeing applied more extensively in production and daily life, and there is an\nincreasingly urgent need for fast and accurate parsing of the content in\ndocument images. Therefore, this report presents PP-DocBee, a novel multimodal\nlarge language model designed for end-to-end document image understanding.\nFirst, we develop a data synthesis strategy tailored to document scenarios in\nwhich we build a diverse dataset to improve the model generalization. Then, we\napply a few training techniques, including dynamic proportional sampling, data\npreprocessing, and OCR postprocessing strategies. Extensive evaluations\ndemonstrate the superior performance of PP-DocBee, achieving state-of-the-art\nresults on English document understanding benchmarks and even outperforming\nexisting open source and commercial models in Chinese document understanding.\nThe source code and pre-trained models are publicly available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04058v1",
    "title": "EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language\n  Models",
    "url": "http://arxiv.org/abs/2503.04058v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04058v1",
    "authors": [
      "Haiyang Yu",
      "Jinghui Lu",
      "Yanjie Wang",
      "Yang Li",
      "Han Wang",
      "Can Huang",
      "Bin Li"
    ],
    "date": "2025-03-06",
    "summary": "The advent of Large Vision-Language Models (LVLMs) has advanced the\nvideo-based tasks, such as video captioning and video understanding. Some\nprevious research indicates that taking texts in videos as input can further\nimprove the performance of video understanding. As a type of indispensable\ninformation in short videos or movies, subtitles can assist LVLMs to better\nunderstand videos. Most existing methods for video subtitle extraction are\nbased on a multi-stage framework, handling each frame independently. They can\nhardly exploit the temporal information of videos. Although some LVLMs exhibit\nthe robust OCR capability, predicting accurate timestamps for subtitle texts is\nstill challenging. In this paper, we propose an End-to-end Video Subtitle\nExtraction method, called EVE, which consists of three modules: a vision\nencoder, an adapter module, and a large language model. To effectively compress\nthe visual tokens from the vision encoder, we propose a novel adapter\nInterleavedVT to interleave two modalities. It contains a visual compressor and\na textual region compressor. The proposed InterleavedVT exploits both the\nmerits of average pooling and Q-Former in token compression. Taking the\ntemporal information of videos into account, we introduce a sliding-window\nmechanism in the textual region compressor. To benchmark the video subtitle\nextraction task, we propose a large dataset ViSa including 2.5M videos.\nExtensive experiments on ViSa demonstrate that the proposed EVE can outperform\nexisting open-sourced tools and LVLMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_04034v1",
    "title": "GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world\n  Scene Understanding",
    "url": "http://arxiv.org/abs/2503.04034v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04034v1",
    "authors": [
      "Xihan Wang",
      "Dianyi Yang",
      "Yu Gao",
      "Yufeng Yue",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in 3D Gaussian Splatting(3DGS) have significantly\nimproved semantic scene understanding, enabling natural language queries to\nlocalize objects within a scene. However, existing methods primarily focus on\nembedding compressed CLIP features to 3D Gaussians, suffering from low object\nsegmentation accuracy and lack spatial reasoning capabilities. To address these\nlimitations, we propose GaussianGraph, a novel framework that enhances\n3DGS-based scene understanding by integrating adaptive semantic clustering and\nscene graph generation. We introduce a \"Control-Follow\" clustering strategy,\nwhich dynamically adapts to scene scale and feature distribution, avoiding\nfeature compression and significantly improving segmentation accuracy.\nAdditionally, we enrich scene representation by integrating object attributes\nand spatial relations extracted from 2D foundation models. To address\ninaccuracies in spatial relationships, we propose 3D correction modules that\nfilter implausible relations through spatial consistency verification, ensuring\nreliable scene graph construction. Extensive experiments on three datasets\ndemonstrate that GaussianGraph outperforms state-of-the-art methods in both\nsemantic segmentation and object grounding tasks, providing a robust solution\nfor complex scene understanding and interaction.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_04006v1",
    "title": "DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for\n  Robust Few-Shot Segmentation",
    "url": "http://arxiv.org/abs/2503.04006v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04006v1",
    "authors": [
      "Amin Karimi",
      "Charalambos Poullis"
    ],
    "date": "2025-03-06",
    "summary": "Few-shot semantic segmentation (FSS) aims to enable models to segment\nnovel/unseen object classes using only a limited number of labeled examples.\nHowever, current FSS methods frequently struggle with generalization due to\nincomplete and biased feature representations, especially when support images\ndo not capture the full appearance variability of the target class. To improve\nthe FSS pipeline, we propose a novel framework that utilizes large language\nmodels (LLMs) to adapt general class semantic information to the query image.\nFurthermore, the framework employs dense pixel-wise matching to identify\nsimilarities between query and support images, resulting in enhanced FSS\nperformance. Inspired by reasoning-based segmentation frameworks, our method,\nnamed DSV-LFS, introduces an additional token into the LLM vocabulary, allowing\na multimodal LLM to generate a \"semantic prompt\" from class descriptions. In\nparallel, a dense matching module identifies visual similarities between the\nquery and support images, generating a \"visual prompt\". These prompts are then\njointly employed to guide the prompt-based decoder for accurate segmentation of\nthe query image. Comprehensive experiments on the benchmark datasets\nPascal-$5^{i}$ and COCO-$20^{i}$ demonstrate that our framework achieves\nstate-of-the-art performance-by a significant margin-demonstrating superior\ngeneralization to novel classes and robustness across diverse scenarios. The\nsource code is available at\n\\href{https://github.com/aminpdik/DSV-LFS}{https://github.com/aminpdik/DSV-LFS}",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03987v1",
    "title": "RetinalGPT: A Retinal Clinical Preference Conversational Assistant\n  Powered by Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.03987v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03987v1",
    "authors": [
      "Wenhui Zhu",
      "Xin Li",
      "Xiwen Chen",
      "Peijie Qiu",
      "Vamsi Krishna Vasa",
      "Xuanzhao Dong",
      "Yanxi Chen",
      "Natasha Lepore",
      "Oana Dumitrascu",
      "Yi Su",
      "Yalin Wang"
    ],
    "date": "2025-03-06",
    "summary": "Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention for their remarkable ability to process and analyze non-textual data,\nsuch as images, videos, and audio. Notably, several adaptations of\ngeneral-domain MLLMs to the medical field have been explored, including\nLLaVA-Med. However, these medical adaptations remain insufficiently advanced in\nunderstanding and interpreting retinal images. In contrast, medical experts\nemphasize the importance of quantitative analyses for disease detection and\ninterpretation. This underscores a gap between general-domain and\nmedical-domain MLLMs: while general-domain MLLMs excel in broad applications,\nthey lack the specialized knowledge necessary for precise diagnostic and\ninterpretative tasks in the medical field. To address these challenges, we\nintroduce \\textit{RetinalGPT}, a multimodal conversational assistant for\nclinically preferred quantitative analysis of retinal images. Specifically, we\nachieve this by compiling a large retinal image dataset, developing a novel\ndata pipeline, and employing customized visual instruction tuning to enhance\nboth retinal analysis and enrich medical knowledge. In particular, RetinalGPT\noutperforms MLLM in the generic domain by a large margin in the diagnosis of\nretinal diseases in 8 benchmark retinal datasets. Beyond disease diagnosis,\nRetinalGPT features quantitative analyses and lesion localization, representing\na pioneering step in leveraging LLMs for an interpretable and end-to-end\nclinical research framework. The code is available at\nhttps://github.com/Retinal-Research/RetinalGPT",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_05543v1",
    "title": "Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of\n  Diagrammatic Information",
    "url": "http://arxiv.org/abs/2503.05543v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05543v1",
    "authors": [
      "Junbo Zhao",
      "Ting Zhang",
      "Jiayu Sun",
      "Mi Tian",
      "Hua Huang"
    ],
    "date": "2025-03-06",
    "summary": "Geometry problem solving has garnered increasing attention due to its\npotential applications in intelligent education field. Inspired by the\nobservation that text often introduces ambiguities that diagrams can clarify,\nthis paper presents Pi-GPS, a novel framework that unleashes the power of\ndiagrammatic information to resolve textual ambiguities, an aspect largely\noverlooked in prior research. Specifically, we design a micro module comprising\na rectifier and verifier: the rectifier employs MLLMs to disambiguate text\nbased on the diagrammatic context, while the verifier ensures the rectified\noutput adherence to geometric rules, mitigating model hallucinations.\nAdditionally, we explore the impact of LLMs in theorem predictor based on the\ndisambiguated formal language. Empirical results demonstrate that Pi-GPS\nsurpasses state-of-the-art models, achieving a nearly 10\\% improvement on\nGeometry3K over prior neural-symbolic approaches. We hope this work highlights\nthe significance of resolving textual ambiguity in multimodal mathematical\nreasoning, a crucial factor limiting performance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ]
  },
  {
    "id": "paperswithcode_Supervised_Multimodal_Bitransformers_for_Classifyi",
    "title": "Supervised Multimodal Bitransformers for Classifying Images and Text",
    "url": "https://paperswithcode.com/paper/supervised-multimodal-bitransformers-for",
    "authors": [],
    "date": "2025-03-06",
    "summary": "Self-supervised bidirectional transformer models such as BERT have led to dramatic improvements in a wide variety of textual classification tasks.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null
  },
  {
    "id": "2503_04982v1",
    "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
    "url": "http://arxiv.org/abs/2503.04982v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04982v1",
    "authors": [
      "Souvik Kundu",
      "Anahita Bhiwandiwalla",
      "Sungduk Yu",
      "Phillip Howard",
      "Tiep Le",
      "Sharath Nittur Sridhar",
      "David Cobbley",
      "Hao Kang",
      "Vasudev Lal"
    ],
    "date": "2025-03-06",
    "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "visual reasoning"
    ]
  },
  {
    "id": "2503_04971v1",
    "title": "Incentivizing Multi-Tenant Split Federated Learning for Foundation\n  Models at the Network Edge",
    "url": "http://arxiv.org/abs/2503.04971v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04971v1",
    "authors": [
      "Songyuan Li",
      "Jia Hu",
      "Geyong Min",
      "Haojun Huang"
    ],
    "date": "2025-03-06",
    "summary": "Foundation models (FMs) such as GPT-4 exhibit exceptional generative\ncapabilities across diverse downstream tasks through fine-tuning. Split\nFederated Learning (SFL) facilitates privacy-preserving FM fine-tuning on\nresource-constrained local devices by offloading partial FM computations to\nedge servers, enabling device-edge synergistic fine-tuning. Practical edge\nnetworks often host multiple SFL tenants to support diversified downstream\ntasks. However, existing research primarily focuses on single-tenant SFL\nscenarios, and lacks tailored incentive mechanisms for multi-tenant settings,\nwhich are essential to effectively coordinate self-interested local devices for\nparticipation in various downstream tasks, ensuring that each SFL tenant's\ndistinct FM fine-tuning requirements (e.g., FM types, performance targets, and\nfine-tuning deadlines) are met. To address this gap, we propose a novel\nPrice-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offer\nstrategic price incentives, which solicit high-quality device participation for\nefficient FM fine-tuning. Specifically, we first develop a bias-resilient\nglobal SFL model aggregation scheme to eliminate model biases caused by\nindependent device participation. We then derive a rigorous SFL convergence\nbound to evaluate the contributions of heterogeneous devices to FM performance\nimprovements, guiding the incentive strategies of SFL tenants. Furthermore, we\nmodel inter-tenant device competition as a congestion game for Stackelberg\nequilibrium (SE) analysis, deriving each SFL tenant's optimal incentive\nstrategy. Extensive simulations involving four representative SFL tenant types\n(ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images,\nand audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07x\ncompared to state-of-the-art approaches, while consistently meeting fine-tuning\nperformance targets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ]
  },
  {
    "id": "2503_04919v1",
    "title": "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D\n  Object Placement",
    "url": "http://arxiv.org/abs/2503.04919v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04919v1",
    "authors": [
      "Ian Huang",
      "Yanan Bao",
      "Karen Truong",
      "Howard Zhou",
      "Cordelia Schmid",
      "Leonidas Guibas",
      "Alireza Fathi"
    ],
    "date": "2025-03-06",
    "summary": "Scene generation with 3D assets presents a complex challenge, requiring both\nhigh-level semantic understanding and low-level geometric reasoning. While\nMultimodal Large Language Models (MLLMs) excel at semantic tasks, their\napplication to 3D scene generation is hindered by their limited grounding on 3D\ngeometry. In this paper, we investigate how to best work with MLLMs in an\nobject placement task. Towards this goal, we introduce a novel framework,\nFirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the\nextraction of relevant geometric details from the 3D scene, (2) constructing\nand solving geometric constraints on the extracted low-level geometry, and (3)\npruning for final placements that conform to common sense. By combining\ngeometric reasoning with real-world understanding of MLLMs, our method can\npropose object placements that satisfy both geometric constraints as well as\nhigh-level semantic common-sense considerations. Our experiments show that\nthese capabilities allow our method to place objects more effectively in\ncomplex scenes with intricate geometry, surpassing the quality of prior work.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ]
  },
  {
    "id": "2503_04918v1",
    "title": "Fine-Tuning Florence2 for Enhanced Object Detection in Un-constructed\n  Environments: Vision-Language Model Approach",
    "url": "http://arxiv.org/abs/2503.04918v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04918v1",
    "authors": [
      "Soumyadeep Ro",
      "Sanapala Satwika",
      "Pamarthi Yasoda Gayathri",
      "Mohmmad Ghaith Balsha",
      "Aysegul Ucar"
    ],
    "date": "2025-03-06",
    "summary": "Artificial intelligence has progressed through the development of\nVision-Language Models (VLMs), which integrate text and visual inputs to\nachieve comprehensive understanding and interaction in various contexts.\nEnhancing the performance of these models such as the transformer based\nFlorence 2 on specialized tasks like object detection in complex and\nunstructured environments requires fine-tuning. The goal of this paper is to\nimprove the efficiency of the Florence 2 model in challenging environments by\nfinetuning it. We accomplished this by experimenting with different\nconfigurations, using various GPU types (T4, L4, A100) and optimizers such as\nAdamW and SGD. We also employed a range of learning rates and LoRA (Low Rank\nAdaptation) settings. Analyzing the performance metrics, such as Mean Average\nPrecision (mAP) scores,reveals that the finetuned Florence 2 models performed\ncomparably to YOLO models, including YOLOv8, YOLOv9, and YOLOv10. This\ndemonstrates how transformer based VLMs can be adapted for detailed object\ndetection tasks. The paper emphasizes the capability of optimized transformer\nbased VLMs to address specific challenges in object detection within\nunstructured environments, opening up promising avenues for practical\napplications in demanding and complex settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_04900v1",
    "title": "Extracting Symbolic Sequences from Visual Representations via\n  Self-Supervised Learning",
    "url": "http://arxiv.org/abs/2503.04900v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04900v1",
    "authors": [
      "Victor Sebastian Martinez Pozos",
      "Ivan Vladimir Meza Ruiz"
    ],
    "date": "2025-03-06",
    "summary": "This paper explores the potential of abstracting complex visual information\ninto discrete, structured symbolic sequences using self-supervised learning\n(SSL). Inspired by how language abstracts and organizes information to enable\nbetter reasoning and generalization, we propose a novel approach for generating\nsymbolic representations from visual data. To learn these sequences, we extend\nthe DINO framework to handle visual and symbolic information. Initial\nexperiments suggest that the generated symbolic sequences capture a meaningful\nlevel of abstraction, though further refinement is required. An advantage of\nour method is its interpretability: the sequences are produced by a decoder\ntransformer using cross-attention, allowing attention maps to be linked to\nspecific symbols and offering insight into how these representations correspond\nto image regions. This approach lays the foundation for creating interpretable\nsymbolic representations with potential applications in high-level scene\nunderstanding.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "DINO"
    ]
  },
  {
    "id": "2503_04606v1",
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion\n  Models for Video Generation",
    "url": "http://arxiv.org/abs/2503.04606v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04606v1",
    "authors": [
      "Aoxiong Yin",
      "Kai Shen",
      "Yichong Leng",
      "Xu Tan",
      "Xinyu Zhou",
      "Juncheng Li",
      "Siliang Tang"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Keling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-video",
      "Sora"
    ]
  },
  {
    "id": "2503_04871v1",
    "title": "Toward Lightweight and Fast Decoders for Diffusion Models in Image and\n  Video Generation",
    "url": "http://arxiv.org/abs/2503.04871v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04871v1",
    "authors": [
      "Alexey Buzovkin",
      "Evgeny Shilov"
    ],
    "date": "2025-03-06",
    "summary": "We investigate methods to reduce inference time and memory footprint in\nstable diffusion models by introducing lightweight decoders for both image and\nvideo synthesis. Traditional latent diffusion pipelines rely on large\nVariational Autoencoder decoders that can slow down generation and consume\nconsiderable GPU memory. We propose custom-trained decoders using lightweight\nVision Transformer and Taming Transformer architectures. Experiments show up to\n15% overall speed-ups for image generation on COCO2017 and up to 20 times\nfaster decoding in the sub-module, with additional gains on UCF-101 for video\ntasks. Memory requirements are moderately reduced, and while there is a small\ndrop in perceptual quality compared to the default decoder, the improvements in\nspeed and scalability are crucial for large-scale inference scenarios such as\ngenerating 100K images. Our work is further contextualized by advances in\nefficient video generation, including dual masking strategies, illustrating a\nbroader effort to improve the scalability and efficiency of generative models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion",
      "vision transformer",
      "image generation"
    ]
  },
  {
    "id": "2503_04308v1",
    "title": "Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses\n  in Human-Robot Bartending Tasks",
    "url": "http://arxiv.org/abs/2503.04308v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04308v1",
    "authors": [
      "Luk\u00e1\u0161 Gajdo\u0161ech",
      "Hassan Ali",
      "Jan-Gerrit Habekost",
      "Martin Madaras",
      "Matthias Kerzel",
      "Stefan Wermter"
    ],
    "date": "2025-03-06",
    "summary": "Datasets for object detection often do not account for enough variety of\nglasses, due to their transparent and reflective properties. Specifically,\nopen-vocabulary object detectors, widely used in embodied robotic agents, fail\nto distinguish subclasses of glasses. This scientific gap poses an issue to\nrobotic applications that suffer from accumulating errors between detection,\nplanning, and action execution. The paper introduces a novel method for the\nacquisition of real-world data from RGB-D sensors that minimizes human effort.\nWe propose an auto-labeling pipeline that generates labels for all the acquired\nframes based on the depth measurements. We provide a novel real-world glass\nobject dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a\nhumanoid robot platform. The data set consists of 7850 images recorded from\nfive different cameras. We show that our trained baseline model outperforms\nstate-of-the-art open-vocabulary approaches. In addition, we deploy our\nbaseline model in an embodied agent approach to the NICOL platform, on which it\nachieves a success rate of 81% in a human-robot bartending scenario.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual understanding"
    ]
  },
  {
    "id": "2503_04215v1",
    "title": "Energy-Guided Optimization for Personalized Image Editing with\n  Pretrained Text-to-Image Diffusion Models",
    "url": "http://arxiv.org/abs/2503.04215v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04215v1",
    "authors": [
      "Rui Jiang",
      "Xinghe Fu",
      "Guangcong Zheng",
      "Teng Li",
      "Taiping Yao",
      "Xi Li"
    ],
    "date": "2025-03-06",
    "summary": "The rapid advancement of pretrained text-driven diffusion models has\nsignificantly enriched applications in image generation and editing. However,\nas the demand for personalized content editing increases, new challenges emerge\nespecially when dealing with arbitrary objects and complex scenes. Existing\nmethods usually mistakes mask as the object shape prior, which struggle to\nachieve a seamless integration result. The mostly used inversion noise\ninitialization also hinders the identity consistency towards the target object.\nTo address these challenges, we propose a novel training-free framework that\nformulates personalized content editing as the optimization of edited images in\nthe latent space, using diffusion models as the energy function guidance\nconditioned by reference text-image pairs. A coarse-to-fine strategy is\nproposed that employs text energy guidance at the early stage to achieve a\nnatural transition toward the target class and uses point-to-point\nfeature-level image energy guidance to perform fine-grained appearance\nalignment with the target object. Additionally, we introduce the latent space\ncontent composition to enhance overall identity consistency with the target.\nExtensive experiments demonstrate that our method excels in object replacement\neven with a large domain gap, highlighting its potential for high-quality,\npersonalized image editing.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ]
  },
  {
    "id": "2503_04858v1",
    "title": "SHAPE : Self-Improved Visual Preference Alignment by Iteratively\n  Generating Holistic Winner",
    "url": "http://arxiv.org/abs/2503.04858v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04858v1",
    "authors": [
      "Kejia Chen",
      "Jiawen Zhang",
      "Jiacong Hu",
      "Jiazhen Yang",
      "Jian Lou",
      "Zunlei Feng",
      "Mingli Song"
    ],
    "date": "2025-03-06",
    "summary": "Large Visual Language Models (LVLMs) increasingly rely on preference\nalignment to ensure reliability, which steers the model behavior via preference\nfine-tuning on preference data structured as ``image - winner text - loser\ntext'' triplets. However, existing approaches often suffer from limited\ndiversity and high costs associated with human-annotated preference data,\nhindering LVLMs from fully achieving their intended alignment capabilities. We\npresent \\projectname, a self-supervised framework capable of transforming the\nalready abundant supervised text-image pairs into holistic preference triplets\nfor more effective and cheaper LVLM alignment, eliminating the need for human\npreference annotations. Our approach facilitates LVLMs in progressively\nenhancing alignment capabilities through iterative self-improvement. The key\ndesign rationale is to devise preference triplets where the winner text\nconsistently improves in holisticness and outperforms the loser response in\nquality, thereby pushing the model to ``strive to the utmost'' of alignment\nperformance through preference fine-tuning. For each given text-image pair,\nSHAPE introduces multiple visual augmentations and pairs them with a summarized\ntext to serve as the winner response, while designating the original text as\nthe loser response. Experiments across \\textbf{12} benchmarks on various model\narchitectures and sizes, including LLaVA and DeepSeek-VL, show that SHAPE\nachieves significant gains, for example, achieving +11.3\\% on MMVet\n(comprehensive evaluation), +1.4\\% on MMBench (general VQA), and +8.0\\% on POPE\n(hallucination robustness) over baselines in 7B models. Notably, qualitative\nanalyses confirm enhanced attention to visual details and better alignment with\nhuman preferences for holistic descriptions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "VQA"
    ]
  },
  {
    "id": "2503_04170v1",
    "title": "Towards Intelligent Transportation with Pedestrians and Vehicles\n  In-the-Loop: A Surveillance Video-Assisted Federated Digital Twin Framework",
    "url": "http://arxiv.org/abs/2503.04170v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04170v1",
    "authors": [
      "Xiaolong Li",
      "Jianhao Wei",
      "Haidong Wang",
      "Li Dong",
      "Ruoyang Chen",
      "Changyan Yi",
      "Jun Cai",
      "Dusit Niyato",
      "Xuemin",
      "Shen"
    ],
    "date": "2025-03-06",
    "summary": "In intelligent transportation systems (ITSs), incorporating pedestrians and\nvehicles in-the-loop is crucial for developing realistic and safe traffic\nmanagement solutions. However, there is falls short of simulating complex\nreal-world ITS scenarios, primarily due to the lack of a digital twin\nimplementation framework for characterizing interactions between pedestrians\nand vehicles at different locations in different traffic environments. In this\narticle, we propose a surveillance video assisted federated digital twin\n(SV-FDT) framework to empower ITSs with pedestrians and vehicles in-the-loop.\nSpecifically, SVFDT builds comprehensive pedestrian-vehicle interaction models\nby leveraging multi-source traffic surveillance videos. Its architecture\nconsists of three layers: (i) the end layer, which collects traffic\nsurveillance videos from multiple sources; (ii) the edge layer, responsible for\nsemantic segmentation-based visual understanding, twin agent-based interaction\nmodeling, and local digital twin system (LDTS) creation in local regions; and\n(iii) the cloud layer, which integrates LDTSs across different regions to\nconstruct a global DT model in realtime. We analyze key design requirements and\nchallenges and present core guidelines for SVFDT's system implementation. A\ntestbed evaluation demonstrates its effectiveness in optimizing traffic\nmanagement. Comparisons with traditional terminal-server frameworks highlight\nSV-FDT's advantages in mirroring delays, recognition accuracy, and subjective\nevaluation. Finally, we identify some open challenges and discuss future\nresearch directions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual understanding"
    ]
  },
  {
    "id": "2503_04154v1",
    "title": "CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised\n  Monocular 3D Detection",
    "url": "http://arxiv.org/abs/2503.04154v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04154v1",
    "authors": [
      "Chupeng Liu",
      "Runkai Zhao",
      "Weidong Cai"
    ],
    "date": "2025-03-06",
    "summary": "Weakly supervised monocular 3D detection, while less annotation-intensive,\noften struggles to capture the global context required for reliable 3D\nreasoning. Conventional label-efficient methods focus on object-centric\nfeatures, neglecting contextual semantic relationships that are critical in\ncomplex scenes. In this work, we propose a Context-Aware Weak Supervision for\nMonocular 3D object detection, namely CA-W3D, to address this limitation in a\ntwo-stage training paradigm. Specifically, we first introduce a pre-training\nstage employing Region-wise Object Contrastive Matching (ROCM), which aligns\nregional object embeddings derived from a trainable monocular 3D encoder and a\nfrozen open-vocabulary 2D visual grounding model. This alignment encourages the\nmonocular encoder to discriminate scene-specific attributes and acquire richer\ncontextual knowledge. In the second stage, we incorporate a pseudo-label\ntraining process with a Dual-to-One Distillation (D2OD) mechanism, which\neffectively transfers contextual priors into the monocular encoder while\npreserving spatial fidelity and maintaining computational efficiency during\ninference. Extensive experiments conducted on the public KITTI benchmark\ndemonstrate the effectiveness of our approach, surpassing the SoTA method over\nall metrics, highlighting the importance of contextual-aware knowledge in\nweakly-supervised monocular 3D detection.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual grounding"
    ]
  },
  {
    "id": "2503_04107v1",
    "title": "Fractional Correspondence Framework in Detection Transformer",
    "url": "http://arxiv.org/abs/2503.04107v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04107v1",
    "authors": [
      "Masoumeh Zareapoor",
      "Pourya Shamsolmoali",
      "Huiyu Zhou",
      "Yue Lu",
      "Salvador Garc\u00eda"
    ],
    "date": "2025-03-06",
    "summary": "The Detection Transformer (DETR), by incorporating the Hungarian algorithm,\nhas significantly simplified the matching process in object detection tasks.\nThis algorithm facilitates optimal one-to-one matching of predicted bounding\nboxes to ground-truth annotations during training. While effective, this strict\nmatching process does not inherently account for the varying densities and\ndistributions of objects, leading to suboptimal correspondences such as failing\nto handle multiple detections of the same object or missing small objects. To\naddress this, we propose the Regularized Transport Plan (RTP). RTP introduces a\nflexible matching strategy that captures the cost of aligning predictions with\nground truths to find the most accurate correspondences between these sets. By\nutilizing the differentiable Sinkhorn algorithm, RTP allows for soft,\nfractional matching rather than strict one-to-one assignments. This approach\nenhances the model's capability to manage varying object densities and\ndistributions effectively. Our extensive evaluations on the MS-COCO and VOC\nbenchmarks demonstrate the effectiveness of our approach. RTP-DETR, surpassing\nthe performance of the Deform-DETR and the recently introduced DINO-DETR,\nachieving absolute gains in mAP of +3.8% and +1.7%, respectively.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "DINO"
    ]
  },
  {
    "id": "2503_04852v1",
    "title": "CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data",
    "url": "http://arxiv.org/abs/2503.04852v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04852v1",
    "authors": [
      "Disheng Liu",
      "Yiran Qiao",
      "Wuche Liu",
      "Yiren Lu",
      "Yunlai Zhou",
      "Tuo Liang",
      "Yu Yin",
      "Jing Ma"
    ],
    "date": "2025-03-06",
    "summary": "True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "ViT"
    ]
  },
  {
    "id": "2503_04050v1",
    "title": "Underlying Semantic Diffusion for Effective and Efficient In-Context\n  Learning",
    "url": "http://arxiv.org/abs/2503.04050v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04050v1",
    "authors": [
      "Zhong Ji",
      "Weilong Cao",
      "Yan Zhang",
      "Yanwei Pang",
      "Jungong Han",
      "Xuelong Li"
    ],
    "date": "2025-03-06",
    "summary": "Diffusion models has emerged as a powerful framework for tasks like image\ncontrollable generation and dense prediction. However, existing models often\nstruggle to capture underlying semantics (e.g., edges, textures, shapes) and\neffectively utilize in-context learning, limiting their contextual\nunderstanding and image generation quality. Additionally, high computational\ncosts and slow inference speeds hinder their real-time applicability. To\naddress these challenges, we propose Underlying Semantic Diffusion\n(US-Diffusion), an enhanced diffusion model that boosts underlying semantics\nlearning, computational efficiency, and in-context learning capabilities on\nmulti-task scenarios. We introduce Separate & Gather Adapter (SGA), which\ndecouples input conditions for different tasks while sharing the architecture,\nenabling better in-context learning and generalization across diverse visual\ndomains. We also present a Feedback-Aided Learning (FAL) framework, which\nleverages feedback signals to guide the model in capturing semantic details and\ndynamically adapting to task-specific contextual cues. Furthermore, we propose\na plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time\nsteps with high-noise levels, which aims at optimizing training and inference\nefficiency while maintaining strong in-context learning performance.\nExperimental results demonstrate that US-Diffusion outperforms the\nstate-of-the-art method, achieving an average reduction of 7.47 in FID on\nMap2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks,\nwhile achieving approximately 9.45 times faster inference speed. Our method\nalso demonstrates superior training efficiency and in-context learning\ncapabilities, excelling in new datasets and tasks, highlighting its robustness\nand adaptability across diverse visual domains.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ]
  },
  {
    "id": "2503_03947v1",
    "title": "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.03947v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03947v1",
    "authors": [
      "Aurelio Noca",
      "Xianmei Lei",
      "Jonathan Becktor",
      "Jeffrey Edlund",
      "Anna Sabel",
      "Patrick Spieler",
      "Curtis Padgett",
      "Alexandre Alahi",
      "Deegan Atha"
    ],
    "date": "2025-03-05",
    "summary": "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_03935v1",
    "title": "GlucoLens: Explainable Postprandial Blood Glucose Prediction from Diet\n  and Physical Activity",
    "url": "http://arxiv.org/abs/2503.03935v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03935v1",
    "authors": [
      "Abdullah Mamun",
      "Asiful Arefeen",
      "Susan B. Racette",
      "Dorothy D. Sears",
      "Corrie M. Whisner",
      "Matthew P. Buman",
      "Hassan Ghasemzadeh"
    ],
    "date": "2025-03-05",
    "summary": "Postprandial hyperglycemia, marked by the blood glucose level exceeding the\nnormal range after meals, is a critical indicator of progression toward type 2\ndiabetes in prediabetic and healthy individuals. A key metric for understanding\nblood glucose dynamics after eating is the postprandial area under the curve\n(PAUC). Predicting PAUC in advance based on a person's diet and activity level\nand explaining what affects postprandial blood glucose could allow an\nindividual to adjust their lifestyle accordingly to maintain normal glucose\nlevels. In this paper, we propose GlucoLens, an explainable machine learning\napproach to predict PAUC and hyperglycemia from diet, activity, and recent\nglucose patterns. We conducted a five-week user study with 10 full-time working\nindividuals to develop and evaluate the computational model. Our machine\nlearning model takes multimodal data including fasting glucose, recent glucose,\nrecent activity, and macronutrient amounts, and provides an interpretable\nprediction of the postprandial glucose pattern. Our extensive analyses of the\ncollected data revealed that the trained model achieves a normalized root mean\nsquared error (NRMSE) of 0.123. On average, GlucoLense with a Random Forest\nbackbone provides a 16% better result than the baseline models. Additionally,\nGlucoLens predicts hyperglycemia with an accuracy of 74% and recommends\ndifferent options to help avoid hyperglycemia through diverse counterfactual\nexplanations. Code available: https://github.com/ab9mamun/GlucoLens.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03854v1",
    "title": "Vision-Language Models Struggle to Align Entities across Modalities",
    "url": "http://arxiv.org/abs/2503.03854v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03854v1",
    "authors": [
      "I\u00f1igo Alonso",
      "Ander Salaberria",
      "Gorka Azkune",
      "Jeremy Barnes",
      "Oier Lopez de Lacalle"
    ],
    "date": "2025-03-05",
    "summary": "Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_03848v1",
    "title": "Nexar Dashcam Collision Prediction Dataset and Challenge",
    "url": "http://arxiv.org/abs/2503.03848v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03848v1",
    "authors": [
      "Daniel C. Moura",
      "Shizhan Zhu",
      "Orly Zvitia"
    ],
    "date": "2025-03-05",
    "summary": "This paper presents the Nexar Dashcam Collision Prediction Dataset and\nChallenge, designed to support research in traffic event analysis, collision\nprediction, and autonomous vehicle safety. The dataset consists of 1,500\nannotated video clips, each approximately 40 seconds long, capturing a diverse\nrange of real-world traffic scenarios. Videos are labeled with event type\n(collision/near-collision vs. normal driving), environmental conditions\n(lighting conditions and weather), and scene type (urban, rural, highway,\netc.). For collision and near-collision cases, additional temporal labels are\nprovided, including the precise moment of the event and the alert time, marking\nwhen the collision first becomes predictable.\n  To advance research on accident prediction, we introduce the Nexar Dashcam\nCollision Prediction Challenge, a public competition on top of this dataset.\nParticipants are tasked with developing machine learning models that predict\nthe likelihood of an imminent collision, given an input video. Model\nperformance is evaluated using the average precision (AP) computed across\nmultiple intervals before the accident (i.e. 500 ms, 1000 ms, and 1500 ms prior\nto the event), emphasizing the importance of early and reliable predictions.\n  The dataset is released under an open license with restrictions on unethical\nuse, ensuring responsible research and innovation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_03840v1",
    "title": "Decoupling the components of geometric understanding in Vision Language\n  Models",
    "url": "http://arxiv.org/abs/2503.03840v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03840v1",
    "authors": [
      "Eliza Kosoy",
      "Annya Dahmani",
      "Andrew K. Lampinen",
      "Iulia M. Comsa",
      "Soojin Jeong",
      "Ishita Dasgupta",
      "Kelsey Allen"
    ],
    "date": "2025-03-05",
    "summary": "Understanding geometry relies heavily on vision. In this work, we evaluate\nwhether state-of-the-art vision language models (VLMs) can understand simple\ngeometric concepts. We use a paradigm from cognitive science that isolates\nvisual understanding of simple geometry from the many other capabilities it is\noften conflated with such as reasoning and world knowledge. We compare model\nperformance with human adults from the USA, as well as with prior research on\nhuman adults without formal education from an Amazonian indigenous group. We\nfind that VLMs consistently underperform both groups of human adults, although\nthey succeed with some concepts more than others. We also find that VLM\ngeometric understanding is more brittle than human understanding, and is not\nrobust when tasks require mental rotation. This work highlights interesting\ndifferences in the origin of geometric understanding in humans and machines --\ne.g. from printed materials used in formal education vs. interactions with the\nphysical world or a combination of the two -- and a small step toward\nunderstanding these differences.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_03743v1",
    "title": "CHOP: Mobile Operating Assistant with Constrained High-frequency\n  Optimized Subtask Planning",
    "url": "http://arxiv.org/abs/2503.03743v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03743v1",
    "authors": [
      "Yuqi Zhou",
      "Shuai Wang",
      "Sunhao Dai",
      "Qinglin Jia",
      "Zhaocheng Du",
      "Zhenhua Dong",
      "Jun Xu"
    ],
    "date": "2025-03-05",
    "summary": "The advancement of visual language models (VLMs) has enhanced mobile device\noperations, allowing simulated human-like actions to address user requirements.\nCurrent VLM-based mobile operating assistants can be structured into three\nlevels: task, subtask, and action. The subtask level, linking high-level goals\nwith low-level executable actions, is crucial for task completion but faces two\nchallenges: ineffective subtasks that lower-level agent cannot execute and\ninefficient subtasks that fail to contribute to the completion of the\nhigher-level task. These challenges stem from VLM's lack of experience in\ndecomposing subtasks within GUI scenarios in multi-agent architecture. To\naddress these, we propose a new mobile assistant architecture with constrained\nhigh-frequency o}ptimized planning (CHOP). Our approach overcomes the VLM's\ndeficiency in GUI scenarios planning by using human-planned subtasks as the\nbasis vector. We evaluate our architecture in both English and Chinese contexts\nacross 20 Apps, demonstrating significant improvements in both effectiveness\nand efficiency. Our dataset and code is available at\nhttps://github.com/Yuqi-Zhou/CHOP",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ]
  },
  {
    "id": "2503_03803v1",
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "url": "http://arxiv.org/abs/2503.03803v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03803v1",
    "authors": [
      "Jingkang Yang",
      "Shuai Liu",
      "Hongming Guo",
      "Yuhao Dong",
      "Xiamengwei Zhang",
      "Sicheng Zhang",
      "Pengyun Wang",
      "Zitang Zhou",
      "Binzhu Xie",
      "Ziyue Wang",
      "Bei Ouyang",
      "Zhengyu Lin",
      "Marco Cominelli",
      "Zhongang Cai",
      "Yuanhan Zhang",
      "Peiyuan Zhang",
      "Fangzhou Hong",
      "Joerg Widmer",
      "Francesco Gringoli",
      "Lei Yang",
      "Bo Li",
      "Ziwei Liu"
    ],
    "date": "2025-03-05",
    "summary": "We introduce EgoLife, a project to develop an egocentric life assistant that\naccompanies and enhances personal efficiency through AI-powered wearable\nglasses. To lay the foundation for this assistant, we conducted a comprehensive\ndata collection study where six participants lived together for one week,\ncontinuously recording their daily activities - including discussions,\nshopping, cooking, socializing, and entertainment - using AI glasses for\nmultimodal egocentric video capture, along with synchronized third-person-view\nvideo references. This effort resulted in the EgoLife Dataset, a comprehensive\n300-hour egocentric, interpersonal, multiview, and multimodal daily life\ndataset with intensive annotation. Leveraging this dataset, we introduce\nEgoLifeQA, a suite of long-context, life-oriented question-answering tasks\ndesigned to provide meaningful assistance in daily life by addressing practical\nquestions such as recalling past relevant events, monitoring health habits, and\noffering personalized recommendations. To address the key technical challenges\nof (1) developing robust visual-audio models for egocentric data, (2) enabling\nidentity recognition, and (3) facilitating long-context question answering over\nextensive temporal information, we introduce EgoButler, an integrated system\ncomprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on\negocentric datasets, achieving state-of-the-art performance on egocentric video\nunderstanding. EgoRAG is a retrieval-based component that supports answering\nultra-long-context questions. Our experimental studies verify their working\nmechanisms and reveal critical factors and bottlenecks, guiding future\nimprovements. By releasing our datasets, models, and benchmarks, we aim to\nstimulate further research in egocentric AI assistants.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03734v1",
    "title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction",
    "url": "http://arxiv.org/abs/2503.03734v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03734v1",
    "authors": [
      "Huang Huang",
      "Fangchen Liu",
      "Letian Fu",
      "Tingfan Wu",
      "Mustafa Mukadam",
      "Jitendra Malik",
      "Ken Goldberg",
      "Pieter Abbeel"
    ],
    "date": "2025-03-05",
    "summary": "Vision-Language-Action (VLA) models aim to predict robotic actions based on\nvisual observations and language instructions. Existing approaches require\nfine-tuning pre-trained visionlanguage models (VLMs) as visual and language\nfeatures are independently fed into downstream policies, degrading the\npre-trained semantic alignments. We propose OTTER, a novel VLA architecture\nthat leverages these existing alignments through explicit, text-aware visual\nfeature extraction. Instead of processing all visual features, OTTER\nselectively extracts and passes only task-relevant visual features that are\nsemantically aligned with the language instruction to the policy transformer.\nThis allows OTTER to keep the pre-trained vision-language encoders frozen.\nThereby, OTTER preserves and utilizes the rich semantic understanding learned\nfrom large-scale pre-training, enabling strong zero-shot generalization\ncapabilities. In simulation and real-world experiments, OTTER significantly\noutperforms existing VLA models, demonstrating strong zeroshot generalization\nto novel objects and environments. Video, code, checkpoints, and dataset:\nhttps://ottervla.github.io/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ]
  },
  {
    "id": "2503_03689v1",
    "title": "DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with\n  Reward Guidance",
    "url": "http://arxiv.org/abs/2503.03689v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03689v1",
    "authors": [
      "Zhao Yang",
      "Zezhong Qian",
      "Xiaofan Li",
      "Weixiang Xu",
      "Gongpeng Zhao",
      "Ruohong Yu",
      "Lingsi Zhu",
      "Longjun Liu"
    ],
    "date": "2025-03-05",
    "summary": "Accurate and high-fidelity driving scene reconstruction demands the effective\nutilization of comprehensive scene information as conditional inputs. Existing\nmethods predominantly rely on 3D bounding boxes and BEV road maps for\nforeground and background control, which fail to capture the full complexity of\ndriving scenes and adequately integrate multimodal information. In this work,\nwe present DualDiff, a dual-branch conditional diffusion model designed to\nenhance driving scene generation across multiple views and video sequences.\nSpecifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional\ninput, offering rich foreground and background semantics alongside 3D spatial\ngeometry to precisely control the generation of both elements. To improve the\nsynthesis of fine-grained foreground objects, particularly complex and distant\nones, we propose a Foreground-Aware Mask (FGM) denoising loss function.\nAdditionally, we develop the Semantic Fusion Attention (SFA) mechanism to\ndynamically prioritize relevant information and suppress noise, enabling more\neffective multimodal fusion. Finally, to ensure high-quality image-to-video\ngeneration, we introduce the Reward-Guided Diffusion (RGD) framework, which\nmaintains global consistency and semantic coherence in generated videos.\nExtensive experiments demonstrate that DualDiff achieves state-of-the-art\n(SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff\nreduces the FID score by 4.09% compared to the best baseline. In downstream\ntasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and\nroad mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP\nincreases by 1.46%. Code will be made available at\nhttps://github.com/yangzhaojason/DualDiff.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03664v1",
    "title": "A Generative Approach to High Fidelity 3D Reconstruction from Text Data",
    "url": "http://arxiv.org/abs/2503.03664v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03664v1",
    "authors": [
      "Venkat Kumar R",
      "Deepak Saravanan"
    ],
    "date": "2025-03-05",
    "summary": "The convergence of generative artificial intelligence and advanced computer\nvision technologies introduces a groundbreaking approach to transforming\ntextual descriptions into three-dimensional representations. This research\nproposes a fully automated pipeline that seamlessly integrates text-to-image\ngeneration, various image processing techniques, and deep learning methods for\nreflection removal and 3D reconstruction. By leveraging state-of-the-art\ngenerative models like Stable Diffusion, the methodology translates natural\nlanguage inputs into detailed 3D models through a multi-stage workflow.\n  The reconstruction process begins with the generation of high-quality images\nfrom textual prompts, followed by enhancement by a reinforcement learning agent\nand reflection removal using the Stable Delight model. Advanced image upscaling\nand background removal techniques are then applied to further enhance visual\nfidelity. These refined two-dimensional representations are subsequently\ntransformed into volumetric 3D models using sophisticated machine learning\nalgorithms, capturing intricate spatial relationships and geometric\ncharacteristics. This process achieves a highly structured and detailed output,\nensuring that the final 3D models reflect both semantic accuracy and geometric\nprecision.\n  This approach addresses key challenges in generative reconstruction, such as\nmaintaining semantic coherence, managing geometric complexity, and preserving\ndetailed visual information. Comprehensive experimental evaluations will assess\nreconstruction quality, semantic accuracy, and geometric fidelity across\ndiverse domains and varying levels of complexity. By demonstrating the\npotential of AI-driven 3D reconstruction techniques, this research offers\nsignificant implications for fields such as augmented reality (AR), virtual\nreality (VR), and digital content creation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion",
      "text-to-image"
    ]
  },
  {
    "id": "2503_03663v2",
    "title": "LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant",
    "url": "http://arxiv.org/abs/2503.03663v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03663v2",
    "authors": [
      "Wei Li",
      "Bing Hu",
      "Rui Shao",
      "Leyang Shen",
      "Liqiang Nie"
    ],
    "date": "2025-03-05",
    "summary": "First-person video assistants are highly anticipated to enhance our daily\nlives through online video dialogue. However, existing online video assistants\noften sacrifice assistant efficacy for real-time efficiency by processing\nlow-frame-rate videos with coarse-grained visual features.To overcome the\ntrade-off between efficacy and efficiency, we propose \"Fast & Slow\nVideo-Language Thinker\" as an onLIne videO assistaNt, LION-FS, achieving\nreal-time, proactive, temporally accurate, and contextually precise responses.\nLION-FS adopts a two-stage optimization strategy: 1)Fast Path: Routing-Based\nResponse Determination evaluates frame-by-frame whether an immediate response\nis necessary. To enhance response determination accuracy and handle higher\nframe-rate inputs efficiently, we employ Token Aggregation Routing to\ndynamically fuse spatiotemporal features without increasing token numbers,\nwhile utilizing Token Dropping Routing to eliminate redundant features. 2)Slow\nPath: Multi-granularity Keyframe Augmentation optimizes keyframes during\nresponse generation. To provide comprehensive and detailed responses beyond\natomic actions constrained by training data, fine-grained spatial features and\nhuman-environment interaction features are extracted through multi-granular\npooling. These features are further integrated into a meticulously designed\nmultimodal Thinking Template to guide more precise response generation.\nComprehensive evaluations on online video tasks demonstrate that LION-FS\nachieves state-of-the-art efficacy and efficiency.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03651v1",
    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in\n  Multimodal Cycles",
    "url": "http://arxiv.org/abs/2503.03651v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03651v1",
    "authors": [
      "Rui Zhao",
      "Weijia Mao",
      "Mike Zheng Shou"
    ],
    "date": "2025-03-05",
    "summary": "Adapting generative models to specific domains presents an effective solution\nfor satisfying specialized requirements. However, adapting to some complex\ndomains remains challenging, especially when these domains require substantial\npaired data to capture the targeted distributions. Since unpaired data from a\nsingle modality, such as vision or language, is more readily available, we\nutilize the bidirectional mappings between vision and language learned by the\nunified generative model to enable training on unpaired data for domain\nadaptation. Specifically, we propose DoraCycle, which integrates two multimodal\ncycles: text-to-image-to-text and image-to-text-to-image. The model is\noptimized through cross-entropy loss computed at the cycle endpoints, where\nboth endpoints share the same modality. This facilitates self-evolution of the\nmodel without reliance on annotated text-image pairs. Experimental results\ndemonstrate that for tasks independent of paired knowledge, such as\nstylization, DoraCycle can effectively adapt the unified model using only\nunpaired data. For tasks involving new paired knowledge, such as specific\nidentities, a combination of a small set of paired image-text examples and\nlarger-scale unpaired data is sufficient for effective domain-oriented\nadaptation. The code will be released at https://github.com/showlab/DoraCycle.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "text-to-image",
      "image-to-text"
    ]
  },
  {
    "id": "2503_03644v2",
    "title": "DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating\n  Semantic Understanding of Dongba Pictograms",
    "url": "http://arxiv.org/abs/2503.03644v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03644v2",
    "authors": [
      "Xiaojun Bi",
      "Shuo Li",
      "Ziyue Wang",
      "Fuwen Luo",
      "Weizheng Qiao",
      "Lu Han",
      "Ziwei Sun",
      "Peng Li",
      "Yang Liu"
    ],
    "date": "2025-03-05",
    "summary": "Dongba pictographs are the only pictographs still in use in the world. They\nhave pictorial ideographic features, and their symbols carry rich cultural and\ncontextual information. Due to the lack of relevant datasets, existing research\nhas difficulty in advancing the study of semantic understanding of Dongba\npictographs. To this end, we propose DongbaMIE, the first multimodal dataset\nfor semantic understanding and extraction of Dongba pictographs. The dataset\nconsists of Dongba pictograph images and their corresponding Chinese semantic\nannotations. It contains 23,530 sentence-level and 2,539 paragraph-level\nimages, covering four semantic dimensions: objects, actions, relations, and\nattributes. We systematically evaluate the GPT-4o, Gemini-2.0, and Qwen2-VL\nmodels. Experimental results show that the F1 scores of GPT-4o and Gemini in\nthe best object extraction are only 3.16 and 3.11 respectively. The F1 score of\nQwen2-VL after supervised fine-tuning is only 11.49. These results suggest that\ncurrent large multimodal models still face significant challenges in accurately\nrecognizing the diverse semantic information in Dongba pictographs. The dataset\ncan be obtained from this URL.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03613v1",
    "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards\n  Zero-shot Adversarial Robustness of CLIP",
    "url": "http://arxiv.org/abs/2503.03613v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03613v1",
    "authors": [
      "Songlong Xing",
      "Zhengyu Zhao",
      "Nicu Sebe"
    ],
    "date": "2025-03-05",
    "summary": "Despite its prevalent use in image-text matching tasks in a zero-shot manner,\nCLIP has been shown to be highly vulnerable to adversarial perturbations added\nonto images. Recent studies propose to finetune the vision encoder of CLIP with\nadversarial samples generated on the fly, and show improved robustness against\nadversarial attacks on a spectrum of downstream datasets, a property termed as\nzero-shot robustness. In this paper, we show that malicious perturbations that\nseek to maximise the classification loss lead to `falsely stable' images, and\npropose to leverage the pre-trained vision encoder of CLIP to counterattack\nsuch adversarial images during inference to achieve robustness. Our paradigm is\nsimple and training-free, providing the first method to defend CLIP from\nadversarial attacks at test time, which is orthogonal to existing methods\naiming to boost zero-shot adversarial robustness of CLIP. We conduct\nexperiments across 16 classification datasets, and demonstrate stable and\nconsistent gains compared to test-time defence methods adapted from existing\nadversarial robustness studies that do not rely on external networks, without\nnoticeably impairing performance on clean images. We also show that our\nparadigm can be employed on CLIP models that have been adversarially finetuned\nto further enhance their robustness at test time. Our code is available\n\\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image-text",
      "CLIP"
    ]
  },
  {
    "id": "2503_03579v1",
    "title": "A Generative System for Robot-to-Human Handovers: from Intent Inference\n  to Spatial Configuration Imagery",
    "url": "http://arxiv.org/abs/2503.03579v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03579v1",
    "authors": [
      "Hanxin Zhang",
      "Abdulqader Dhafer",
      "Zhou Daniel Hao",
      "Hongbiao Dong"
    ],
    "date": "2025-03-05",
    "summary": "We propose a novel system for robot-to-human object handover that emulates\nhuman coworker interactions. Unlike most existing studies that focus primarily\non grasping strategies and motion planning, our system focus on 1. inferring\nhuman handover intents, 2. imagining spatial handover configuration. The first\none integrates multimodal perception-combining visual and verbal cues-to infer\nhuman intent. The second one using a diffusion-based model to generate the\nhandover configuration, involving the spacial relationship among robot's\ngripper, the object, and the human hand, thereby mimicking the cognitive\nprocess of motor imagery. Experimental results demonstrate that our approach\neffectively interprets human cues and achieves fluent, human-like handovers,\noffering a promising solution for collaborative robotics. Code, videos, and\ndata are available at: https://i3handover.github.io.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03507v1",
    "title": "Mineral segmentation using electron microscope images and spectral\n  sampling through multimodal graph neural networks",
    "url": "http://arxiv.org/abs/2503.03507v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03507v1",
    "authors": [
      "Samuel Repka",
      "Bo\u0159ek Reich",
      "Fedor Zolotarev",
      "Tuomas Eerola",
      "Pavel Zem\u010d\u00edk"
    ],
    "date": "2025-03-05",
    "summary": "We propose a novel Graph Neural Network-based method for segmentation based\non data fusion of multimodal Scanning Electron Microscope (SEM) images. In most\ncases, Backscattered Electron (BSE) images obtained using SEM do not contain\nsufficient information for mineral segmentation. Therefore, imaging is often\ncomplemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS)\nspectral measurements that provide highly accurate information about the\nchemical composition but that are time-consuming to acquire. This motivates the\nuse of sparse spectral data in conjunction with BSE images for mineral\nsegmentation. The unstructured nature of the spectral data makes most\ntraditional image fusion techniques unsuitable for BSE-EDS fusion. We propose\nusing graph neural networks to fuse the two modalities and segment the mineral\nphases simultaneously. Our results demonstrate that providing EDS data for as\nfew as 1% of BSE pixels produces accurate segmentation, enabling rapid analysis\nof mineral samples. The proposed data fusion pipeline is versatile and can be\nadapted to other domains that involve image data and point-wise measurements.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03465v2",
    "title": "DTU-Net: A Multi-Scale Dilated Transformer Network for Nonlinear\n  Hyperspectral Unmixing",
    "url": "http://arxiv.org/abs/2503.03465v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03465v2",
    "authors": [
      "ChenTong Wang",
      "Jincheng Gao",
      "Fei Zhu",
      "Abderrahim Halimi",
      "C\u00e9dric Richard"
    ],
    "date": "2025-03-05",
    "summary": "Transformers have shown significant success in hyperspectral unmixing (HU).\nHowever, challenges remain. While multi-scale and long-range spatial\ncorrelations are essential in unmixing tasks, current Transformer-based\nunmixing networks, built on Vision Transformer (ViT) or Swin-Transformer,\nstruggle to capture them effectively. Additionally, current Transformer-based\nunmixing networks rely on the linear mixing model, which lacks the flexibility\nto accommodate scenarios where nonlinear effects are significant. To address\nthese limitations, we propose a multi-scale Dilated Transformer-based unmixing\nnetwork for nonlinear HU (DTU-Net). The encoder employs two branches. The first\none performs multi-scale spatial feature extraction using Multi-Scale Dilated\nAttention (MSDA) in the Dilated Transformer, which varies dilation rates across\nattention heads to capture long-range and multi-scale spatial correlations. The\nsecond one performs spectral feature extraction utilizing 3D-CNNs with channel\nattention. The outputs from both branches are then fused to integrate\nmulti-scale spatial and spectral information, which is subsequently transformed\nto estimate the abundances. The decoder is designed to accommodate both linear\nand nonlinear mixing scenarios. Its interpretability is enhanced by explicitly\nmodeling the relationships between endmembers, abundances, and nonlinear\ncoefficients in accordance with the polynomial post-nonlinear mixing model\n(PPNMM). Experiments on synthetic and real datasets validate the effectiveness\nof the proposed DTU-Net compared to PPNMM-derived methods and several advanced\nunmixing networks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_03335v1",
    "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective\n  Responses to News",
    "url": "http://arxiv.org/abs/2503.03335v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03335v1",
    "authors": [
      "Tiancheng Hu",
      "Nigel Collier"
    ],
    "date": "2025-03-05",
    "summary": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03321v1",
    "title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
    "url": "http://arxiv.org/abs/2503.03321v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03321v1",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ],
    "date": "2025-03-05",
    "summary": "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_03285v2",
    "title": "Enhancing Vietnamese VQA through Curriculum Learning on Raw and\n  Augmented Text Representations",
    "url": "http://arxiv.org/abs/2503.03285v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03285v2",
    "authors": [
      "Khoi Anh Nguyen",
      "Linh Yen Vu",
      "Thang Dinh Duong",
      "Thuan Nguyen Duong",
      "Huy Thanh Nguyen",
      "Vinh Quang Dinh"
    ],
    "date": "2025-03-05",
    "summary": "Visual Question Answering (VQA) is a multimodal task requiring reasoning\nacross textual and visual inputs, which becomes particularly challenging in\nlow-resource languages like Vietnamese due to linguistic variability and the\nlack of high-quality datasets. Traditional methods often rely heavily on\nextensive annotated datasets, computationally expensive pipelines, and large\npre-trained models, specifically in the domain of Vietnamese VQA, limiting\ntheir applicability in such scenarios. To address these limitations, we propose\na training framework that combines a paraphrase-based feature augmentation\nmodule with a dynamic curriculum learning strategy. Explicitly, augmented\nsamples are considered \"easy\" while raw samples are regarded as \"hard\". The\nframework then utilizes a mechanism that dynamically adjusts the ratio of easy\nto hard samples during training, progressively modifying the same dataset to\nincrease its difficulty level. By enabling gradual adaptation to task\ncomplexity, this approach helps the Vietnamese VQA model generalize well, thus\nimproving overall performance. Experimental results show consistent\nimprovements on the OpenViVQA dataset and mixed outcomes on the ViVQA dataset,\nhighlighting both the potential and challenges of our approach in advancing VQA\nfor Vietnamese language.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA"
    ]
  },
  {
    "id": "2503_03280v1",
    "title": "BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation",
    "url": "http://arxiv.org/abs/2503.03280v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03280v1",
    "authors": [
      "Hiep Truong Cong",
      "Ajay Kumar Sigatapu",
      "Arindam Das",
      "Yashwanth Sharma",
      "Venkatesh Satagopan",
      "Ganesh Sistu",
      "Ciaran Eising"
    ],
    "date": "2025-03-05",
    "summary": "Accurate motion understanding of the dynamic objects within the scene in\nbird's-eye-view (BEV) is critical to ensure a reliable obstacle avoidance\nsystem and smooth path planning for autonomous vehicles. However, this task has\nreceived relatively limited exploration when compared to object detection and\nsegmentation with only a few recent vision-based approaches presenting\npreliminary findings that significantly deteriorate in low-light, nighttime,\nand adverse weather conditions such as rain. Conversely, LiDAR and radar\nsensors remain almost unaffected in these scenarios, and radar provides key\nvelocity information of the objects. Therefore, we introduce BEVMOSNet, to our\nknowledge, the first end-to-end multimodal fusion leveraging cameras, LiDAR,\nand radar to precisely predict the moving objects in BEV. In addition, we\nperform a deeper analysis to find out the optimal strategy for deformable\ncross-attention-guided sensor fusion for cross-sensor knowledge sharing in BEV.\nWhile evaluating BEVMOSNet on the nuScenes dataset, we show an overall\nimprovement in IoU score of 36.59% compared to the vision-based unimodal\nbaseline BEV-MoSeg (Sigatapu et al., 2023), and 2.35% compared to the\nmultimodel SimpleBEV (Harley et al., 2022), extended for the motion\nsegmentation task, establishing this method as the state-of-the-art in BEV\nmotion segmentation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03278v1",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with\n  Knowledge Descriptions",
    "url": "http://arxiv.org/abs/2503.03278v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03278v1",
    "authors": [
      "Jun Li",
      "Che Liu",
      "Wenjia Bai",
      "Rossella Arcucci",
      "Cosmin I. Bercea",
      "Julia A. Schnabel"
    ],
    "date": "2025-03-05",
    "summary": "Visual Language Models (VLMs) have demonstrated impressive capabilities in\nvisual grounding tasks. However, their effectiveness in the medical domain,\nparticularly for abnormality detection and localization within medical images,\nremains underexplored. A major challenge is the complex and abstract nature of\nmedical terminology, which makes it difficult to directly associate\npathological anomaly terms with their corresponding visual features. In this\nwork, we introduce a novel approach to enhance VLM performance in medical\nabnormality detection and localization by leveraging decomposed medical\nknowledge. Instead of directly prompting models to recognize specific\nabnormalities, we focus on breaking down medical concepts into fundamental\nattributes and common visual patterns. This strategy promotes a stronger\nalignment between textual descriptions and visual features, improving both the\nrecognition and localization of abnormalities in medical images.We evaluate our\nmethod on the 0.23B Florence-2 base model and demonstrate that it achieves\ncomparable performance in abnormality grounding to significantly larger 7B\nLLaVA-based medical VLMs, despite being trained on only 1.5% of the data used\nfor such models. Experimental results also demonstrate the effectiveness of our\napproach in both known and previously unseen abnormalities, suggesting its\nstrong generalization capabilities.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_03792v1",
    "title": "Rebalanced Multimodal Learning with Data-aware Unimodal Sampling",
    "url": "http://arxiv.org/abs/2503.03792v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03792v1",
    "authors": [
      "Qingyuan Jiang",
      "Zhouyang Chi",
      "Xiao Ma",
      "Qirong Mao",
      "Yang Yang",
      "Jinhui Tang"
    ],
    "date": "2025-03-05",
    "summary": "To address the modality learning degeneration caused by modality imbalance,\nexisting multimodal learning~(MML) approaches primarily attempt to balance the\noptimization process of each modality from the perspective of model learning.\nHowever, almost all existing methods ignore the modality imbalance caused by\nunimodal data sampling, i.e., equal unimodal data sampling often results in\ndiscrepancies in informational content, leading to modality imbalance.\nTherefore, in this paper, we propose a novel MML approach called\n\\underline{D}ata-aware \\underline{U}nimodal \\underline{S}ampling~(\\method),\nwhich aims to dynamically alleviate the modality imbalance caused by sampling.\nSpecifically, we first propose a novel cumulative modality discrepancy to\nmonitor the multimodal learning process. Based on the learning status, we\npropose a heuristic and a reinforcement learning~(RL)-based data-aware unimodal\nsampling approaches to adaptively determine the quantity of sampled data at\neach iteration, thus alleviating the modality imbalance from the perspective of\nsampling. Meanwhile, our method can be seamlessly incorporated into almost all\nexisting multimodal learning approaches as a plugin. Experiments demonstrate\nthat \\method~can achieve the best performance by comparing with diverse\nstate-of-the-art~(SOTA) baselines.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03244v1",
    "title": "Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection\n  in Neonatal Care",
    "url": "http://arxiv.org/abs/2503.03244v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03244v1",
    "authors": [
      "Jorge Garc\u00eda-Torres",
      "\u00d8yvind Meinich-Bache",
      "Sara Brunner",
      "Siren Rettedal",
      "Vilde Kolstad",
      "Kjersti Engan"
    ],
    "date": "2025-03-05",
    "summary": "Around 10% of newborns require some help to initiate breathing, and 5\\% need\nventilation assistance. Accurate Time of Birth (ToB) documentation is essential\nfor optimizing neonatal care, as timely interventions are vital for proper\nresuscitation. However, current clinical methods for recording ToB often rely\non manual processes, which can be prone to inaccuracies. In this study, we\npresent a novel two-stream fusion system that combines the power of image and\nvideo analysis to accurately detect the ToB from thermal recordings in the\ndelivery room and operating theater. By integrating static and dynamic streams,\nour approach captures richer birth-related spatiotemporal features, leading to\nmore robust and precise ToB estimation. We demonstrate that this synergy\nbetween data modalities enhances performance over single-stream approaches. Our\nsystem achieves 95.7% precision and 84.8% recall in detecting birth within\nshort video clips. Additionally, with the help of a score aggregation module,\nit successfully identifies ToB in 100% of test cases, with a median absolute\nerror of 2 seconds and an absolute mean deviation of 4.5 seconds compared to\nmanual annotations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_03215v1",
    "title": "COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open\n  Source Intelligence",
    "url": "http://arxiv.org/abs/2503.03215v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03215v1",
    "authors": [
      "Wentao Li",
      "Congcong Wang",
      "Xiaoxiao Cui",
      "Zhi Liu",
      "Wei Guo",
      "Lizhen Cui"
    ],
    "date": "2025-03-05",
    "summary": "Open Source Intelligence (OSINT) requires the integration and reasoning of\ndiverse multimodal data, presenting significant challenges in deriving\nactionable insights. Traditional approaches, including multimodal large\nlanguage models (MLLMs), often struggle to infer complex contextual\nrelationships or deliver comprehensive intelligence from unstructured data\nsources. In this paper, we introduce COSINT-Agent, a knowledge-driven\nmultimodal agent tailored to address the challenges of OSINT in the Chinese\ndomain. COSINT-Agent seamlessly integrates the perceptual capabilities of\nfine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene\nKnowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match\nframework, which bridges COSINT-MLLM and EES-KG, enabling systematic\nextraction, reasoning, and contextualization of multimodal insights. This\nintegration facilitates precise entity recognition, event interpretation, and\ncontext retrieval, effectively transforming raw multimodal data into actionable\nintelligence. Extensive experiments validate the superior performance of\nCOSINT-Agent across core OSINT tasks, including entity recognition, EES\ngeneration, and context matching. These results underscore its potential as a\nrobust and scalable solution for advancing automated multimodal reasoning and\nenhancing the effectiveness of OSINT methodologies.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03202v1",
    "title": "Variance-Aware Loss Scheduling for Multimodal Alignment in Low-Data\n  Settings",
    "url": "http://arxiv.org/abs/2503.03202v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03202v1",
    "authors": [
      "Sneh Pillai"
    ],
    "date": "2025-03-05",
    "summary": "Training vision-language models for image-text alignment typically requires\nlarge datasets to achieve robust performance. In low-data scenarios, standard\ncontrastive learning can struggle to align modalities effectively due to\noverfitting and unstable training dynamics. In this paper, we propose a\nvariance-aware loss scheduling approach that dynamically adjusts the weighting\nof the contrastive loss based on the statistical variability (uncertainty) in\nthe model's alignment predictions. Using a subset of the Flickr8k image-caption\ndataset to simulate limited data conditions, we demonstrate that our approach\nimproves image-text retrieval accuracy compared to a fixed-weight baseline. We\nalso compare against other adaptive weighting strategies (using output entropy\nand cosine similarity spread) and find that variance-aware scheduling provides\nthe best overall trade-off. Qualitatively, our method yields more distinct\nmultimodal embeddings as shown by t-SNE visualizations. Moreover, in a stress\ntest with noise-injected captions and images, the variance-guided loss proves\nmore robust, maintaining higher recall when random perturbations are\nintroduced. These results highlight the benefit of adaptive loss weighting for\nmultimodal alignment in low-data regimes.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language",
      "image-text"
    ]
  },
  {
    "id": "2503_03196v1",
    "title": "SpiritSight Agent: Advanced GUI Agent with One Look",
    "url": "http://arxiv.org/abs/2503.03196v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03196v1",
    "authors": [
      "Zhiyuan Huang",
      "Ziming Cheng",
      "Junting Pan",
      "Zhaohui Hou",
      "Mingjie Zhan"
    ],
    "date": "2025-03-05",
    "summary": "Graphical User Interface (GUI) agents show amazing abilities in assisting\nhuman-computer interaction, automating human user's navigation on digital\ndevices. An ideal GUI agent is expected to achieve high accuracy, low latency,\nand compatibility for different GUI platforms. Recent vision-based approaches\nhave shown promise by leveraging advanced Vision Language Models (VLMs). While\nthey generally meet the requirements of compatibility and low latency, these\nvision-based GUI agents tend to have low accuracy due to their limitations in\nelement grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a\nvision-based, end-to-end GUI agent that excels in GUI navigation tasks across\nvarious GUI platforms. First, we create a multi-level, large-scale,\nhigh-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods,\nempowering SpiritSight with robust GUI understanding and grounding\ncapabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$\nmethod to resolve the ambiguity problem in dynamic high-resolution of visual\ninputs, further enhancing SpiritSight's ability to ground GUI objects. Through\nthese efforts, SpiritSight agent outperforms other advanced methods on diverse\nGUI benchmarks, demonstrating its superior capability and compatibility in GUI\nnavigation tasks. Models are available at\n$\\href{https://huggingface.co/SenseLLM/SpiritSight-Agent-8B}{this\\ URL}$.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_03190v2",
    "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
    "url": "http://arxiv.org/abs/2503.03190v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03190v2",
    "authors": [
      "Jingzhou Luo",
      "Yang Liu",
      "Weixing Chen",
      "Zhen Li",
      "Yaowei Wang",
      "Guanbin Li",
      "Liang Lin"
    ],
    "date": "2025-03-05",
    "summary": "3D Question Answering (3D QA) requires the model to comprehensively\nunderstand its situated 3D scene described by the text, then reason about its\nsurrounding environment and answer a question under that situation. However,\nexisting methods usually rely on global scene perception from pure 3D point\nclouds and overlook the importance of rich local texture details from\nmulti-view images. Moreover, due to the inherent noise in camera poses and\ncomplex occlusions, there exists significant feature degradation and reduced\nfeature robustness problems when aligning 3D point cloud with multi-view\nimages. In this paper, we propose a Dual-vision Scene Perception Network\n(DSPNet), to comprehensively integrate multi-view and point cloud features to\nimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) module\nprioritizes image views that closely match the semantic content of the text. To\nadaptively fuse back-projected multi-view images with point cloud features, we\ndesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scene\ncomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)\nmodule facilitates robust reasoning by integrating contextual information\nacross visual and linguistic modalities. Experimental results on SQA3D and\nScanQA datasets demonstrate the superiority of our DSPNet. Codes will be\navailable at https://github.com/LZ-CH/DSPNet.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03122v1",
    "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models",
    "url": "http://arxiv.org/abs/2503.03122v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03122v1",
    "authors": [
      "Zichao Li",
      "Xueru Wen",
      "Jie Lou",
      "Yuqiu Ji",
      "Yaojie Lu",
      "Xianpei Han",
      "Debing Zhang",
      "Le Sun"
    ],
    "date": "2025-03-05",
    "summary": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03112v1",
    "title": "A Multimodal Framework for Topic Propagation Classification in Social\n  Networks",
    "url": "http://arxiv.org/abs/2503.03112v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03112v1",
    "authors": [
      "Yuchuan Jiang",
      "Chaolong Jia",
      "Yunyi Qin",
      "Wei Cai",
      "Yongsen Qian"
    ],
    "date": "2025-03-05",
    "summary": "The rapid proliferation of the Internet and the widespread adoption of social\nnetworks have significantly accelerated information dissemination. However,\nthis transformation has introduced complexities in information capture and\nprocessing, posing substantial challenges for researchers and practitioners.\nPredicting the dissemination of topic-related information within social\nnetworks has thus become a critical research focus. This paper proposes a\npredictive model for topic dissemination in social networks by integrating\nmultidimensional features derived from key dissemination characteristics.\nSpecifically, we introduce two novel indicators, user relationship breadth and\nuser authority, into the PageRank algorithm to quantify user influence more\neffectively. Additionally, we employ a Text-CNN model for sentiment\nclassification, extracting sentiment features from textual content. Temporal\nembeddings of nodes are encoded using a Bi-LSTM model to capture temporal\ndynamics. Furthermore, we refine the measurement of user interaction traces\nwith topics, replacing traditional topic view metrics with a more precise\ncommunication characteristics measure. Finally, we integrate the extracted\nmultidimensional features using a Transformer model, significantly enhancing\npredictive performance. Experimental results demonstrate that our proposed\nmodel outperforms traditional machine learning and unimodal deep learning\nmodels in terms of FI-Score, AUC, and Recall, validating its effectiveness in\npredicting topic propagation within social networks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_03107v1",
    "title": "External Reliable Information-enhanced Multimodal Contrastive Learning\n  for Fake News Detection",
    "url": "http://arxiv.org/abs/2503.03107v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03107v1",
    "authors": [
      "Biwei Cao",
      "Qihang Wu",
      "Jiuxin Cao",
      "Bo Liu",
      "Jie Gui"
    ],
    "date": "2025-03-05",
    "summary": "With the rapid development of the Internet, the information dissemination\nparadigm has changed and the efficiency has been improved greatly. While this\nalso brings the quick spread of fake news and leads to negative impacts on\ncyberspace. Currently, the information presentation formats have evolved\ngradually, with the news formats shifting from texts to multimodal contents. As\na result, detecting multimodal fake news has become one of the research\nhotspots. However, multimodal fake news detection research field still faces\ntwo main challenges: the inability to fully and effectively utilize multimodal\ninformation for detection, and the low credibility or static nature of the\nintroduced external information, which limits dynamic updates. To bridge the\ngaps, we propose ERIC-FND, an external reliable information-enhanced multimodal\ncontrastive learning framework for fake news detection. ERIC-FND strengthens\nthe representation of news contents by entity-enriched external information\nenhancement method. It also enriches the multimodal news information via\nmultimodal semantic interaction method where the multimodal constrative\nlearning is employed to make different modality representations learn from each\nother. Moreover, an adaptive fusion method is taken to integrate the news\nrepresentations from different dimensions for the eventual classification.\nExperiments are done on two commonly used datasets in different languages, X\n(Twitter) and Weibo. Experiment results demonstrate that our proposed model\nERIC-FND outperforms existing state-of-the-art fake news detection methods\nunder the same settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_05595v1",
    "title": "Anti-Diffusion: Preventing Abuse of Modifications of Diffusion-Based\n  Models",
    "url": "http://arxiv.org/abs/2503.05595v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05595v1",
    "authors": [
      "Zheng Li",
      "Liangbin Xie",
      "Jiantao Zhou",
      "Xintao Wang",
      "Haiwei Wu",
      "Jinyu Tian"
    ],
    "date": "2025-03-05",
    "summary": "Although diffusion-based techniques have shown remarkable success in image\ngeneration and editing tasks, their abuse can lead to severe negative social\nimpacts. Recently, some works have been proposed to provide defense against the\nabuse of diffusion-based methods. However, their protection may be limited in\nspecific scenarios by manually defined prompts or the stable diffusion (SD)\nversion. Furthermore, these methods solely focus on tuning methods, overlooking\nediting methods that could also pose a significant threat. In this work, we\npropose Anti-Diffusion, a privacy protection system designed for general\ndiffusion-based methods, applicable to both tuning and editing techniques. To\nmitigate the limitations of manually defined prompts on defense performance, we\nintroduce the prompt tuning (PT) strategy that enables precise expression of\noriginal images. To provide defense against both tuning and editing methods, we\npropose the semantic disturbance loss (SDL) to disrupt the semantic information\nof protected images. Given the limited research on the defense against editing\nmethods, we develop a dataset named Defense-Edit to assess the defense\nperformance of various methods. Experiments demonstrate that our Anti-Diffusion\nachieves superior defense performance across a wide range of diffusion-based\ntechniques in different scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion"
    ]
  },
  {
    "id": "paperswithcode_LayoutXLM__Multimodal_Pre_training_for_Multilingua",
    "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding",
    "url": "https://paperswithcode.com/paper/layoutxlm-multimodal-pre-training-for",
    "authors": [],
    "date": "2025-03-05",
    "summary": "In this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null
  },
  {
    "id": "2503_04842v1",
    "title": "Replicating Human Social Perception in Generative AI: Evaluating the\n  Valence-Dominance Model",
    "url": "http://arxiv.org/abs/2503.04842v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04842v1",
    "authors": [
      "Necdet Gurkan",
      "Kimathi Njoki",
      "Jordan W. Suchow"
    ],
    "date": "2025-03-05",
    "summary": "As artificial intelligence (AI) continues to advance--particularly in\ngenerative models--an open question is whether these systems can replicate\nfoundational models of human social perception. A well-established framework in\nsocial cognition suggests that social judgments are organized along two primary\ndimensions: valence (e.g., trustworthiness, warmth) and dominance (e.g., power,\nassertiveness). This study examines whether multimodal generative AI systems\ncan reproduce this valence-dominance structure when evaluating facial images\nand how their representations align with those observed across world regions.\nThrough principal component analysis (PCA), we found that the extracted\ndimensions closely mirrored the theoretical structure of valence and dominance,\nwith trait loadings aligning with established definitions. However, many world\nregions and generative AI models also exhibited a third component, the nature\nand significance of which warrant further investigation. These findings\ndemonstrate that multimodal generative AI systems can replicate key aspects of\nhuman social perception, raising important questions about their implications\nfor AI-driven decision-making and human-AI interactions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_04839v1",
    "title": "Advancing Multimodal In-Context Learning in Large Vision-Language Models\n  with Task-aware Demonstrations",
    "url": "http://arxiv.org/abs/2503.04839v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04839v1",
    "authors": [
      "Yanshu Li"
    ],
    "date": "2025-03-05",
    "summary": "Multimodal in-context learning (ICL) has emerged as a key capability of Large\nVision-Language Models (LVLMs), driven by their increasing scale and\napplicability. Despite its promise, effective ICL in the multimodal setting\nremains challenging due to the inherent complexity of image-text inputs and the\nhigh sensitivity of ICL performance to input configurations. In this work, we\nshed light on the core mechanism underlying multimodal ICL, identifying task\nmapping as a crucial factor in configuring robust in-context demonstration\n(ICD) sequences. Building on these insights, we propose \\textit{SabER}, a\nlightweight yet powerful decoder-only transformer equipped with task-aware\nattention, which intelligently selects and arranges ICDs from a demonstration\nlibrary in an autoregressive fashion. This design enables fine-grained feature\nextraction and cross-modal reasoning, iteratively refining task mapping to\ngenerate high-quality ICD sequences. Through extensive experiments covering\nfive LVLMs and nine benchmark datasets, SabER not only demonstrates strong\nempirical performance, but also provides deeper understanding of how task\nsemantics interact with multimodal ICDs. Our findings highlight the importance\nof principled ICD sequence configuration and open new avenues to enhance\nmultimodal ICL in a wide range of real-world scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language",
      "image-text",
      "ViT"
    ]
  },
  {
    "id": "2503_02950v1",
    "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
    "url": "http://arxiv.org/abs/2503.02950v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02950v1",
    "authors": [
      "Danqing Zhang",
      "Balaji Rama",
      "Jingyi Ni",
      "Shiying He",
      "Fu Zhao",
      "Kunyu Chen",
      "Arnold Chen",
      "Junyu Cao"
    ],
    "date": "2025-03-04",
    "summary": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ]
  },
  {
    "id": "2503_02876v1",
    "title": "SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and\n  Baseline Models",
    "url": "http://arxiv.org/abs/2503.02876v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02876v1",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "date": "2025-03-04",
    "summary": "Advancing AI in computational pathology requires large, high-quality, and\ndiverse datasets, yet existing public datasets are often limited in organ\ndiversity, class coverage, or annotation quality. To bridge this gap, we\nintroduce SPIDER (Supervised Pathology Image-DEscription Repository), the\nlargest publicly available patch-level dataset covering multiple organ types,\nincluding Skin, Colorectal, and Thorax, with comprehensive class coverage for\neach organ. SPIDER provides high-quality annotations verified by expert\npathologists and includes surrounding context patches, which enhance\nclassification performance by providing spatial context.\n  Alongside the dataset, we present baseline models trained on SPIDER using the\nHibou-L foundation model as a feature extractor combined with an\nattention-based classification head. The models achieve state-of-the-art\nperformance across multiple tissue categories and serve as strong benchmarks\nfor future digital pathology research. Beyond patch classification, the model\nenables rapid identification of significant areas, quantitative tissue metrics,\nand establishes a foundation for multimodal approaches.\n  Both the dataset and trained models are publicly available to advance\nresearch, reproducibility, and AI-driven pathology development. Access them at:\nhttps://github.com/HistAI/SPIDER",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02865v2",
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "url": "http://arxiv.org/abs/2503.02865v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02865v2",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "date": "2025-03-04",
    "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_02861v1",
    "title": "Evaluation of Architectural Synthesis Using Generative AI",
    "url": "http://arxiv.org/abs/2503.02861v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02861v1",
    "authors": [
      "Jingfei Huang",
      "Alexandros Haridis"
    ],
    "date": "2025-03-04",
    "summary": "Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02853v1",
    "title": "CADDI: An in-Class Activity Detection Dataset using IMU data from\n  low-cost sensors",
    "url": "http://arxiv.org/abs/2503.02853v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02853v1",
    "authors": [
      "Luis Marquez-Carpintero",
      "Sergio Suescun-Ferrandiz",
      "Monica Pina-Navarro",
      "Miguel Cazorla",
      "Francisco Gomez-Donoso"
    ],
    "date": "2025-03-04",
    "summary": "The monitoring and prediction of in-class student activities is of paramount\nimportance for the comprehension of engagement and the enhancement of\npedagogical efficacy. The accurate detection of these activities enables\neducators to modify their lessons in real time, thereby reducing negative\nemotional states and enhancing the overall learning experience. To this end,\nthe use of non-intrusive devices, such as inertial measurement units (IMUs)\nembedded in smartwatches, represents a viable solution. The development of\nreliable predictive systems has been limited by the lack of large, labeled\ndatasets in education. To bridge this gap, we present a novel dataset for\nin-class activity detection using affordable IMU sensors. The dataset comprises\n19 diverse activities, both instantaneous and continuous, performed by 12\nparticipants in typical classroom scenarios. It includes accelerometer,\ngyroscope, rotation vector data, and synchronized stereo images, offering a\ncomprehensive resource for developing multimodal algorithms using sensor and\nvisual data. This dataset represents a key step toward scalable solutions for\nactivity recognition in educational settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02849v1",
    "title": "Multimodal Deep Learning for Subtype Classification in Breast Cancer\n  Using Histopathological Images and Gene Expression Data",
    "url": "http://arxiv.org/abs/2503.02849v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02849v1",
    "authors": [
      "Amin Honarmandi Shandiz"
    ],
    "date": "2025-03-04",
    "summary": "Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02824v1",
    "title": "Developing a PET/CT Foundation Model for Cross-Modal Anatomical and\n  Functional Imaging",
    "url": "http://arxiv.org/abs/2503.02824v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02824v1",
    "authors": [
      "Yujin Oh",
      "Robert Seifert",
      "Yihan Cao",
      "Christoph Clement",
      "Justin Ferdinandus",
      "Constantin Lapa",
      "Alessandro Liebich",
      "Michelle Amon",
      "Johanna Enke",
      "Sifan Song",
      "Runqi Meng",
      "Fang Zeng",
      "Ning Guo",
      "Xiang Li",
      "Pedram Heidari",
      "Axel Rominger",
      "Kuangyu Shi",
      "Quanzheng Li"
    ],
    "date": "2025-03-04",
    "summary": "In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision transformer"
    ]
  },
  {
    "id": "2503_02823v1",
    "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
    "url": "http://arxiv.org/abs/2503.02823v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
    "authors": [
      "Matteo Spanio",
      "Massimiliano Zampini",
      "Antonio Rod\u00e0",
      "Franco Pierucci"
    ],
    "date": "2025-03-04",
    "summary": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02800v2",
    "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
    "url": "http://arxiv.org/abs/2503.02800v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02800v2",
    "authors": [
      "Alicia Russell-Gilbert",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Maria Seale",
      "Joseph Jabour",
      "Thomas Arnold",
      "Joshua Church"
    ],
    "date": "2025-03-04",
    "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02798v1",
    "title": "Spike-and-Slab Posterior Sampling in High Dimensions",
    "url": "http://arxiv.org/abs/2503.02798v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02798v1",
    "authors": [
      "Syamantak Kumar",
      "Purnamrita Sarkar",
      "Kevin Tian",
      "Yusong Zhu"
    ],
    "date": "2025-03-04",
    "summary": "Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal\ndistribution used to model uncertainty in variable selection, is considered the\ntheoretical gold standard method for Bayesian sparse linear regression [CPS09,\nRoc18]. However, designing provable algorithms for performing this sampling\ntask is notoriously challenging. Existing posterior samplers for Bayesian\nsparse variable selection tasks either require strong assumptions about the\nsignal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows\nat least linearly in the dimension [MW24], or rely on heuristic approximations\nto the posterior. We give the first provable algorithms for spike-and-slab\nposterior sampling that apply for any SNR, and use a measurement count\nsublinear in the problem dimension. Concretely, assume we are given a\nmeasurement matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ and noisy\nobservations $\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\star + \\mathbf{\\xi}$ of a\nsignal $\\mathbf{\\theta}^\\star$ drawn from a spike-and-slab prior $\\pi$ with a\nGaussian diffuse density and expected sparsity k, where $\\mathbf{\\xi} \\sim\n\\mathcal{N}(\\mathbb{0}_n, \\sigma^2\\mathbf{I}_n)$. We give a polynomial-time\nhigh-accuracy sampler for the posterior $\\pi(\\cdot \\mid \\mathbf{X},\n\\mathbf{y})$, for any SNR $\\sigma^{-1}$ > 0, as long as $n \\geq k^3 \\cdot\n\\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the\nrestricted isometry property. We further give a sampler that runs in\nnear-linear time $\\approx nd$ in the same setting, as long as $n \\geq k^5 \\cdot\n\\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend\nour result to spike-and-slab posterior sampling with Laplace diffuse densities,\nachieving similar guarantees when $\\sigma = O(\\frac{1}{k})$ is bounded.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02781v1",
    "title": "Multimodal AI predicts clinical outcomes of drug combinations from\n  preclinical data",
    "url": "http://arxiv.org/abs/2503.02781v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02781v1",
    "authors": [
      "Yepeng Huang",
      "Xiaorui Su",
      "Varun Ullanat",
      "Ivy Liang",
      "Lindsay Clegg",
      "Damilola Olabode",
      "Nicholas Ho",
      "Bino John",
      "Megan Gibbs",
      "Marinka Zitnik"
    ],
    "date": "2025-03-04",
    "summary": "Predicting clinical outcomes from preclinical data is essential for\nidentifying safe and effective drug combinations. Current models rely on\nstructural or target-based features to identify high-efficacy, low-toxicity\ndrug combinations. However, these approaches fail to incorporate the multimodal\ndata necessary for accurate, clinically-relevant predictions. Here, we\nintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,\ncell viability, and transcriptomic data to predict drug combination effects\nacross 953 clinical outcomes and 21842 compounds, including combinations of\napproved drugs and novel compounds in development. MADRIGAL uses a transformer\nbottleneck module to unify preclinical drug data modalities while handling\nmissing data during training and inference--a major challenge in multimodal\nlearning. It outperforms single-modality methods and state-of-the-art models in\npredicting adverse drug interactions. MADRIGAL performs virtual screening of\nanticancer drug combinations and supports polypharmacy management for type II\ndiabetes and metabolic dysfunction-associated steatohepatitis (MASH). It\nidentifies transporter-mediated drug interactions. MADRIGAL predicts\nresmetirom, the first and only FDA-approved drug for MASH, among therapies with\nthe most favorable safety profile. It supports personalized cancer therapy by\nintegrating genomic profiles from cancer patients. Using primary acute myeloid\nleukemia samples and patient-derived xenograft models, it predicts the efficacy\nof personalized drug combinations. Integrating MADRIGAL with a large language\nmodel allows users to describe clinical outcomes in natural language, improving\nsafety assessment by identifying potential adverse interactions and toxicity\nrisks. MADRIGAL provides a multimodal approach for designing combination\ntherapies with improved predictive accuracy and clinical relevance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02733v1",
    "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural\n  Video Compression",
    "url": "http://arxiv.org/abs/2503.02733v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02733v1",
    "authors": [
      "Jia Wang",
      "Xinfeng Zhang",
      "Gai Zhang",
      "Jun Zhu",
      "Lv Tang",
      "Li Zhang"
    ],
    "date": "2025-03-04",
    "summary": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "2503_02616v1",
    "title": "Smoothing the Shift: Towards Stable Test-Time Adaptation under Complex\n  Multimodal Noises",
    "url": "http://arxiv.org/abs/2503.02616v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02616v1",
    "authors": [
      "Zirun Guo",
      "Tao Jin"
    ],
    "date": "2025-03-04",
    "summary": "Test-Time Adaptation (TTA) aims to tackle distribution shifts using unlabeled\ntest data without access to the source data. In the context of multimodal data,\nthere are more complex noise patterns than unimodal data such as simultaneous\ncorruptions for multiple modalities and missing modalities. Besides, in\nreal-world applications, corruptions from different distribution shifts are\nalways mixed. Existing TTA methods always fail in such multimodal scenario\nbecause the abrupt distribution shifts will destroy the prior knowledge from\nthe source model, thus leading to performance degradation. To this end, we\nreveal a new challenge named multimodal wild TTA. To address this challenging\nproblem, we propose two novel strategies: sample identification with\ninterquartile range Smoothing and unimodal assistance, and Mutual information\nsharing (SuMi). SuMi smooths the adaptation process by interquartile range\nwhich avoids the abrupt distribution shifts. Then, SuMi fully utilizes the\nunimodal features to select low-entropy samples with rich multimodal\ninformation for optimization. Furthermore, mutual information sharing is\nintroduced to align the information, reduce the discrepancies and enhance the\ninformation utilization across different modalities. Extensive experiments on\ntwo public datasets show the effectiveness and superiority over existing\nmethods under the complex noise patterns in multimodal data. Code is available\nat https://github.com/zrguo/SuMi.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02600v1",
    "title": "Resource-Efficient Affordance Grounding with Complementary Depth and\n  Semantic Prompts",
    "url": "http://arxiv.org/abs/2503.02600v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02600v1",
    "authors": [
      "Yizhou Huang",
      "Fan Yang",
      "Guoliang Zhu",
      "Gen Li",
      "Hao Shi",
      "Yukun Zuo",
      "Wenrui Chen",
      "Zhiyong Li",
      "Kailun Yang"
    ],
    "date": "2025-03-04",
    "summary": "Affordance refers to the functional properties that an agent perceives and\nutilizes from its environment, and is key perceptual information required for\nrobots to perform actions. This information is rich and multimodal in nature.\nExisting multimodal affordance methods face limitations in extracting useful\ninformation, mainly due to simple structural designs, basic fusion methods, and\nlarge model parameters, making it difficult to meet the performance\nrequirements for practical deployment. To address these issues, this paper\nproposes the BiT-Align image-depth-text affordance mapping framework. The\nframework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance\n(TFG) attention selection mechanism. BPM integrates the auxiliary modality\ndepth image directly as a prompt to the primary modality RGB image, embedding\nit into the primary modality encoder without introducing additional encoders.\nThis reduces the model's parameter count and effectively improves functional\nregion localization accuracy. The TFG mechanism guides the selection and\nenhancement of attention heads in the image encoder using textual features,\nimproving the understanding of affordance characteristics. Experimental results\ndemonstrate that the proposed method achieves significant performance\nimprovements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,\ncompared with the current state-of-the-art method, we achieve a 6.0%\nimprovement in the KLD metric, while reducing model parameters by 88.8%,\ndemonstrating practical application values. The source code will be made\npublicly available at https://github.com/DAWDSE/BiT-Align.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02597v1",
    "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.02597v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02597v1",
    "authors": [
      "Wei-Yao Wang",
      "Zhao Wang",
      "Helen Suzuki",
      "Yoshiyuki Kobayashi"
    ],
    "date": "2025-03-04",
    "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_02589v2",
    "title": "MCiteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs",
    "url": "http://arxiv.org/abs/2503.02589v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02589v2",
    "authors": [
      "Caiyu Hu",
      "Yikai Zhang",
      "Tinghui Zhu",
      "Yiwei Ye",
      "Yanghua Xiao"
    ],
    "date": "2025-03-04",
    "summary": "Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, overlooking the challenges and opportunities\nof multimodal contexts. To address this gap, we introduce MCiteBench, the first\nbenchmark designed to evaluate and analyze the multimodal citation text\ngeneration ability of MLLMs. Our benchmark comprises data derived from academic\npapers and review-rebuttal interactions, featuring diverse information sources\nand multimodal content. We comprehensively evaluate models from multiple\ndimensions, including citation quality, source reliability, and answer\naccuracy. Through extensive experiments, we observe that MLLMs struggle with\nmultimodal citation text generation. We also conduct deep analyses of models'\nperformance, revealing that the bottleneck lies in attributing the correct\nsources rather than understanding the multimodal content.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02579v1",
    "title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic\n  Understanding of High-Intensity Surgical Environments",
    "url": "http://arxiv.org/abs/2503.02579v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02579v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Chantal Pellegrini",
      "Tobias Czempiel",
      "Felix Tristram",
      "Kun Yuan",
      "David Bani-Harouni",
      "Ulrich Eck",
      "Benjamin Busam",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "date": "2025-03-04",
    "summary": "Operating rooms (ORs) are complex, high-stakes environments requiring precise\nunderstanding of interactions among medical staff, tools, and equipment for\nenhancing surgical assistance, situational awareness, and patient safety.\nCurrent datasets fall short in scale, realism and do not capture the multimodal\nnature of OR scenes, limiting progress in OR modeling. To this end, we\nintroduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR\ndataset, and the first dataset to enable multimodal scene graph generation.\nMM-OR captures comprehensive OR scenes containing RGB-D data, detail views,\naudio, speech transcripts, robotic logs, and tracking data and is annotated\nwith panoptic segmentations, semantic scene graphs, and downstream task labels.\nFurther, we propose MM2SG, the first multimodal large vision-language model for\nscene graph generation, and through extensive experiments, demonstrate its\nability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG\nestablish a new benchmark for holistic OR understanding, and open the path\ntowards multimodal scene analysis in complex, high-stakes environments. Our\ncode, and data is available at https://github.com/egeozsoy/MM-OR.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ]
  },
  {
    "id": "2503_02917v1",
    "title": "Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided\n  Prompting of Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.02917v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02917v1",
    "authors": [
      "Deval Mehta",
      "Yiwen Jiang",
      "Catherine L Jan",
      "Mingguang He",
      "Kshitij Jadhav",
      "Zongyuan Ge"
    ],
    "date": "2025-03-04",
    "summary": "Recent advancements in deep learning have shown significant potential for\nclassifying retinal diseases using color fundus images. However, existing works\npredominantly rely exclusively on image data, lack interpretability in their\ndiagnostic decisions, and treat medical professionals primarily as annotators\nfor ground truth labeling. To fill this gap, we implement two key strategies:\nextracting interpretable concepts of retinal diseases using the knowledge base\nof GPT models and incorporating these concepts as a language component in\nprompt-learning to train vision-language (VL) models with both fundus images\nand their associated concepts. Our method not only improves retinal disease\nclassification but also enriches few-shot and zero-shot detection (novel\ndisease detection), while offering the added benefit of concept-based model\ninterpretability. Our extensive evaluation across two diverse retinal fundus\nimage datasets illustrates substantial performance gains in VL-model based\nfew-shot methodologies through our concept integration approach, demonstrating\nan average improvement of approximately 5.8\\% and 2.7\\% mean average precision\nfor 16-shot learning and zero-shot (novel class) detection respectively. Our\nmethod marks a pivotal step towards interpretable and efficient retinal disease\nrecognition for real-world clinical applications.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ]
  },
  {
    "id": "2503_02511v1",
    "title": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place\n  Recognition",
    "url": "http://arxiv.org/abs/2503.02511v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02511v1",
    "authors": [
      "Oliver Grainge",
      "Michael Milford",
      "Indu Bodala",
      "Sarvapali D. Ramchurn",
      "Shoaib Ehsan"
    ],
    "date": "2025-03-04",
    "summary": "Visual Place Recognition (VPR) localizes a query image by matching it against\na database of geo-tagged reference images, making it essential for navigation\nand mapping in robotics. Although Vision Transformer (ViT) solutions deliver\nhigh accuracy, their large models often exceed the memory and compute budgets\nof resource-constrained platforms such as drones and mobile robots. To address\nthis issue, we propose TeTRA, a ternary transformer approach that progressively\nquantizes the ViT backbone to 2-bit precision and binarizes its final embedding\nlayer, offering substantial reductions in model size and latency. A carefully\ndesigned progressive distillation strategy preserves the representational power\nof a full-precision teacher, allowing TeTRA to retain or even surpass the\naccuracy of uncompressed convolutional counterparts, despite using fewer\nresources. Experiments on standard VPR benchmarks demonstrate that TeTRA\nreduces memory consumption by up to 69% compared to efficient baselines, while\nlowering inference latency by 35%, with either no loss or a slight improvement\nin recall@1. These gains enable high-accuracy VPR on power-constrained,\nmemory-limited robotic platforms, making TeTRA an appealing solution for\nreal-world deployment.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_02476v1",
    "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA",
    "url": "http://arxiv.org/abs/2503.02476v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02476v1",
    "authors": [
      "Zhengyang Ji",
      "Shang Gao",
      "Li Liu",
      "Yifan Jia",
      "Yutao Yue"
    ],
    "date": "2025-03-04",
    "summary": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "visual question answering",
      "VQA"
    ]
  },
  {
    "id": "2503_02459v1",
    "title": "Exploring Token-Level Augmentation in Vision Transformer for\n  Semi-Supervised Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.02459v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02459v1",
    "authors": [
      "Dengke Zhang",
      "Quan Tang",
      "Fagui Liu",
      "C. L. Philip Chen",
      "Haiqing Mei"
    ],
    "date": "2025-03-04",
    "summary": "Semi-supervised semantic segmentation has witnessed remarkable advancements\nin recent years. However, existing algorithms are based on convolutional neural\nnetworks and directly applying them to Vision Transformers poses certain\nlimitations due to conceptual disparities. To this end, we propose TokenMix, a\ndata augmentation technique specifically designed for semi-supervised semantic\nsegmentation with Vision Transformers. TokenMix aligns well with the global\nattention mechanism by mixing images at the token level, enhancing learning\ncapability for contexutual information among image patches. We further\nincorporate image augmentation and feature augmentation to promote the\ndiversity of augmentation. Moreover, to enhance consistency regularization, we\npropose a dual-branch framework where each branch applies both image\naugmentation and feature augmentation to the input image. We conduct extensive\nexperiments across multiple benchmark datasets, including Pascal VOC 2012,\nCityscapes, and COCO. Results suggest that the proposed method outperforms\nstate-of-the-art algorithms with notably observed accuracy improvement,\nespecially under the circumstance of limited fine annotations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_02420v1",
    "title": "Exploring Model Quantization in GenAI-based Image Inpainting and\n  Detection of Arable Plants",
    "url": "http://arxiv.org/abs/2503.02420v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02420v1",
    "authors": [
      "Sourav Modak",
      "Ahmet O\u011fuz Salt\u0131k",
      "Anthony Stein"
    ],
    "date": "2025-03-04",
    "summary": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion"
    ]
  },
  {
    "id": "2503_02394v3",
    "title": "BHViT: Binarized Hybrid Vision Transformer",
    "url": "http://arxiv.org/abs/2503.02394v3",
    "pdf_url": "http://arxiv.org/pdf/2503.02394v3",
    "authors": [
      "Tian Gao",
      "Zhiyuan Zhang",
      "Yu Zhang",
      "Huajun Liu",
      "Kaijie Yin",
      "Chengzhong Xu",
      "Hui Kong"
    ],
    "date": "2025-03-04",
    "summary": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ]
  },
  {
    "id": "2503_02393v1",
    "title": "Vision-Language Model IP Protection via Prompt-based Learning",
    "url": "http://arxiv.org/abs/2503.02393v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02393v1",
    "authors": [
      "Lianyu Wang",
      "Meng Wang",
      "Huazhu Fu",
      "Daoqiang Zhang"
    ],
    "date": "2025-03-04",
    "summary": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image\nPre-Training) have seen remarkable success in visual recognition, highlighting\nthe increasing need to safeguard the intellectual property (IP) of well-trained\nmodels. Effective IP protection extends beyond ensuring authorized usage; it\nalso necessitates restricting model deployment to authorized data domains,\nparticularly when the model is fine-tuned for specific target domains. However,\ncurrent IP protection methods often rely solely on the visual backbone, which\nmay lack sufficient semantic richness. To bridge this gap, we introduce\nIP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a\nprompt-based learning approach. By leveraging the frozen visual backbone of\nCLIP, we extract both image style and content information, incorporating them\ninto the learning of IP prompt. This strategy acts as a robust barrier,\neffectively preventing the unauthorized transfer of features from authorized\ndomains to unauthorized ones. Additionally, we propose a style-enhancement\nbranch that constructs feature banks for both authorized and unauthorized\ndomains. This branch integrates self-enhanced and cross-domain features,\nfurther strengthening IP-CLIP's capability to block features from unauthorized\ndomains. Finally, we present new three metrics designed to better balance the\nperformance degradation of authorized and unauthorized domains. Comprehensive\nexperiments in various scenarios demonstrate its promising potential for\napplication in IP protection tasks for VLMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "CLIP"
    ]
  },
  {
    "id": "2503_02379v1",
    "title": "Teaching Metric Distance to Autoregressive Multimodal Foundational\n  Models",
    "url": "http://arxiv.org/abs/2503.02379v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02379v1",
    "authors": [
      "Jiwan Chung",
      "Saejin Kim",
      "Yongrae Jo",
      "Jaewoo Park",
      "Dongjun Min",
      "Youngjae Yu"
    ],
    "date": "2025-03-04",
    "summary": "As large language models expand beyond natural language to domains such as\nmathematics, multimodal understanding, and embodied agents, tokens increasingly\nreflect metric relationships rather than purely linguistic meaning. We\nintroduce DIST2Loss, a distance-aware framework designed to train\nautoregressive discrete models by leveraging predefined distance relationships\namong output tokens. At its core, DIST2Loss transforms continuous exponential\nfamily distributions derived from inherent distance metrics into discrete,\ncategorical optimization targets compatible with the models' architectures.\nThis approach enables the models to learn and preserve meaningful distance\nrelationships during token generation while maintaining compatibility with\nexisting architectures. Empirical evaluations show consistent performance gains\nin diverse multimodal applications, including visual grounding, robotic\nmanipulation, generative reward modeling, and image generation using\nvector-quantized features. These improvements are pronounced in cases of\nlimited training data, highlighting DIST2Loss's effectiveness in\nresource-constrained settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "2503_02358v1",
    "title": "Are Large Vision Language Models Good Game Players?",
    "url": "http://arxiv.org/abs/2503.02358v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02358v1",
    "authors": [
      "Xinyu Wang",
      "Bohan Zhuang",
      "Qi Wu"
    ],
    "date": "2025-03-04",
    "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable abilities\nin understanding and reasoning about both visual and textual information.\nHowever, existing evaluation methods for LVLMs, primarily based on benchmarks\nlike Visual Question Answering and image captioning, often fail to capture the\nfull scope of LVLMs' capabilities. These benchmarks are limited by issues such\nas inadequate assessment of detailed visual perception, data contamination, and\na lack of focus on multi-turn reasoning. To address these challenges, we\npropose \\method{}, a game-based evaluation framework designed to provide a\ncomprehensive assessment of LVLMs' cognitive and reasoning skills in structured\nenvironments. \\method{} uses a set of games to evaluate LVLMs on four core\ntasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,\nwith each target task designed to assess specific abilities, including visual\nperception, reasoning, decision-making, etc. Based on this framework, we\nconduct extensive experiments that explore the limitations of current LVLMs,\nsuch as handling long structured outputs and perceiving detailed and dense\nelements. Code and data are publicly available at\nhttps://github.com/xinke-wang/LVLM-Playground.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "image captioning",
      "visual question answering"
    ]
  },
  {
    "id": "2503_02334v1",
    "title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language\n  Models",
    "url": "http://arxiv.org/abs/2503.02334v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02334v1",
    "authors": [
      "Sonnet Xu",
      "Joseph Janizek",
      "Yixing Jiang",
      "Roxana Daneshjou"
    ],
    "date": "2025-03-04",
    "summary": "Vision language models (VLMs) show promise in medical diagnosis, but their\nperformance across demographic subgroups when using in-context learning (ICL)\nremains poorly understood. We examine how the demographic composition of\ndemonstration examples affects VLM performance in two medical imaging tasks:\nskin lesion malignancy prediction and pneumothorax detection from chest\nradiographs. Our analysis reveals that ICL influences model predictions through\nmultiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease\nbase rates from prompts and (2) ICL leads VLMs to make predictions that perform\ndifferently across demographic groups, even after controlling for\nsubgroup-specific disease base rates. Our empirical results inform\nbest-practices for prompting current VLMs (specifically examining demographic\nsubgroup performance, and matching base rates of labels to target distribution\nat a bulk level and within subgroups), while also suggesting next steps for\nimproving our theoretical understanding of these models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ]
  },
  {
    "id": "2503_02330v1",
    "title": "Exploring Simple Siamese Network for High-Resolution Video Quality\n  Assessment",
    "url": "http://arxiv.org/abs/2503.02330v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02330v1",
    "authors": [
      "Guotao Shen",
      "Ziheng Yan",
      "Xin Jin",
      "Longhai Wu",
      "Jie Chen",
      "Ilhyun Cho",
      "Cheul-Hee Hahm"
    ],
    "date": "2025-03-04",
    "summary": "In the research of video quality assessment (VQA), two-branch network has\nemerged as a promising solution. It decouples VQA with separate technical and\naesthetic branches to measure the perception of low-level distortions and\nhigh-level semantics respectively. However, we argue that while technical and\naesthetic perspectives are complementary, the technical perspective itself\nshould be measured in semantic-aware manner. We hypothesize that existing\ntechnical branch struggles to perceive the semantics of high-resolution videos,\nas it is trained on local mini-patches sampled from videos. This issue can be\nhidden by apparently good results on low-resolution videos, but indeed becomes\ncritical for high-resolution VQA. This work introduces SiamVQA, a simple but\neffective Siamese network for highre-solution VQA. SiamVQA shares weights\nbetween technical and aesthetic branches, enhancing the semantic perception\nability of technical branch to facilitate technical-quality representation\nlearning. Furthermore, it integrates a dual cross-attention layer for fusing\ntechnical and aesthetic features. SiamVQA achieves state-of-the-art accuracy on\nhigh-resolution benchmarks, and competitive results on lower-resolution\nbenchmarks. Codes will be available at: https://github.com/srcn-ivl/SiamVQA",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VQA"
    ]
  },
  {
    "id": "2503_05626v1",
    "title": "FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE\n  Framework",
    "url": "http://arxiv.org/abs/2503.05626v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05626v1",
    "authors": [
      "Jingyu Xu",
      "Yang Wang"
    ],
    "date": "2025-03-04",
    "summary": "Artificial intelligence has shown the potential to improve diagnostic\naccuracy through medical image analysis for pneumonia diagnosis. However,\ntraditional multimodal approaches often fail to address real-world challenges\nsuch as incomplete data and modality loss. In this study, a Flexible Multimodal\nTransformer (FMT) was proposed, which uses ResNet-50 and BERT for joint\nrepresentation learning, followed by a dynamic masked attention strategy that\nsimulates clinical modality loss to improve robustness; finally, a sequential\nmixture of experts (MOE) architecture was used to achieve multi-level decision\nrefinement. After evaluation on a small multimodal pneumonia dataset, FMT\nachieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1\nscore, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and the\nmedical benchmark CheXMed (90%), providing a scalable solution for multimodal\ndiagnosis of pneumonia in resource-constrained medical settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_MiniGPT_v2__large_language_model_as_a_unified_inte",
    "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning",
    "url": "https://paperswithcode.com/paper/minigpt-v2-large-language-model-as-a-unified",
    "authors": [],
    "date": "2025-03-04",
    "summary": "Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language",
      "visual question answering",
      "visual grounding"
    ],
    "code_url": null
  },
  {
    "id": "2503_05639v1",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control",
    "url": "http://arxiv.org/abs/2503.05639v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05639v1",
    "authors": [
      "Yuxuan Bian",
      "Zhaoyang Zhang",
      "Xuan Ju",
      "Mingdeng Cao",
      "Liangbin Xie",
      "Ying Shan",
      "Qiang Xu"
    ],
    "date": "2025-03-03",
    "summary": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ]
  },
  {
    "id": "paperswithcode_F_VLM__Open_Vocabulary_Object_Detection_upon_Froze",
    "title": "F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models",
    "url": "https://paperswithcode.com/paper/f-vlm-open-vocabulary-object-detection-upon",
    "authors": [],
    "date": "2025-03-03",
    "summary": "We present F-VLM, a simple open-vocabulary object detection method built upon Frozen Vision and Language Models.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null
  },
  {
    "id": "2503_05684v1",
    "title": "Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints",
    "url": "http://arxiv.org/abs/2503.05684v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05684v1",
    "authors": [
      "Parameswaran Kamalaruban",
      "Mark Anderson",
      "Stuart Burrell",
      "Maeve Madigan",
      "Piotr Skalski",
      "David Sutton"
    ],
    "date": "2025-03-02",
    "summary": "Pre-trained foundation models can be adapted for specific tasks using\nLow-Rank Adaptation (LoRA). However, the fairness properties of these adapted\nclassifiers remain underexplored. Existing fairness-aware fine-tuning methods\nrely on direct access to sensitive attributes or their predictors, but in\npractice, these sensitive attributes are often held under strict consumer\nprivacy controls, and neither the attributes nor their predictors are available\nto model developers, hampering the development of fair models. To address this\nissue, we introduce a set of LoRA-based fine-tuning methods that can be trained\nin a distributed fashion, where model developers and fairness auditors\ncollaborate without sharing sensitive attributes or predictors. In this paper,\nwe evaluate three such methods - sensitive unlearning, adversarial training,\nand orthogonality loss - against a fairness-unaware baseline, using experiments\non the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base\nmodel. We find that orthogonality loss consistently reduces bias while\nmaintaining or improving utility, whereas adversarial training improves False\nPositive Rate Parity and Demographic Parity in some cases, and sensitive\nunlearning provides no clear benefit. In tasks where significant biases are\npresent, distributed fairness-aware fine-tuning methods can effectively\neliminate bias without compromising consumer privacy and, in most cases,\nimprove model utility.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ]
  },
  {
    "id": "paperswithcode_VILA__Learning_Image_Aesthetics_from_User_Comments",
    "title": "VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining",
    "url": "https://paperswithcode.com/paper/vila-learning-image-aesthetics-from-user",
    "authors": [],
    "date": "2025-03-02",
    "summary": "Our results show that our pretrained aesthetic vision-language model outperforms prior works on image aesthetic captioning over the AVA-Captions dataset, and it has powerful zero-shot capability for aesthetic tasks such as zero-shot style classification and zero-shot IAA, surpassing many supervised baselines.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language"
    ],
    "code_url": null
  },
  {
    "id": "2503_05689v1",
    "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.05689v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05689v1",
    "authors": [
      "Zebin Xing",
      "Xingyu Zhang",
      "Yang Hu",
      "Bo Jiang",
      "Tong He",
      "Qian Zhang",
      "Xiaoxiao Long",
      "Wei Yin"
    ],
    "date": "2025-03-01",
    "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the Navsim\\cite{Dauner2024_navsim},\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ]
  },
  {
    "id": "paperswithcode_MiniGPT_4__Enhancing_Vision_Language_Understanding",
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "url": "https://paperswithcode.com/paper/minigpt-4-enhancing-vision-language",
    "authors": [],
    "date": "2025-03-01",
    "summary": "Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language"
    ],
    "code_url": null
  }
]