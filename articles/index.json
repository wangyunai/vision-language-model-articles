[
  {
    "id": "2503.03987v1",
    "title": "RetinalGPT: A Retinal Clinical Preference Conversational Assistant\n  Powered by Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.03987v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03987v1",
    "authors": [
      "Wenhui Zhu",
      "Xin Li",
      "Xiwen Chen",
      "Peijie Qiu",
      "Vamsi Krishna Vasa",
      "Xuanzhao Dong",
      "Yanxi Chen",
      "Natasha Lepore",
      "Oana Dumitrascu",
      "Yi Su",
      "Yalin Wang"
    ],
    "date": "2025-03-06",
    "summary": "Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention for their remarkable ability to process and analyze non-textual data,\nsuch as images, videos, and audio. Notably, several adaptations of\ngeneral-domain MLLMs to the medical field have been explored, including\nLLaVA-Med. However, these medical adaptations remain insufficiently advanced in\nunderstanding and interpreting retinal images. In contrast, medical experts\nemphasize the importance of quantitative analyses for disease detection and\ninterpretation. This underscores a gap between general-domain and\nmedical-domain MLLMs: while general-domain MLLMs excel in broad applications,\nthey lack the specialized knowledge necessary for precise diagnostic and\ninterpretative tasks in the medical field. To address these challenges, we\nintroduce \\textit{RetinalGPT}, a multimodal conversational assistant for\nclinically preferred quantitative analysis of retinal images. Specifically, we\nachieve this by compiling a large retinal image dataset, developing a novel\ndata pipeline, and employing customized visual instruction tuning to enhance\nboth retinal analysis and enrich medical knowledge. In particular, RetinalGPT\noutperforms MLLM in the generic domain by a large margin in the diagnosis of\nretinal diseases in 8 benchmark retinal datasets. Beyond disease diagnosis,\nRetinalGPT features quantitative analyses and lesion localization, representing\na pioneering step in leveraging LLMs for an interpretable and end-to-end\nclinical research framework. The code is available at\nhttps://github.com/Retinal-Research/RetinalGPT",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language",
      "visual instruction tuning",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 3.87,
    "attention_components": {
      "base_score": 2.0,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04839v1",
    "title": "Advancing Multimodal In-Context Learning in Large Vision-Language Models\n  with Task-aware Demonstrations",
    "url": "http://arxiv.org/abs/2503.04839v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04839v1",
    "authors": [
      "Yanshu Li"
    ],
    "date": "2025-03-05",
    "summary": "Multimodal in-context learning (ICL) has emerged as a key capability of Large\nVision-Language Models (LVLMs), driven by their increasing scale and\napplicability. Despite its promise, effective ICL in the multimodal setting\nremains challenging due to the inherent complexity of image-text inputs and the\nhigh sensitivity of ICL performance to input configurations. In this work, we\nshed light on the core mechanism underlying multimodal ICL, identifying task\nmapping as a crucial factor in configuring robust in-context demonstration\n(ICD) sequences. Building on these insights, we propose \\textit{SabER}, a\nlightweight yet powerful decoder-only transformer equipped with task-aware\nattention, which intelligently selects and arranges ICDs from a demonstration\nlibrary in an autoregressive fashion. This design enables fine-grained feature\nextraction and cross-modal reasoning, iteratively refining task mapping to\ngenerate high-quality ICD sequences. Through extensive experiments covering\nfive LVLMs and nine benchmark datasets, SabER not only demonstrates strong\nempirical performance, but also provides deeper understanding of how task\nsemantics interact with multimodal ICDs. Our findings highlight the importance\nof principled ICD sequence configuration and open new avenues to enhance\nmultimodal ICL in a wide range of real-world scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language",
      "image-text",
      "ViT"
    ],
    "attention_score": 3.84,
    "attention_components": {
      "base_score": 2.0,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04543v1",
    "title": "Keeping Yourself is Important in Downstream Tuning Multimodal Large\n  Language Model",
    "url": "http://arxiv.org/abs/2503.04543v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04543v1",
    "authors": [
      "Wenke Huang",
      "Jian Liang",
      "Xianda Guo",
      "Yiyang Fang",
      "Guancheng Wan",
      "Xuankun Rong",
      "Chi Wen",
      "Zekun Shi",
      "Qingyun Li",
      "Didi Zhu",
      "Yanbiao Ma",
      "Ke Liang",
      "Bin Yang",
      "He Li",
      "Jiawei Shao",
      "Mang Ye",
      "Bo Du"
    ],
    "date": "2025-03-06",
    "summary": "Multi-modal Large Language Models (MLLMs) integrate visual and linguistic\nreasoning to address complex tasks such as image captioning and visual question\nanswering. While MLLMs demonstrate remarkable versatility, MLLMs appears\nlimited performance on special applications. But tuning MLLMs for downstream\ntasks encounters two key challenges: Task-Expert Specialization, where\ndistribution shifts between pre-training and target datasets constrain target\nperformance, and Open-World Stabilization, where catastrophic forgetting erases\nthe model general knowledge. In this work, we systematically review recent\nadvancements in MLLM tuning methodologies, classifying them into three\nparadigms: (I) Selective Tuning, (II) Additive Tuning, and (III)\nReparameterization Tuning. Furthermore, we benchmark these tuning strategies\nacross popular MLLM architectures and diverse downstream tasks to establish\nstandardized evaluation analysis and systematic tuning principles. Finally, we\nhighlight several open challenges in this domain and propose future research\ndirections. To facilitate ongoing progress in this rapidly evolving field, we\nprovide a public repository that continuously tracks developments:\nhttps://github.com/WenkeHuang/Awesome-MLLM-Tuning.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image captioning",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 3.48,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04229v1",
    "title": "Synthetic Data is an Elegant GIFT for Continual Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.04229v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04229v1",
    "authors": [
      "Bin Wu",
      "Wuxuan Shi",
      "Jinqiao Wang",
      "Mang Ye"
    ],
    "date": "2025-03-06",
    "summary": "Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to\nefficiently update their knowledge and adapt to various downstream tasks\nwithout retraining from scratch. However, for VLMs, in addition to the loss of\nknowledge previously learned from downstream tasks, pre-training knowledge is\nalso corrupted during continual fine-tuning. This issue is exacerbated by the\nunavailability of original pre-training data, leaving VLM's generalization\nability degrading. In this paper, we propose GIFT, a novel continual\nfine-tuning approach that utilizes synthetic data to overcome catastrophic\nforgetting in VLMs. Taking advantage of recent advances in text-to-image\nsynthesis, we employ a pre-trained diffusion model to recreate both\npre-training and learned downstream task data. In this way, the VLM can revisit\nprevious knowledge through distillation on matching diffusion-generated images\nand corresponding text prompts. Leveraging the broad distribution and high\nalignment between synthetic image-text pairs in VLM's feature space, we propose\na contrastive distillation loss along with an image-text alignment constraint.\nTo further combat in-distribution overfitting and enhance distillation\nperformance with limited amount of generated data, we incorporate adaptive\nweight consolidation, utilizing Fisher information from these synthetic\nimage-text pairs and achieving a better stability-plasticity balance. Extensive\nexperiments demonstrate that our method consistently outperforms previous\nstate-of-the-art approaches across various settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image-text",
      "text-to-image"
    ],
    "attention_score": 3.48,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03651v1",
    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in\n  Multimodal Cycles",
    "url": "http://arxiv.org/abs/2503.03651v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03651v1",
    "authors": [
      "Rui Zhao",
      "Weijia Mao",
      "Mike Zheng Shou"
    ],
    "date": "2025-03-05",
    "summary": "Adapting generative models to specific domains presents an effective solution\nfor satisfying specialized requirements. However, adapting to some complex\ndomains remains challenging, especially when these domains require substantial\npaired data to capture the targeted distributions. Since unpaired data from a\nsingle modality, such as vision or language, is more readily available, we\nutilize the bidirectional mappings between vision and language learned by the\nunified generative model to enable training on unpaired data for domain\nadaptation. Specifically, we propose DoraCycle, which integrates two multimodal\ncycles: text-to-image-to-text and image-to-text-to-image. The model is\noptimized through cross-entropy loss computed at the cycle endpoints, where\nboth endpoints share the same modality. This facilitates self-evolution of the\nmodel without reliance on annotated text-image pairs. Experimental results\ndemonstrate that for tasks independent of paired knowledge, such as\nstylization, DoraCycle can effectively adapt the unified model using only\nunpaired data. For tasks involving new paired knowledge, such as specific\nidentities, a combination of a small set of paired image-text examples and\nlarger-scale unpaired data is sufficient for effective domain-oriented\nadaptation. The code will be released at https://github.com/showlab/DoraCycle.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "text-to-image",
      "image-to-text"
    ],
    "attention_score": 3.45,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05543v1",
    "title": "Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of\n  Diagrammatic Information",
    "url": "http://arxiv.org/abs/2503.05543v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05543v1",
    "authors": [
      "Junbo Zhao",
      "Ting Zhang",
      "Jiayu Sun",
      "Mi Tian",
      "Hua Huang"
    ],
    "date": "2025-03-07",
    "summary": "Geometry problem solving has garnered increasing attention due to its\npotential applications in intelligent education field. Inspired by the\nobservation that text often introduces ambiguities that diagrams can clarify,\nthis paper presents Pi-GPS, a novel framework that unleashes the power of\ndiagrammatic information to resolve textual ambiguities, an aspect largely\noverlooked in prior research. Specifically, we design a micro module comprising\na rectifier and verifier: the rectifier employs MLLMs to disambiguate text\nbased on the diagrammatic context, while the verifier ensures the rectified\noutput adherence to geometric rules, mitigating model hallucinations.\nAdditionally, we explore the impact of LLMs in theorem predictor based on the\ndisambiguated formal language. Empirical results demonstrate that Pi-GPS\nsurpasses state-of-the-art models, achieving a nearly 10\\% improvement on\nGeometry3K over prior neural-symbolic approaches. We hope this work highlights\nthe significance of resolving textual ambiguity in multimodal mathematical\nreasoning, a crucial factor limiting performance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 3.12,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05093v1",
    "title": "Visual Cues of Gender and Race are Associated with Stereotyping in\n  Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.05093v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05093v1",
    "authors": [
      "Messi H. J. Lee",
      "Soyeon Jeon",
      "Jacob M. Montgomery",
      "Calvin K. Lai"
    ],
    "date": "2025-03-07",
    "summary": "Current research on bias in Vision Language Models (VLMs) has important\nlimitations: it is focused exclusively on trait associations while ignoring\nother forms of stereotyping, it examines specific contexts where biases are\nexpected to appear, and it conceptualizes social categories like race and\ngender as binary, ignoring the multifaceted nature of these identities. Using\nstandardized facial images that vary in prototypicality, we test four VLMs for\nboth trait associations and homogeneity bias in open-ended contexts. We find\nthat VLMs consistently generate more uniform stories for women compared to men,\nwith people who are more gender prototypical in appearance being represented\nmore uniformly. By contrast, VLMs represent White Americans more uniformly than\nBlack Americans. Unlike with gender prototypicality, race prototypicality was\nnot related to stronger uniformity. In terms of trait associations, we find\nlimited evidence of stereotyping-Black Americans were consistently linked with\nbasketball across all models, while other racial associations (i.e., art,\nhealthcare, appearance) varied by specific VLM. These findings demonstrate that\nVLM stereotyping manifests in ways that go beyond simple group membership,\nsuggesting that conventional bias mitigation strategies may be insufficient to\naddress VLM stereotyping and that homogeneity bias persists even when trait\nassociations are less apparent in model outputs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "vision-language"
    ],
    "attention_score": 3.12,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05204v1",
    "title": "Data-Efficient Generalization for Zero-shot Composed Image Retrieval",
    "url": "http://arxiv.org/abs/2503.05204v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05204v1",
    "authors": [
      "Zining Chen",
      "Zhicheng Zhao",
      "Fei Su",
      "Xiaoqin Zhang",
      "Shijian Lu"
    ],
    "date": "2025-03-07",
    "summary": "Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image\nbased on a reference image and a text description without requiring\nin-distribution triplets for training. One prevalent approach follows the\nvision-language pretraining paradigm that employs a mapping network to transfer\nthe image embedding to a pseudo-word token in the text embedding space.\nHowever, this approach tends to impede network generalization due to modality\ndiscrepancy and distribution shift between training and inference. To this end,\nwe propose a Data-efficient Generalization (DeG) framework, including two novel\ndesigns, namely, Textual Supplement (TS) module and Semantic-Set (S-Set). The\nTS module exploits compositional textual semantics during training, enhancing\nthe pseudo-word token with more linguistic semantics and thus mitigating the\nmodality discrepancy effectively. The S-Set exploits the zero-shot capability\nof pretrained Vision-Language Models (VLMs), alleviating the distribution shift\nand mitigating the overfitting issue from the redundancy of the large-scale\nimage-text data. Extensive experiments over four ZS-CIR benchmarks show that\nDeG outperforms the state-of-the-art (SOTA) methods with much less training\ndata, and saves substantial training and inference time for practical usage.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image-text"
    ],
    "attention_score": 3.12,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05383v1",
    "title": "VLMs Play StarCraft II: A Benchmark and Multimodal Decision Method",
    "url": "http://arxiv.org/abs/2503.05383v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05383v1",
    "authors": [
      "Weiyu Ma",
      "Yuqian Fu",
      "Zecheng Zhang",
      "Guohao Li"
    ],
    "date": "2025-03-07",
    "summary": "We introduce VLM-Attention, a multimodal StarCraft II environment that aligns\nartificial agent perception with the human gameplay experience. Traditional\nframeworks such as SMAC rely on abstract state representations that diverge\nsignificantly from human perception, limiting the ecological validity of agent\nbehavior. Our environment addresses this limitation by incorporating RGB visual\ninputs and natural language observations that more closely simulate human\ncognitive processes during gameplay. The VLM-Attention framework consists of\nthree integrated components: (1) a vision-language model enhanced with\nspecialized self-attention mechanisms for strategic unit targeting and\nbattlefield assessment, (2) a retrieval-augmented generation system that\nleverages domain-specific StarCraft II knowledge to inform tactical decisions,\nand (3) a dynamic role-based task distribution system that enables coordinated\nmulti-agent behavior. Our experimental evaluation across 21 custom scenarios\ndemonstrates that VLM-based agents powered by foundation models (specifically\nQwen-VL and GPT-4o) can execute complex tactical maneuvers without explicit\ntraining, achieving comparable performance to traditional MARL methods that\nrequire substantial training iterations. This work establishes a foundation for\ndeveloping human-aligned StarCraft II agents and advances the broader research\nagenda of multimodal game AI. Our implementation is available at\nhttps://github.com/camel-ai/VLM-Play-StarCraft2.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language"
    ],
    "attention_score": 3.12,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04592v1",
    "title": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning",
    "url": "http://arxiv.org/abs/2503.04592v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04592v1",
    "authors": [
      "Qing Zhou",
      "Tao Yang",
      "Junyu Gao",
      "Weiping Ni",
      "Junzheng Wu",
      "Qi Wang"
    ],
    "date": "2025-03-06",
    "summary": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "image captioning"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04353v1",
    "title": "ObjMST: An Object-Focused Multimodal Style Transfer Framework",
    "url": "http://arxiv.org/abs/2503.04353v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04353v1",
    "authors": [
      "Chanda Grover Kamra",
      "Indra Deep Mastan",
      "Debayan Gupta"
    ],
    "date": "2025-03-06",
    "summary": "We propose ObjMST, an object-focused multimodal style transfer framework that\nprovides separate style supervision for salient objects and surrounding\nelements while addressing alignment issues in multimodal representation\nlearning. Existing image-text multimodal style transfer methods face the\nfollowing challenges: (1) generating non-aligned and inconsistent multimodal\nstyle representations; and (2) content mismatch, where identical style patterns\nare applied to both salient objects and their surrounding elements. Our\napproach mitigates these issues by: (1) introducing a Style-Specific Masked\nDirectional CLIP Loss, which ensures consistent and aligned style\nrepresentations for both salient objects and their surroundings; and (2)\nincorporating a salient-to-key mapping mechanism for stylizing salient objects,\nfollowed by image harmonization to seamlessly blend the stylized objects with\ntheir environment. We validate the effectiveness of ObjMST through experiments,\nusing both quantitative metrics and qualitative visual evaluations of the\nstylized outputs. Our code is available at:\nhttps://github.com/chandagrover/ObjMST.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "CLIP"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04325v2",
    "title": "GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain\n  tumour Segmentation on mp-MRI",
    "url": "http://arxiv.org/abs/2503.04325v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04325v2",
    "authors": [
      "Cecilia Diana-Albelda",
      "Roberto Alcover-Couso",
      "\u00c1lvaro Garc\u00eda-Mart\u00edn",
      "Jesus Bescos",
      "Marcos Escudero-Vi\u00f1olo"
    ],
    "date": "2025-03-06",
    "summary": "Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT",
      "Segment Anything"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04982v1",
    "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
    "url": "http://arxiv.org/abs/2503.04982v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04982v1",
    "authors": [
      "Souvik Kundu",
      "Anahita Bhiwandiwalla",
      "Sungduk Yu",
      "Phillip Howard",
      "Tiep Le",
      "Sharath Nittur Sridhar",
      "David Cobbley",
      "Hao Kang",
      "Vasudev Lal"
    ],
    "date": "2025-03-06",
    "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "visual reasoning"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04871v1",
    "title": "Toward Lightweight and Fast Decoders for Diffusion Models in Image and\n  Video Generation",
    "url": "http://arxiv.org/abs/2503.04871v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04871v1",
    "authors": [
      "Alexey Buzovkin",
      "Evgeny Shilov"
    ],
    "date": "2025-03-06",
    "summary": "We investigate methods to reduce inference time and memory footprint in\nstable diffusion models by introducing lightweight decoders for both image and\nvideo synthesis. Traditional latent diffusion pipelines rely on large\nVariational Autoencoder decoders that can slow down generation and consume\nconsiderable GPU memory. We propose custom-trained decoders using lightweight\nVision Transformer and Taming Transformer architectures. Experiments show up to\n15% overall speed-ups for image generation on COCO2017 and up to 20 times\nfaster decoding in the sub-module, with additional gains on UCF-101 for video\ntasks. Memory requirements are moderately reduced, and while there is a small\ndrop in perceptual quality compared to the default decoder, the improvements in\nspeed and scalability are crucial for large-scale inference scenarios such as\ngenerating 100K images. Our work is further contextualized by advances in\nefficient video generation, including dual masking strategies, illustrating a\nbroader effort to improve the scalability and efficiency of generative models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion",
      "vision transformer",
      "image generation"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04919v1",
    "title": "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D\n  Object Placement",
    "url": "http://arxiv.org/abs/2503.04919v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04919v1",
    "authors": [
      "Ian Huang",
      "Yanan Bao",
      "Karen Truong",
      "Howard Zhou",
      "Cordelia Schmid",
      "Leonidas Guibas",
      "Alireza Fathi"
    ],
    "date": "2025-03-06",
    "summary": "Scene generation with 3D assets presents a complex challenge, requiring both\nhigh-level semantic understanding and low-level geometric reasoning. While\nMultimodal Large Language Models (MLLMs) excel at semantic tasks, their\napplication to 3D scene generation is hindered by their limited grounding on 3D\ngeometry. In this paper, we investigate how to best work with MLLMs in an\nobject placement task. Towards this goal, we introduce a novel framework,\nFirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the\nextraction of relevant geometric details from the 3D scene, (2) constructing\nand solving geometric constraints on the extracted low-level geometry, and (3)\npruning for final placements that conform to common sense. By combining\ngeometric reasoning with real-world understanding of MLLMs, our method can\npropose object placements that satisfy both geometric constraints as well as\nhigh-level semantic common-sense considerations. Our experiments show that\nthese capabilities allow our method to place objects more effectively in\ncomplex scenes with intricate geometry, surpassing the quality of prior work.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04852v1",
    "title": "CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data",
    "url": "http://arxiv.org/abs/2503.04852v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04852v1",
    "authors": [
      "Disheng Liu",
      "Yiran Qiao",
      "Wuche Liu",
      "Yiren Lu",
      "Yunlai Zhou",
      "Tuo Liang",
      "Yu Yin",
      "Jing Ma"
    ],
    "date": "2025-03-06",
    "summary": "True intelligence hinges on the ability to uncover and leverage hidden causal\nrelations. Despite significant progress in AI and computer vision (CV), there\nremains a lack of benchmarks for assessing models' abilities to infer latent\ncausality from complex visual data. In this paper, we introduce\n\\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates\nstructured data (tables) with corresponding visual representations (images) to\nevaluate causal reasoning. Designed within a systematic framework, Causal3D\ncomprises 19 3D-scene datasets capturing diverse causal relations, views, and\nbackgrounds, enabling evaluations across scenes of varying complexity. We\nassess multiple state-of-the-art methods, including classical causal discovery,\ncausal representation learning, and large/vision-language models (LLMs/VLMs).\nOur experiments show that as causal structures grow more complex without prior\nknowledge, performance declines significantly, highlighting the challenges even\nadvanced methods face in complex causal scenarios. Causal3D serves as a vital\nresource for advancing causal reasoning in CV and fostering trustworthy AI in\ncritical domains.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "ViT"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04205v1",
    "title": "Learning 3D Medical Image Models From Brain Functional Connectivity\n  Network Supervision For Mental Disorder Diagnosis",
    "url": "http://arxiv.org/abs/2503.04205v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04205v1",
    "authors": [
      "Xingcan Hu",
      "Wei Wang",
      "Li Xiao"
    ],
    "date": "2025-03-06",
    "summary": "In MRI-based mental disorder diagnosis, most previous studies focus on\nfunctional connectivity network (FCN) derived from functional MRI (fMRI).\nHowever, the small size of annotated fMRI datasets restricts its wide\napplication. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w)\nMRI, which are commonly used and readily accessible in clinical settings, are\noften overlooked. To integrate the complementary information from both function\nand structure for improved diagnostic accuracy, we propose CINP (Contrastive\nImage-Network Pre-training), a framework that employs contrastive learning\nbetween sMRI and FCN. During pre-training, we incorporate masked image modeling\nand network-image matching to enhance visual representation learning and\nmodality alignment. Since the CINP facilitates knowledge transfer from FCN to\nsMRI, we introduce network prompting. It utilizes only sMRI from suspected\npatients and a small amount of FCNs from different patient classes for\ndiagnosing mental disorders, which is practical in real-world clinical\nscenario. The competitive performance on three mental disorder diagnosis tasks\ndemonstrate the effectiveness of the CINP in integrating multimodal MRI\ninformation, as well as the potential of incorporating sMRI into clinical\ndiagnosis using network prompting.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "ViT",
      "visual representation learning"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04459v2",
    "title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering",
    "url": "http://arxiv.org/abs/2503.04459v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04459v2",
    "authors": [
      "Hongyeob Kim",
      "Inyoung Jung",
      "Dayoon Suh",
      "Youjia Zhang",
      "Sangmin Lee",
      "Sungeun Hong"
    ],
    "date": "2025-03-06",
    "summary": "Audio-Visual Question Answering (AVQA) requires not only question-based\nmultimodal reasoning but also precise temporal grounding to capture subtle\ndynamics for accurate prediction. However, existing methods mainly use question\ninformation implicitly, limiting focus on question-specific details.\nFurthermore, most studies rely on uniform frame sampling, which can miss key\nquestion-relevant frames. Although recent Top-K frame selection methods aim to\naddress this, their discrete nature still overlooks fine-grained temporal\ndetails. This paper proposes QA-TIGER, a novel framework that explicitly\nincorporates question information and models continuous temporal dynamics. Our\nkey idea is to use Gaussian-based modeling to adaptively focus on both\nconsecutive and non-consecutive frames based on the question, while explicitly\ninjecting question information and applying progressive refinement. We leverage\na Mixture of Experts (MoE) to flexibly implement multiple Gaussian models,\nactivating temporal experts specifically tailored to the question. Extensive\nexperiments on multiple AVQA benchmarks show that QA-TIGER consistently\nachieves state-of-the-art performance. Code is available at\nhttps://aim-skku.github.io/QA-TIGER/",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04095v2",
    "title": "Chart-HQA: A Benchmark for Hypothetical Question Answering in Charts",
    "url": "http://arxiv.org/abs/2503.04095v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04095v2",
    "authors": [
      "Xiangnan Chen",
      "Yuancheng Fang",
      "Qian Xiao",
      "Juncheng Li",
      "Jun Lin",
      "Siliang Tang",
      "Yi Yang",
      "Yueting Zhuang"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal Large Language Models (MLLMs) have garnered significant attention\nfor their strong visual-semantic understanding. Most existing chart benchmarks\nevaluate MLLMs' ability to parse information from charts to answer questions.\nHowever, they overlook the inherent output biases of MLLMs, where models rely\non their parametric memory to answer questions rather than genuinely\nunderstanding the chart content. To address this limitation, we introduce a\nnovel Chart Hypothetical Question Answering (HQA) task, which imposes\nassumptions on the same question to compel models to engage in counterfactual\nreasoning based on the chart content. Furthermore, we introduce HAI, a human-AI\ninteractive data synthesis approach that leverages the efficient text-editing\ncapabilities of LLMs alongside human expert knowledge to generate diverse and\nhigh-quality HQA data at a low cost. Using HAI, we construct Chart-HQA, a\nchallenging benchmark synthesized from publicly available data sources.\nEvaluation results on 18 MLLMs of varying model sizes reveal that current\nmodels face significant generalization challenges and exhibit imbalanced\nreasoning performance on the HQA task.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 3.09,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03854v1",
    "title": "Vision-Language Models Struggle to Align Entities across Modalities",
    "url": "http://arxiv.org/abs/2503.03854v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03854v1",
    "authors": [
      "I\u00f1igo Alonso",
      "Ander Salaberria",
      "Gorka Azkune",
      "Jeremy Barnes",
      "Oier Lopez de Lacalle"
    ],
    "date": "2025-03-05",
    "summary": "Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language"
    ],
    "attention_score": 3.07,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03840v1",
    "title": "Decoupling the components of geometric understanding in Vision Language\n  Models",
    "url": "http://arxiv.org/abs/2503.03840v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03840v1",
    "authors": [
      "Eliza Kosoy",
      "Annya Dahmani",
      "Andrew K. Lampinen",
      "Iulia M. Comsa",
      "Soojin Jeong",
      "Ishita Dasgupta",
      "Kelsey Allen"
    ],
    "date": "2025-03-05",
    "summary": "Understanding geometry relies heavily on vision. In this work, we evaluate\nwhether state-of-the-art vision language models (VLMs) can understand simple\ngeometric concepts. We use a paradigm from cognitive science that isolates\nvisual understanding of simple geometry from the many other capabilities it is\noften conflated with such as reasoning and world knowledge. We compare model\nperformance with human adults from the USA, as well as with prior research on\nhuman adults without formal education from an Amazonian indigenous group. We\nfind that VLMs consistently underperform both groups of human adults, although\nthey succeed with some concepts more than others. We also find that VLM\ngeometric understanding is more brittle than human understanding, and is not\nrobust when tasks require mental rotation. This work highlights interesting\ndifferences in the origin of geometric understanding in humans and machines --\ne.g. from printed materials used in formal education vs. interactions with the\nphysical world or a combination of the two -- and a small step toward\nunderstanding these differences.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "visual understanding"
    ],
    "attention_score": 3.07,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05064v1",
    "title": "Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided\n  Precision Robotic Manipulation",
    "url": "http://arxiv.org/abs/2503.05064v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05064v1",
    "authors": [
      "Qingxuan Jia",
      "Guoqin Tang",
      "Zeyuan Huang",
      "Zixuan Hao",
      "Ning Ji",
      "Shihang",
      "Yin",
      "Gang Chen"
    ],
    "date": "2025-03-07",
    "summary": "Vision-Language Models (VLMs) demonstrate remarkable potential in robotic\nmanipulation, yet challenges persist in executing complex fine manipulation\ntasks with high speed and precision. While excelling at high-level planning,\nexisting VLM methods struggle to guide robots through precise sequences of fine\nmotor actions. To address this limitation, we introduce a progressive VLM\nplanning algorithm that empowers robots to perform fast, precise, and\nerror-correctable fine manipulation. Our method decomposes complex tasks into\nsub-actions and maintains three key data structures: task memory structure, 2D\ntopology graphs, and 3D spatial networks, achieving high-precision\nspatial-semantic fusion. These three components collectively accumulate and\nstore critical information throughout task execution, providing rich context\nfor our task-oriented VLM interaction mechanism. This enables VLMs to\ndynamically adjust guidance based on real-time feedback, generating precise\naction plans and facilitating step-wise error correction. Experimental\nvalidation on complex assembly tasks demonstrates that our algorithm\neffectively guides robots to rapidly and precisely accomplish fine manipulation\nin challenging scenarios, significantly advancing robot intelligence for\nprecision tasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "attention_score": 2.73,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05149v1",
    "title": "Development and Enhancement of Text-to-Image Diffusion Models",
    "url": "http://arxiv.org/abs/2503.05149v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05149v1",
    "authors": [
      "Rajdeep Roshan Sahu"
    ],
    "date": "2025-03-07",
    "summary": "This research focuses on the development and enhancement of text-to-image\ndenoising diffusion models, addressing key challenges such as limited sample\ndiversity and training instability. By incorporating Classifier-Free Guidance\n(CFG) and Exponential Moving Average (EMA) techniques, this study significantly\nimproves image quality, diversity, and stability. Utilizing Hugging Face's\nstate-of-the-art text-to-image generation model, the proposed enhancements\nestablish new benchmarks in generative AI. This work explores the underlying\nprinciples of diffusion models, implements advanced strategies to overcome\nexisting limitations, and presents a comprehensive evaluation of the\nimprovements achieved. Results demonstrate substantial progress in generating\nstable, diverse, and high-quality images from textual descriptions, advancing\nthe field of generative artificial intelligence and providing new foundations\nfor future applications.\n  Keywords: Text-to-image, Diffusion model, Classifier-free guidance,\nExponential moving average, Image generation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 2.73,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05132v1",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "url": "http://arxiv.org/abs/2503.05132v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05132v1",
    "authors": [
      "Hengguang Zhou",
      "Xirui Li",
      "Ruochen Wang",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ],
    "date": "2025-03-07",
    "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual reasoning"
    ],
    "attention_score": 2.73,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05305v1",
    "title": "Frequency Autoregressive Image Generation with Continuous Tokens",
    "url": "http://arxiv.org/abs/2503.05305v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05305v1",
    "authors": [
      "Hu Yu",
      "Hao Luo",
      "Hangjie Yuan",
      "Yu Rong",
      "Feng Zhao"
    ],
    "date": "2025-03-07",
    "summary": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 2.73,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04078v1",
    "title": "Spatial-Temporal Perception with Causal Inference for Naturalistic\n  Driving Action Recognition",
    "url": "http://arxiv.org/abs/2503.04078v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04078v1",
    "authors": [
      "Qing Chang",
      "Wei Dai",
      "Zhihao Shuai",
      "Limin Yu",
      "Yutao Yue"
    ],
    "date": "2025-03-06",
    "summary": "Naturalistic driving action recognition is essential for vehicle cabin\nmonitoring systems. However, the complexity of real-world backgrounds presents\nsignificant challenges for this task, and previous approaches have struggled\nwith practical implementation due to their limited ability to observe subtle\nbehavioral differences and effectively learn inter-frame features from video.\nIn this paper, we propose a novel Spatial-Temporal Perception (STP)\narchitecture that emphasizes both temporal information and spatial\nrelationships between key objects, incorporating a causal decoder to perform\nbehavior recognition and temporal action localization. Without requiring\nmultimodal input, STP directly extracts temporal and spatial distance features\nfrom RGB video clips. Subsequently, these dual features are jointly encoded by\nmaximizing the expected likelihood across all possible permutations of the\nfactorization order. By integrating temporal and spatial features at different\nscales, STP can perceive subtle behavioral changes in challenging scenarios.\nAdditionally, we introduce a causal-aware module to explore relationships\nbetween video frame features, significantly enhancing detection efficiency and\nperformance. We validate the effectiveness of our approach using two publicly\navailable driver distraction detection benchmarks. The results demonstrate that\nour framework achieves state-of-the-art performance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "CLIP"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04918v1",
    "title": "Fine-Tuning Florence2 for Enhanced Object Detection in Un-constructed\n  Environments: Vision-Language Model Approach",
    "url": "http://arxiv.org/abs/2503.04918v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04918v1",
    "authors": [
      "Soumyadeep Ro",
      "Sanapala Satwika",
      "Pamarthi Yasoda Gayathri",
      "Mohmmad Ghaith Balsha",
      "Aysegul Ucar"
    ],
    "date": "2025-03-06",
    "summary": "Artificial intelligence has progressed through the development of\nVision-Language Models (VLMs), which integrate text and visual inputs to\nachieve comprehensive understanding and interaction in various contexts.\nEnhancing the performance of these models such as the transformer based\nFlorence 2 on specialized tasks like object detection in complex and\nunstructured environments requires fine-tuning. The goal of this paper is to\nimprove the efficiency of the Florence 2 model in challenging environments by\nfinetuning it. We accomplished this by experimenting with different\nconfigurations, using various GPU types (T4, L4, A100) and optimizers such as\nAdamW and SGD. We also employed a range of learning rates and LoRA (Low Rank\nAdaptation) settings. Analyzing the performance metrics, such as Mean Average\nPrecision (mAP) scores,reveals that the finetuned Florence 2 models performed\ncomparably to YOLO models, including YOLOv8, YOLOv9, and YOLOv10. This\ndemonstrates how transformer based VLMs can be adapted for detailed object\ndetection tasks. The paper emphasizes the capability of optimized transformer\nbased VLMs to address specific challenges in object detection within\nunstructured environments, opening up promising avenues for practical\napplications in demanding and complex settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04121v1",
    "title": "Simple Self Organizing Map with Visual Transformer",
    "url": "http://arxiv.org/abs/2503.04121v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04121v1",
    "authors": [
      "Alan Luo",
      "Kaiwen Yuan"
    ],
    "date": "2025-03-06",
    "summary": "Vision Transformers (ViTs) have demonstrated exceptional performance in\nvarious vision tasks. However, they tend to underperform on smaller datasets\ndue to their inherent lack of inductive biases. Current approaches address this\nlimitation implicitly-often by pairing ViTs with pretext tasks or by distilling\nknowledge from convolutional neural networks (CNNs) to strengthen the prior. In\ncontrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised\nframework, are inherently structured to preserve topology and spatial\norganization, making them a promising candidate to directly address the\nlimitations of ViTs in limited or small training datasets. Despite this\npotential, equipping SOMs with modern deep learning architectures remains\nlargely unexplored. In this study, we conduct a novel exploration on how Vision\nTransformers (ViTs) and Self-Organizing Maps (SOMs) can empower each other,\naiming to bridge this critical research gap. Our findings demonstrate that\nthese architectures can synergistically enhance each other, leading to\nsignificantly improved performance in both unsupervised and supervised tasks.\nCode will be publicly available.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04457v1",
    "title": "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction",
    "url": "http://arxiv.org/abs/2503.04457v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04457v1",
    "authors": [
      "Chao Wang",
      "Weiwei Fu",
      "Yang Zhou"
    ],
    "date": "2025-03-06",
    "summary": "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04545v1",
    "title": "ViT-VS: On the Applicability of Pretrained Vision Transformer Features\n  for Generalizable Visual Servoing",
    "url": "http://arxiv.org/abs/2503.04545v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04545v1",
    "authors": [
      "Alessandro Scherl",
      "Stefan Thalhammer",
      "Bernhard Neuberger",
      "Wilfried W\u00f6ber",
      "Jos\u00e9 Grac\u00eda-Rodr\u00edguez"
    ],
    "date": "2025-03-06",
    "summary": "Visual servoing enables robots to precisely position their end-effector\nrelative to a target object. While classical methods rely on hand-crafted\nfeatures and thus are universally applicable without task-specific training,\nthey often struggle with occlusions and environmental variations, whereas\nlearning-based approaches improve robustness but typically require extensive\ntraining. We present a visual servoing approach that leverages pretrained\nvision transformers for semantic feature extraction, combining the advantages\nof both paradigms while also being able to generalize beyond the provided\nsample. Our approach achieves full convergence in unperturbed scenarios and\nsurpasses classical image-based visual servoing by up to 31.2\\% relative\nimprovement in perturbed scenarios. Even the convergence rates of\nlearning-based methods are matched despite requiring no task- or\nobject-specific training. Real-world evaluations confirm robust performance in\nend-effector positioning, industrial box manipulation, and grasping of unseen\nobjects using only a reference from the same category. Our code and simulation\nenvironment are available at: https://alessandroscherl.github.io/ViT-VS/",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04639v1",
    "title": "Enhancing SAM with Efficient Prompting and Preference Optimization for\n  Semi-supervised Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2503.04639v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04639v1",
    "authors": [
      "Aishik Konwer",
      "Zhijian Yang",
      "Erhan Bas",
      "Cao Xiao",
      "Prateek Prasanna",
      "Parminder Bhatia",
      "Taha Kass-Hout"
    ],
    "date": "2025-03-06",
    "summary": "Foundational models such as the Segment Anything Model (SAM) are gaining\ntraction in medical imaging segmentation, supporting multiple downstream tasks.\nHowever, such models are supervised in nature, still relying on large annotated\ndatasets or prompts supplied by experts. Conventional techniques such as active\nlearning to alleviate such limitations are limited in scope and still\nnecessitate continuous human involvement and complex domain knowledge for label\nrefinement or establishing reward ground truth. To address these challenges, we\npropose an enhanced Segment Anything Model (SAM) framework that utilizes\nannotation-efficient prompts generated in a fully unsupervised fashion, while\nstill capturing essential semantic, location, and shape information through\ncontrastive language-image pretraining and visual question answering. We adopt\nthe direct preference optimization technique to design an optimal policy that\nenables the model to generate high-fidelity segmentations with simple ratings\nor rankings provided by a virtual annotator simulating the human annotation\nprocess. State-of-the-art performance of our framework in tasks such as lung\nsegmentation, breast tumor segmentation, and organ segmentation across various\nmodalities, including X-ray, ultrasound, and abdominal CT, justifies its\neffectiveness in low-annotation data scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual question answering",
      "Segment Anything"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04504v1",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "url": "http://arxiv.org/abs/2503.04504v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04504v1",
    "authors": [
      "Sunghyun Ahn",
      "Youngwan Jo",
      "Kijung Lee",
      "Sein Kwon",
      "Inpyo Hong",
      "Sanghyun Park"
    ],
    "date": "2025-03-06",
    "summary": "Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "visual question answering"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04144v1",
    "title": "DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person\n  Retrieval",
    "url": "http://arxiv.org/abs/2503.04144v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04144v1",
    "authors": [
      "Yating Liu",
      "Zimo Liu",
      "Xiangyuan Lan",
      "Wenming Yang",
      "Yaowei Li",
      "Qingmin Liao"
    ],
    "date": "2025-03-06",
    "summary": "Text-based person retrieval (TPR) has gained significant attention as a\nfine-grained and challenging task that closely aligns with practical\napplications. Tailoring CLIP to person domain is now a emerging research topic\ndue to the abundant knowledge of vision-language pretraining, but challenges\nstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is\ncomputationally expensive and prone to overfitting.(ii) Existing\nparameter-efficient transfer learning (PETL) for TPR lacks of fine-grained\nfeature extraction. To address these issues, we propose Domain-Aware\nMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and\nPETL to enhance fine-grained feature representations while maintaining\nefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to\nMLP layers in both vision and language branches, where different experts\nspecialize in distinct aspects of person knowledge to handle features more\nfinely. To promote the router to exploit domain information effectively and\nalleviate the routing imbalance, Domain-Aware Router is then developed by\nbuilding a novel gating function and injecting learnable domain-aware prompts.\nExtensive experiments show that our DM-Adapter achieves state-of-the-art\nperformance, outperforming previous methods by a significant margin.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04058v1",
    "title": "EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language\n  Models",
    "url": "http://arxiv.org/abs/2503.04058v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04058v1",
    "authors": [
      "Haiyang Yu",
      "Jinghui Lu",
      "Yanjie Wang",
      "Yang Li",
      "Han Wang",
      "Can Huang",
      "Bin Li"
    ],
    "date": "2025-03-06",
    "summary": "The advent of Large Vision-Language Models (LVLMs) has advanced the\nvideo-based tasks, such as video captioning and video understanding. Some\nprevious research indicates that taking texts in videos as input can further\nimprove the performance of video understanding. As a type of indispensable\ninformation in short videos or movies, subtitles can assist LVLMs to better\nunderstand videos. Most existing methods for video subtitle extraction are\nbased on a multi-stage framework, handling each frame independently. They can\nhardly exploit the temporal information of videos. Although some LVLMs exhibit\nthe robust OCR capability, predicting accurate timestamps for subtitle texts is\nstill challenging. In this paper, we propose an End-to-end Video Subtitle\nExtraction method, called EVE, which consists of three modules: a vision\nencoder, an adapter module, and a large language model. To effectively compress\nthe visual tokens from the vision encoder, we propose a novel adapter\nInterleavedVT to interleave two modalities. It contains a visual compressor and\na textual region compressor. The proposed InterleavedVT exploits both the\nmerits of average pooling and Q-Former in token compression. Taking the\ntemporal information of videos into account, we introduce a sliding-window\nmechanism in the textual region compressor. To benchmark the video subtitle\nextraction task, we propose a large dataset ViSa including 2.5M videos.\nExtensive experiments on ViSa demonstrate that the proposed EVE can outperform\nexisting open-sourced tools and LVLMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04506v1",
    "title": "Multi-modal Summarization in Model-Based Engineering: Automotive\n  Software Development Case Study",
    "url": "http://arxiv.org/abs/2503.04506v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04506v1",
    "authors": [
      "Nenad Petrovic",
      "Yurui Zhang",
      "Moaad Maaroufi",
      "Kuo-Yi Chao",
      "Lukasz Mazur",
      "Fengjunjie Pan",
      "Vahid Zolfaghari",
      "Alois Knoll"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal summarization integrating information from diverse data modalities\npresents a promising solution to aid the understanding of information within\nvarious processes. However, the application and advantages of multimodal\nsummarization have not received much attention in model-based engineering\n(MBE), where it has become a cornerstone in the design and development of\ncomplex systems, leveraging formal models to improve understanding, validation\nand automation throughout the engineering lifecycle. UML and EMF diagrams in\nmodel-based engineering contain a large amount of multimodal information and\nintricate relational data. Hence, our study explores the application of\nmultimodal large language models within the domain of model-based engineering\nto evaluate their capacity for understanding and identifying relationships,\nfeatures, and functionalities embedded in UML and EMF diagrams. We aim to\ndemonstrate the transformative potential benefits and limitations of multimodal\nsummarization in improving productivity and accuracy in MBE practices. The\nproposed approach is evaluated within the context of automotive software\ndevelopment, while many promising state-of-art models were taken into account.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "ViT"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04215v1",
    "title": "Energy-Guided Optimization for Personalized Image Editing with\n  Pretrained Text-to-Image Diffusion Models",
    "url": "http://arxiv.org/abs/2503.04215v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04215v1",
    "authors": [
      "Rui Jiang",
      "Xinghe Fu",
      "Guangcong Zheng",
      "Teng Li",
      "Taiping Yao",
      "Xi Li"
    ],
    "date": "2025-03-06",
    "summary": "The rapid advancement of pretrained text-driven diffusion models has\nsignificantly enriched applications in image generation and editing. However,\nas the demand for personalized content editing increases, new challenges emerge\nespecially when dealing with arbitrary objects and complex scenes. Existing\nmethods usually mistakes mask as the object shape prior, which struggle to\nachieve a seamless integration result. The mostly used inversion noise\ninitialization also hinders the identity consistency towards the target object.\nTo address these challenges, we propose a novel training-free framework that\nformulates personalized content editing as the optimization of edited images in\nthe latent space, using diffusion models as the energy function guidance\nconditioned by reference text-image pairs. A coarse-to-fine strategy is\nproposed that employs text energy guidance at the early stage to achieve a\nnatural transition toward the target class and uses point-to-point\nfeature-level image energy guidance to perform fine-grained appearance\nalignment with the target object. Additionally, we introduce the latent space\ncontent composition to enhance overall identity consistency with the target.\nExtensive experiments demonstrate that our method excels in object replacement\neven with a large domain gap, highlighting its potential for high-quality,\npersonalized image editing.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04724v1",
    "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
    "url": "http://arxiv.org/abs/2503.04724v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04724v1",
    "authors": [
      "Sambal Shikhar",
      "Mohammed Irfan Kurpath",
      "Sahal Shaji Mullappilly",
      "Jean Lahoud",
      "Fahad Khan",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Hisham Cholakkal"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04167v1",
    "title": "The Role of Visual Modality in Multimodal Mathematical Reasoning:\n  Challenges and Insights",
    "url": "http://arxiv.org/abs/2503.04167v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04167v1",
    "authors": [
      "Yufang Liu",
      "Yao Du",
      "Tao Ji",
      "Jianing Wang",
      "Yang Liu",
      "Yuanbin Wu",
      "Aimin Zhou",
      "Mengdi Zhang",
      "Xunliang Cai"
    ],
    "date": "2025-03-06",
    "summary": "Recent research has increasingly focused on multimodal mathematical\nreasoning, particularly emphasizing the creation of relevant datasets and\nbenchmarks. Despite this, the role of visual information in reasoning has been\nunderexplored. Our findings show that existing multimodal mathematical models\nminimally leverage visual information, and model performance remains largely\nunaffected by changes to or removal of images in the dataset. We attribute this\nto the dominance of textual information and answer options that inadvertently\nguide the model to correct answers. To improve evaluation methods, we introduce\nthe HC-M3D dataset, specifically designed to require image reliance for\nproblem-solving and to challenge models with similar, yet distinct, images that\nchange the correct answer. In testing leading models, their failure to detect\nthese subtle visual differences suggests limitations in current visual\nperception capabilities. Additionally, we observe that the common approach of\nimproving general VQA capabilities by combining various types of image encoders\ndoes not contribute to math reasoning performance. This finding also presents a\nchallenge to enhancing visual reliance during math reasoning. Our benchmark and\ncode would be available at\n\\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\\_modality\\_role}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "VQA"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04130v1",
    "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.04130v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04130v1",
    "authors": [
      "Jindong Jiang",
      "Xiuyu Li",
      "Zhijian Liu",
      "Muyang Li",
      "Guo Chen",
      "Zhiqi Li",
      "De-An Huang",
      "Guilin Liu",
      "Zhiding Yu",
      "Kurt Keutzer",
      "Sungjin Ahn",
      "Jan Kautz",
      "Hongxu Yin",
      "Yao Lu",
      "Song Han",
      "Wonmin Byeon"
    ],
    "date": "2025-03-06",
    "summary": "Recent advances in video-based multimodal large language models (Video-LLMs)\nhave significantly improved video understanding by processing videos as\nsequences of image frames. However, many existing methods treat frames\nindependently in the vision backbone, lacking explicit temporal modeling, which\nlimits their ability to capture dynamic patterns and efficiently handle long\nvideos. To address these limitations, we introduce STORM\n(\\textbf{S}patiotemporal \\textbf{TO}ken \\textbf{R}eduction for\n\\textbf{M}ultimodal LLMs), a novel architecture incorporating a dedicated\ntemporal encoder between the image encoder and the LLM. Our temporal encoder\nleverages the Mamba State Space Model to integrate temporal information into\nimage tokens, generating enriched representations that preserve inter-frame\ndynamics across the entire video sequence. This enriched encoding not only\nenhances video reasoning capabilities but also enables effective token\nreduction strategies, including test-time sampling and training-based temporal\nand spatial pooling, substantially reducing computational demands on the LLM\nwithout sacrificing key temporal information. By integrating these techniques,\nour approach simultaneously reduces training and inference latency while\nimproving performance, enabling efficient and robust video understanding over\nextended temporal contexts. Extensive evaluations show that STORM achieves\nstate-of-the-art results across various long video understanding benchmarks\n(more than 5\\% improvement on MLVU and LongVideoBench) while reducing the\ncomputation costs by up to $8\\times$ and the decoding latency by\n2.4-2.9$\\times$ for the fixed numbers of input frames. Project page is\navailable at https://research.nvidia.com/labs/lpr/storm",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "multimodal LLM"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04006v1",
    "title": "DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for\n  Robust Few-Shot Segmentation",
    "url": "http://arxiv.org/abs/2503.04006v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04006v1",
    "authors": [
      "Amin Karimi",
      "Charalambos Poullis"
    ],
    "date": "2025-03-06",
    "summary": "Few-shot semantic segmentation (FSS) aims to enable models to segment\nnovel/unseen object classes using only a limited number of labeled examples.\nHowever, current FSS methods frequently struggle with generalization due to\nincomplete and biased feature representations, especially when support images\ndo not capture the full appearance variability of the target class. To improve\nthe FSS pipeline, we propose a novel framework that utilizes large language\nmodels (LLMs) to adapt general class semantic information to the query image.\nFurthermore, the framework employs dense pixel-wise matching to identify\nsimilarities between query and support images, resulting in enhanced FSS\nperformance. Inspired by reasoning-based segmentation frameworks, our method,\nnamed DSV-LFS, introduces an additional token into the LLM vocabulary, allowing\na multimodal LLM to generate a \"semantic prompt\" from class descriptions. In\nparallel, a dense matching module identifies visual similarities between the\nquery and support images, generating a \"visual prompt\". These prompts are then\njointly employed to guide the prompt-based decoder for accurate segmentation of\nthe query image. Comprehensive experiments on the benchmark datasets\nPascal-$5^{i}$ and COCO-$20^{i}$ demonstrate that our framework achieves\nstate-of-the-art performance-by a significant margin-demonstrating superior\ngeneralization to novel classes and robustness across diverse scenarios. The\nsource code is available at\n\\href{https://github.com/aminpdik/DSV-LFS}{https://github.com/aminpdik/DSV-LFS}",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "multimodal LLM"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04858v1",
    "title": "SHAPE : Self-Improved Visual Preference Alignment by Iteratively\n  Generating Holistic Winner",
    "url": "http://arxiv.org/abs/2503.04858v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04858v1",
    "authors": [
      "Kejia Chen",
      "Jiawen Zhang",
      "Jiacong Hu",
      "Jiazhen Yang",
      "Jian Lou",
      "Zunlei Feng",
      "Mingli Song"
    ],
    "date": "2025-03-06",
    "summary": "Large Visual Language Models (LVLMs) increasingly rely on preference\nalignment to ensure reliability, which steers the model behavior via preference\nfine-tuning on preference data structured as ``image - winner text - loser\ntext'' triplets. However, existing approaches often suffer from limited\ndiversity and high costs associated with human-annotated preference data,\nhindering LVLMs from fully achieving their intended alignment capabilities. We\npresent \\projectname, a self-supervised framework capable of transforming the\nalready abundant supervised text-image pairs into holistic preference triplets\nfor more effective and cheaper LVLM alignment, eliminating the need for human\npreference annotations. Our approach facilitates LVLMs in progressively\nenhancing alignment capabilities through iterative self-improvement. The key\ndesign rationale is to devise preference triplets where the winner text\nconsistently improves in holisticness and outperforms the loser response in\nquality, thereby pushing the model to ``strive to the utmost'' of alignment\nperformance through preference fine-tuning. For each given text-image pair,\nSHAPE introduces multiple visual augmentations and pairs them with a summarized\ntext to serve as the winner response, while designating the original text as\nthe loser response. Experiments across \\textbf{12} benchmarks on various model\narchitectures and sizes, including LLaVA and DeepSeek-VL, show that SHAPE\nachieves significant gains, for example, achieving +11.3\\% on MMVet\n(comprehensive evaluation), +1.4\\% on MMBench (general VQA), and +8.0\\% on POPE\n(hallucination robustness) over baselines in 7B models. Notably, qualitative\nanalyses confirm enhanced attention to visual details and better alignment with\nhuman preferences for holistic descriptions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "VQA"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04606v1",
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion\n  Models for Video Generation",
    "url": "http://arxiv.org/abs/2503.04606v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04606v1",
    "authors": [
      "Aoxiong Yin",
      "Kai Shen",
      "Yichong Leng",
      "Xu Tan",
      "Xinyu Zhou",
      "Juncheng Li",
      "Siliang Tang"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Keling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-video",
      "Sora"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04417v1",
    "title": "From Idea to CAD: A Language Model-Driven Multi-Agent System for\n  Collaborative Design",
    "url": "http://arxiv.org/abs/2503.04417v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04417v1",
    "authors": [
      "Felix Ocker",
      "Stefan Menzel",
      "Ahmed Sadik",
      "Thiago Rios"
    ],
    "date": "2025-03-06",
    "summary": "Creating digital models using Computer Aided Design (CAD) is a process that\nrequires in-depth expertise. In industrial product development, this process\ntypically involves entire teams of engineers, spanning requirements\nengineering, CAD itself, and quality assurance. We present an approach that\nmirrors this team structure with a Vision Language Model (VLM)-based Multi\nAgent System, with access to parametric CAD tooling and tool documentation.\nCombining agents for requirements engineering, CAD engineering, and\nvision-based quality assurance, a model is generated automatically from\nsketches and/ or textual descriptions. The resulting model can be refined\ncollaboratively in an iterative validation loop with the user. Our approach has\nthe potential to increase the effectiveness of design processes, both for\nindustry experts and for hobbyists who create models for 3D printing. We\ndemonstrate the potential of the architecture at the example of various design\ntasks and provide several ablations that show the benefits of the\narchitecture's individual components.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04201v1",
    "title": "Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative\n  Approach to Few-shot Multimodal Dialogue Intention Recognition",
    "url": "http://arxiv.org/abs/2503.04201v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04201v1",
    "authors": [
      "Bin Chen",
      "Yu Zhang",
      "Hongfei Ye",
      "Ziyi Huang",
      "Hongyang Chen"
    ],
    "date": "2025-03-06",
    "summary": "Few-shot multimodal dialogue intention recognition is a critical challenge in\nthe e-commerce domainn. Previous methods have primarily enhanced model\nclassification capabilities through post-training techniques. However, our\nanalysis reveals that training for few-shot multimodal dialogue intention\nrecognition involves two interconnected tasks, leading to a seesaw effect in\nmulti-task learning. This phenomenon is attributed to knowledge interference\nstemming from the superposition of weight matrix updates during the training\nprocess. To address these challenges, we propose Knowledge-Decoupled Synergetic\nLearning (KDSL), which mitigates these issues by utilizing smaller models to\ntransform knowledge into interpretable rules, while applying the post-training\nof larger models. By facilitating collaboration between the large and small\nmultimodal large language models for prediction, our approach demonstrates\nsignificant improvements. Notably, we achieve outstanding results on two real\nTaobao datasets, with enhancements of 6.37\\% and 6.28\\% in online weighted F1\nscores compared to the state-of-the-art method, thereby validating the efficacy\nof our framework.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "MLLM"
    ],
    "attention_score": 2.71,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03664v1",
    "title": "A Generative Approach to High Fidelity 3D Reconstruction from Text Data",
    "url": "http://arxiv.org/abs/2503.03664v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03664v1",
    "authors": [
      "Venkat Kumar R",
      "Deepak Saravanan"
    ],
    "date": "2025-03-05",
    "summary": "The convergence of generative artificial intelligence and advanced computer\nvision technologies introduces a groundbreaking approach to transforming\ntextual descriptions into three-dimensional representations. This research\nproposes a fully automated pipeline that seamlessly integrates text-to-image\ngeneration, various image processing techniques, and deep learning methods for\nreflection removal and 3D reconstruction. By leveraging state-of-the-art\ngenerative models like Stable Diffusion, the methodology translates natural\nlanguage inputs into detailed 3D models through a multi-stage workflow.\n  The reconstruction process begins with the generation of high-quality images\nfrom textual prompts, followed by enhancement by a reinforcement learning agent\nand reflection removal using the Stable Delight model. Advanced image upscaling\nand background removal techniques are then applied to further enhance visual\nfidelity. These refined two-dimensional representations are subsequently\ntransformed into volumetric 3D models using sophisticated machine learning\nalgorithms, capturing intricate spatial relationships and geometric\ncharacteristics. This process achieves a highly structured and detailed output,\nensuring that the final 3D models reflect both semantic accuracy and geometric\nprecision.\n  This approach addresses key challenges in generative reconstruction, such as\nmaintaining semantic coherence, managing geometric complexity, and preserving\ndetailed visual information. Comprehensive experimental evaluations will assess\nreconstruction quality, semantic accuracy, and geometric fidelity across\ndiverse domains and varying levels of complexity. By demonstrating the\npotential of AI-driven 3D reconstruction techniques, this research offers\nsignificant implications for fields such as augmented reality (AR), virtual\nreality (VR), and digital content creation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion",
      "text-to-image"
    ],
    "attention_score": 2.68,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03613v1",
    "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards\n  Zero-shot Adversarial Robustness of CLIP",
    "url": "http://arxiv.org/abs/2503.03613v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03613v1",
    "authors": [
      "Songlong Xing",
      "Zhengyu Zhao",
      "Nicu Sebe"
    ],
    "date": "2025-03-05",
    "summary": "Despite its prevalent use in image-text matching tasks in a zero-shot manner,\nCLIP has been shown to be highly vulnerable to adversarial perturbations added\nonto images. Recent studies propose to finetune the vision encoder of CLIP with\nadversarial samples generated on the fly, and show improved robustness against\nadversarial attacks on a spectrum of downstream datasets, a property termed as\nzero-shot robustness. In this paper, we show that malicious perturbations that\nseek to maximise the classification loss lead to `falsely stable' images, and\npropose to leverage the pre-trained vision encoder of CLIP to counterattack\nsuch adversarial images during inference to achieve robustness. Our paradigm is\nsimple and training-free, providing the first method to defend CLIP from\nadversarial attacks at test time, which is orthogonal to existing methods\naiming to boost zero-shot adversarial robustness of CLIP. We conduct\nexperiments across 16 classification datasets, and demonstrate stable and\nconsistent gains compared to test-time defence methods adapted from existing\nadversarial robustness studies that do not rely on external networks, without\nnoticeably impairing performance on clean images. We also show that our\nparadigm can be employed on CLIP models that have been adversarially finetuned\nto further enhance their robustness at test time. Our code is available\n\\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image-text",
      "CLIP"
    ],
    "attention_score": 2.68,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03803v1",
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "url": "http://arxiv.org/abs/2503.03803v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03803v1",
    "authors": [
      "Jingkang Yang",
      "Shuai Liu",
      "Hongming Guo",
      "Yuhao Dong",
      "Xiamengwei Zhang",
      "Sicheng Zhang",
      "Pengyun Wang",
      "Zitang Zhou",
      "Binzhu Xie",
      "Ziyue Wang",
      "Bei Ouyang",
      "Zhengyu Lin",
      "Marco Cominelli",
      "Zhongang Cai",
      "Yuanhan Zhang",
      "Peiyuan Zhang",
      "Fangzhou Hong",
      "Joerg Widmer",
      "Francesco Gringoli",
      "Lei Yang",
      "Bo Li",
      "Ziwei Liu"
    ],
    "date": "2025-03-05",
    "summary": "We introduce EgoLife, a project to develop an egocentric life assistant that\naccompanies and enhances personal efficiency through AI-powered wearable\nglasses. To lay the foundation for this assistant, we conducted a comprehensive\ndata collection study where six participants lived together for one week,\ncontinuously recording their daily activities - including discussions,\nshopping, cooking, socializing, and entertainment - using AI glasses for\nmultimodal egocentric video capture, along with synchronized third-person-view\nvideo references. This effort resulted in the EgoLife Dataset, a comprehensive\n300-hour egocentric, interpersonal, multiview, and multimodal daily life\ndataset with intensive annotation. Leveraging this dataset, we introduce\nEgoLifeQA, a suite of long-context, life-oriented question-answering tasks\ndesigned to provide meaningful assistance in daily life by addressing practical\nquestions such as recalling past relevant events, monitoring health habits, and\noffering personalized recommendations. To address the key technical challenges\nof (1) developing robust visual-audio models for egocentric data, (2) enabling\nidentity recognition, and (3) facilitating long-context question answering over\nextensive temporal information, we introduce EgoButler, an integrated system\ncomprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on\negocentric datasets, achieving state-of-the-art performance on egocentric video\nunderstanding. EgoRAG is a retrieval-based component that supports answering\nultra-long-context questions. Our experimental studies verify their working\nmechanisms and reveal critical factors and bottlenecks, guiding future\nimprovements. By releasing our datasets, models, and benchmarks, we aim to\nstimulate further research in egocentric AI assistants.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "ViT"
    ],
    "attention_score": 2.68,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03935v1",
    "title": "GlucoLens: Explainable Postprandial Blood Glucose Prediction from Diet\n  and Physical Activity",
    "url": "http://arxiv.org/abs/2503.03935v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03935v1",
    "authors": [
      "Abdullah Mamun",
      "Asiful Arefeen",
      "Susan B. Racette",
      "Dorothy D. Sears",
      "Corrie M. Whisner",
      "Matthew P. Buman",
      "Hassan Ghasemzadeh"
    ],
    "date": "2025-03-05",
    "summary": "Postprandial hyperglycemia, marked by the blood glucose level exceeding the\nnormal range after meals, is a critical indicator of progression toward type 2\ndiabetes in prediabetic and healthy individuals. A key metric for understanding\nblood glucose dynamics after eating is the postprandial area under the curve\n(PAUC). Predicting PAUC in advance based on a person's diet and activity level\nand explaining what affects postprandial blood glucose could allow an\nindividual to adjust their lifestyle accordingly to maintain normal glucose\nlevels. In this paper, we propose GlucoLens, an explainable machine learning\napproach to predict PAUC and hyperglycemia from diet, activity, and recent\nglucose patterns. We conducted a five-week user study with 10 full-time working\nindividuals to develop and evaluate the computational model. Our machine\nlearning model takes multimodal data including fasting glucose, recent glucose,\nrecent activity, and macronutrient amounts, and provides an interpretable\nprediction of the postprandial glucose pattern. Our extensive analyses of the\ncollected data revealed that the trained model achieves a normalized root mean\nsquared error (NRMSE) of 0.123. On average, GlucoLense with a Random Forest\nbackbone provides a 16% better result than the baseline models. Additionally,\nGlucoLens predicts hyperglycemia with an accuracy of 74% and recommends\ndifferent options to help avoid hyperglycemia through diverse counterfactual\nexplanations. Code available: https://github.com/ab9mamun/GlucoLens.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "ViT"
    ],
    "attention_score": 2.68,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03734v1",
    "title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction",
    "url": "http://arxiv.org/abs/2503.03734v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03734v1",
    "authors": [
      "Huang Huang",
      "Fangchen Liu",
      "Letian Fu",
      "Tingfan Wu",
      "Mustafa Mukadam",
      "Jitendra Malik",
      "Ken Goldberg",
      "Pieter Abbeel"
    ],
    "date": "2025-03-05",
    "summary": "Vision-Language-Action (VLA) models aim to predict robotic actions based on\nvisual observations and language instructions. Existing approaches require\nfine-tuning pre-trained visionlanguage models (VLMs) as visual and language\nfeatures are independently fed into downstream policies, degrading the\npre-trained semantic alignments. We propose OTTER, a novel VLA architecture\nthat leverages these existing alignments through explicit, text-aware visual\nfeature extraction. Instead of processing all visual features, OTTER\nselectively extracts and passes only task-relevant visual features that are\nsemantically aligned with the language instruction to the policy transformer.\nThis allows OTTER to keep the pre-trained vision-language encoders frozen.\nThereby, OTTER preserves and utilizes the rich semantic understanding learned\nfrom large-scale pre-training, enabling strong zero-shot generalization\ncapabilities. In simulation and real-world experiments, OTTER significantly\noutperforms existing VLA models, demonstrating strong zeroshot generalization\nto novel objects and environments. Video, code, checkpoints, and dataset:\nhttps://ottervla.github.io/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "attention_score": 2.68,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05689v1",
    "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
    "url": "http://arxiv.org/abs/2503.05689v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05689v1",
    "authors": [
      "Zebin Xing",
      "Xingyu Zhang",
      "Yang Hu",
      "Bo Jiang",
      "Tong He",
      "Qian Zhang",
      "Xiaoxiao Long",
      "Wei Yin"
    ],
    "date": "2025-03-07",
    "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the Navsim\\cite{Dauner2024_navsim},\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05228v1",
    "title": "RecipeGen: A Benchmark for Real-World Recipe Image Generation",
    "url": "http://arxiv.org/abs/2503.05228v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05228v1",
    "authors": [
      "Ruoxuan Zhang",
      "Hongxia Xie",
      "Yi Yao",
      "Jian-Yu Jiang-Lin",
      "Bin Wen",
      "Ling Lo",
      "Hong-Han Shuai",
      "Yung-Hui Li",
      "Wen-Huang Cheng"
    ],
    "date": "2025-03-07",
    "summary": "Recipe image generation is an important challenge in food computing, with\napplications from culinary education to interactive recipe platforms. However,\nthere is currently no real-world dataset that comprehensively connects recipe\ngoals, sequential steps, and corresponding images. To address this, we\nintroduce RecipeGen, the first real-world goal-step-image benchmark for recipe\ngeneration, featuring diverse ingredients, varied recipe steps, multiple\ncooking styles, and a broad collection of food categories. Data is in\nhttps://github.com/zhangdaxia22/RecipeGen.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05255v1",
    "title": "CMMCoT: Enhancing Complex Multi-Image Comprehension via Multi-Modal\n  Chain-of-Thought and Memory Augmentation",
    "url": "http://arxiv.org/abs/2503.05255v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05255v1",
    "authors": [
      "Guanghao Zhang",
      "Tao Zhong",
      "Yan Xia",
      "Zhelun Yu",
      "Haoyuan Li",
      "Wanggui He",
      "Fangxun Shu",
      "Mushui Liu",
      "Dong She",
      "Yi Wang",
      "Hao Jiang"
    ],
    "date": "2025-03-07",
    "summary": "While previous multimodal slow-thinking methods have demonstrated remarkable\nsuccess in single-image understanding scenarios, their effectiveness becomes\nfundamentally constrained when extended to more complex multi-image\ncomprehension tasks. This limitation stems from their predominant reliance on\ntext-based intermediate reasoning processes. While for human, when engaging in\nsophisticated multi-image analysis, they typically perform two complementary\ncognitive operations: (1) continuous cross-image visual comparison through\nregion-of-interest matching, and (2) dynamic memorization of critical visual\nconcepts throughout the reasoning chain. Motivated by these observations, we\npropose the Complex Multi-Modal Chain-of-Thought (CMMCoT) framework, a\nmulti-step reasoning framework that mimics human-like \"slow thinking\" for\nmulti-image understanding. Our approach incorporates two key innovations: 1.\nThe construction of interleaved multimodal multi-step reasoning chains, which\nutilize critical visual region tokens, extracted from intermediate reasoning\nsteps, as supervisory signals. This mechanism not only facilitates\ncomprehensive cross-modal understanding but also enhances model\ninterpretability. 2. The introduction of a test-time memory augmentation module\nthat expands the model reasoning capacity during inference while preserving\nparameter efficiency. Furthermore, to facilitate research in this direction, we\nhave curated a novel multi-image slow-thinking dataset. Extensive experiments\ndemonstrate the effectiveness of our model.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05236v1",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "url": "http://arxiv.org/abs/2503.05236v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05236v1",
    "authors": [
      "Yibin Wang",
      "Yuhang Zang",
      "Hao Li",
      "Cheng Jin",
      "Jiaqi Wang"
    ],
    "date": "2025-03-07",
    "summary": "Recent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guide preference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposes UnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling both pairwise ranking and pointwise scoring, which can be\nemployed for vision model preference alignment. Specifically, (1) we first\ndevelop UnifiedReward on our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment through Direct Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significantly improving the performance in each\ndomain.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05214v1",
    "title": "Gaussian Random Fields as an Abstract Representation of Patient Metadata\n  for Multimodal Medical Image Segmentation",
    "url": "http://arxiv.org/abs/2503.05214v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05214v1",
    "authors": [
      "Bill Cassidy",
      "Christian McBride",
      "Connah Kendrick",
      "Neil D. Reeves",
      "Joseph M. Pappachan",
      "Shaghayegh Raad",
      "Moi Hoon Yap"
    ],
    "date": "2025-03-07",
    "summary": "The growing rate of chronic wound occurrence, especially in patients with\ndiabetes, has become a concerning trend in recent years. Chronic wounds are\ndifficult and costly to treat, and have become a serious burden on health care\nsystems worldwide. Chronic wounds can have devastating consequences for the\npatient, with infection often leading to reduced quality of life and increased\nmortality risk. Innovative deep learning methods for the detection and\nmonitoring of such wounds have the potential to reduce the impact to both\npatient and clinician. We present a novel multimodal segmentation method which\nallows for the introduction of patient metadata into the training workflow\nwhereby the patient data are expressed as Gaussian random fields. Our results\nindicate that the proposed method improved performance when utilising multiple\nmodels, each trained on different metadata categories. Using the Diabetic Foot\nUlcer Challenge 2022 test set, when compared to the baseline results\n(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we\ndemonstrate improvements of +0.0220 and +0.0229 for intersection over union and\nDice similarity coefficient respectively. This paper presents the first study\nto focus on integrating patient data into a chronic wound segmentation\nworkflow. Our results show significant performance gains when training\nindividual models using specific metadata categories, followed by average\nmerging of prediction masks using distance transforms. All source code for this\nstudy is available at:\nhttps://github.com/mmu-dermatology-research/multimodal-grf",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05534v1",
    "title": "S4M: Segment Anything with 4 Extreme Points",
    "url": "http://arxiv.org/abs/2503.05534v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05534v1",
    "authors": [
      "Adrien Meyer",
      "Lorenzo Arboit",
      "Giuseppe Massimiani",
      "Francesco Brucchi",
      "Luca Emanuele Amodio",
      "Didier Mutter",
      "Nicolas Padoy"
    ],
    "date": "2025-03-07",
    "summary": "The Segment Anything Model (SAM) has revolutionized open-set interactive\nimage segmentation, inspiring numerous adapters for the medical domain.\nHowever, SAM primarily relies on sparse prompts such as point or bounding box,\nwhich may be suboptimal for fine-grained instance segmentation, particularly in\nendoscopic imagery, where precise localization is critical and existing prompts\nstruggle to capture object boundaries effectively. To address this, we\nintroduce S4M (Segment Anything with 4 Extreme Points), which augments SAM by\nleveraging extreme points -- the top-, bottom-, left-, and right-most points of\nan instance -- prompts. These points are intuitive to identify and provide a\nfaster, structured alternative to box prompts. However, a na\\\"ive use of\nextreme points degrades performance, due to SAM's inability to interpret their\nsemantic roles. To resolve this, we introduce dedicated learnable embeddings,\nenabling the model to distinguish extreme points from generic free-form points\nand better reason about their spatial relationships. We further propose an\nauxiliary training task through the Canvas module, which operates solely on\nprompts -- without vision input -- to predict a coarse instance mask. This\nencourages the model to internalize the relationship between extreme points and\nmask distributions, leading to more robust segmentation. S4M outperforms other\nSAM-based approaches on three endoscopic surgical datasets, demonstrating its\neffectiveness in complex scenarios. Finally, we validate our approach through a\nhuman annotation study on surgical endoscopic videos, confirming that extreme\npoints are faster to acquire than bounding boxes.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Segment Anything"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05379v1",
    "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning",
    "url": "http://arxiv.org/abs/2503.05379v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05379v1",
    "authors": [
      "Jiaxing Zhao",
      "Xihan Wei",
      "Liefeng Bo"
    ],
    "date": "2025-03-07",
    "summary": "In this work, we present the first application of Reinforcement Learning with\nVerifiable Reward (RLVR) to an Omni-multimodal large language model in the\ncontext of emotion recognition, a task where both visual and audio modalities\nplay crucial roles. We leverage RLVR to optimize the Omni model, significantly\nenhancing its performance in three key aspects: reasoning capability, emotion\nrecognition accuracy, and generalization ability. The introduction of RLVR not\nonly improves the model's overall performance on in-distribution data but also\ndemonstrates superior robustness when evaluated on out-of-distribution\ndatasets. More importantly, the improved reasoning capability enables clear\nanalysis of the contributions of different modalities, particularly visual and\naudio information, in the emotion recognition process. This provides valuable\ninsights into the optimization of multimodal large language models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05424v1",
    "title": "Towards Locally Explaining Prediction Behavior via Gradual Interventions\n  and Measuring Property Gradients",
    "url": "http://arxiv.org/abs/2503.05424v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05424v1",
    "authors": [
      "Niklas Penzel",
      "Joachim Denzler"
    ],
    "date": "2025-03-07",
    "summary": "Deep learning models achieve high predictive performance but lack intrinsic\ninterpretability, hindering our understanding of the learned prediction\nbehavior. Existing local explainability methods focus on associations,\nneglecting the causal drivers of model predictions. Other approaches adopt a\ncausal perspective but primarily provide more general global explanations.\nHowever, for specific inputs, it's unclear whether globally identified factors\napply locally. To address this limitation, we introduce a novel framework for\nlocal interventional explanations by leveraging recent advances in\nimage-to-image editing models. Our approach performs gradual interventions on\nsemantic properties to quantify the corresponding impact on a model's\npredictions using a novel score, the expected property gradient magnitude. We\ndemonstrate the effectiveness of our approach through an extensive empirical\nevaluation on a wide range of architectures and tasks. First, we validate it in\na synthetic scenario and demonstrate its ability to locally identify biases.\nAfterward, we apply our approach to analyze network training dynamics,\ninvestigate medical skin lesion classifiers, and study a pre-trained CLIP model\nwith real-life interventional data. Our results highlight the potential of\ninterventional explanations on the property level to reveal new insights into\nthe behavior of deep models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05639v1",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control",
    "url": "http://arxiv.org/abs/2503.05639v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05639v1",
    "authors": [
      "Yuxuan Bian",
      "Zhaoyang Zhang",
      "Xuan Ju",
      "Mingdeng Cao",
      "Liangbin Xie",
      "Ying Shan",
      "Qiang Xu"
    ],
    "date": "2025-03-07",
    "summary": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05298v1",
    "title": "Coreference as an indicator of context scope in multimodal narrative",
    "url": "http://arxiv.org/abs/2503.05298v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05298v1",
    "authors": [
      "Nikolai Ilinykh",
      "Shalom Lappin",
      "Asad Sayeed",
      "Sharid Lo\u00e1iciga"
    ],
    "date": "2025-03-07",
    "summary": "We demonstrate that large multimodal language models differ substantially\nfrom humans in the distribution of coreferential expressions in a visual\nstorytelling task. We introduce a number of metrics to quantify the\ncharacteristics of coreferential patterns in both human- and machine-written\ntexts. Humans distribute coreferential expressions in a way that maintains\nconsistency across texts and images, interleaving references to different\nentities in a highly varied way. Machines are less able to track mixed\nreferences, despite achieving perceived improvements in generation quality.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05595v1",
    "title": "Anti-Diffusion: Preventing Abuse of Modifications of Diffusion-Based\n  Models",
    "url": "http://arxiv.org/abs/2503.05595v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05595v1",
    "authors": [
      "Zheng Li",
      "Liangbin Xie",
      "Jiantao Zhou",
      "Xintao Wang",
      "Haiwei Wu",
      "Jinyu Tian"
    ],
    "date": "2025-03-07",
    "summary": "Although diffusion-based techniques have shown remarkable success in image\ngeneration and editing tasks, their abuse can lead to severe negative social\nimpacts. Recently, some works have been proposed to provide defense against the\nabuse of diffusion-based methods. However, their protection may be limited in\nspecific scenarios by manually defined prompts or the stable diffusion (SD)\nversion. Furthermore, these methods solely focus on tuning methods, overlooking\nediting methods that could also pose a significant threat. In this work, we\npropose Anti-Diffusion, a privacy protection system designed for general\ndiffusion-based methods, applicable to both tuning and editing techniques. To\nmitigate the limitations of manually defined prompts on defense performance, we\nintroduce the prompt tuning (PT) strategy that enables precise expression of\noriginal images. To provide defense against both tuning and editing methods, we\npropose the semantic disturbance loss (SDL) to disrupt the semantic information\nof protected images. Given the limited research on the defense against editing\nmethods, we develop a dataset named Defense-Edit to assess the defense\nperformance of various methods. Experiments demonstrate that our Anti-Diffusion\nachieves superior defense performance across a wide range of diffusion-based\ntechniques in different scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05684v1",
    "title": "Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints",
    "url": "http://arxiv.org/abs/2503.05684v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05684v1",
    "authors": [
      "Parameswaran Kamalaruban",
      "Mark Anderson",
      "Stuart Burrell",
      "Maeve Madigan",
      "Piotr Skalski",
      "David Sutton"
    ],
    "date": "2025-03-07",
    "summary": "Pre-trained foundation models can be adapted for specific tasks using\nLow-Rank Adaptation (LoRA). However, the fairness properties of these adapted\nclassifiers remain underexplored. Existing fairness-aware fine-tuning methods\nrely on direct access to sensitive attributes or their predictors, but in\npractice, these sensitive attributes are often held under strict consumer\nprivacy controls, and neither the attributes nor their predictors are available\nto model developers, hampering the development of fair models. To address this\nissue, we introduce a set of LoRA-based fine-tuning methods that can be trained\nin a distributed fashion, where model developers and fairness auditors\ncollaborate without sharing sensitive attributes or predictors. In this paper,\nwe evaluate three such methods - sensitive unlearning, adversarial training,\nand orthogonality loss - against a fairness-unaware baseline, using experiments\non the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base\nmodel. We find that orthogonality loss consistently reduces bias while\nmaintaining or improving utility, whereas adversarial training improves False\nPositive Rate Parity and Demographic Parity in some cases, and sensitive\nunlearning provides no clear benefit. In tasks where significant biases are\npresent, distributed fairness-aware fine-tuning methods can effectively\neliminate bias without compromising consumer privacy and, in most cases,\nimprove model utility.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05335v1",
    "title": "New multimodal similarity measure for image registration via modeling\n  local functional dependence with linear combination of learned basis\n  functions",
    "url": "http://arxiv.org/abs/2503.05335v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05335v1",
    "authors": [
      "Joel Honkamaa",
      "Pekka Marttinen"
    ],
    "date": "2025-03-07",
    "summary": "The deformable registration of images of different modalities, essential in\nmany medical imaging applications, remains challenging. The main challenge is\ndeveloping a robust measure for image overlap despite the compared images\ncapturing different aspects of the underlying tissue. Here, we explore\nsimilarity metrics based on functional dependence between intensity values of\nregistered images. Although functional dependence is too restrictive on the\nglobal scale, earlier work has shown competitive performance in deformable\nregistration when such measures are applied over small enough contexts. We\nconfirm this finding and further develop the idea by modeling local functional\ndependence via the linear basis function model with the basis functions learned\njointly with the deformation. The measure can be implemented via convolutions,\nmaking it efficient to compute on GPUs. We release the method as an easy-to-use\ntool and show good performance on three datasets compared to well-established\nbaseline and earlier functional dependence-based methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05319v1",
    "title": "Robust Multimodal Learning for Ophthalmic Disease Grading via\n  Disentangled Representation",
    "url": "http://arxiv.org/abs/2503.05319v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05319v1",
    "authors": [
      "Xinkun Wang",
      "Yifang Wang",
      "Senwei Liang",
      "Feilong Tang",
      "Chengzhi Liu",
      "Ming Hu",
      "Chao Hu",
      "Junjun He",
      "Zongyuan Ge",
      "Imran Razzak"
    ],
    "date": "2025-03-07",
    "summary": "This paper discusses how ophthalmologists often rely on multimodal data to\nimprove diagnostic accuracy. However, complete multimodal data is rare in\nreal-world applications due to a lack of medical equipment and concerns about\ndata privacy. Traditional deep learning methods typically address these issues\nby learning representations in latent space. However, the paper highlights two\nkey limitations of these approaches: (i) Task-irrelevant redundant information\n(e.g., numerous slices) in complex modalities leads to significant redundancy\nin latent space representations. (ii) Overlapping multimodal representations\nmake it difficult to extract unique features for each modality. To overcome\nthese challenges, the authors propose the Essence-Point and Disentangle\nRepresentation Learning (EDRL) strategy, which integrates a self-distillation\nmechanism into an end-to-end framework to enhance feature selection and\ndisentanglement for more robust multimodal learning. Specifically, the\nEssence-Point Representation Learning module selects discriminative features\nthat improve disease grading performance. The Disentangled Representation\nLearning module separates multimodal data into modality-common and\nmodality-unique representations, reducing feature entanglement and enhancing\nboth robustness and interpretability in ophthalmic disease diagnosis.\nExperiments on multimodal ophthalmology datasets show that the proposed EDRL\nstrategy significantly outperforms current state-of-the-art methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05231v1",
    "title": "Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot\n  Learning and Human-Robot Interaction",
    "url": "http://arxiv.org/abs/2503.05231v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05231v1",
    "authors": [
      "Shuo Jiang",
      "Haonan Li",
      "Ruochen Ren",
      "Yanmin Zhou",
      "Zhipeng Wang",
      "Bin He"
    ],
    "date": "2025-03-07",
    "summary": "Cutting-edge robot learning techniques including foundation models and\nimitation learning from humans all pose huge demands on large-scale and\nhigh-quality datasets which constitute one of the bottleneck in the general\nintelligent robot fields. This paper presents the Kaiwu multimodal dataset to\naddress the missing real-world synchronized multimodal data problems in the\nsophisticated assembling scenario,especially with dynamics information and its\nfine-grained labelling. The dataset first provides an integration of\nhuman,environment and robot data collection framework with 20 subjects and 30\ninteraction objects resulting in totally 11,664 instances of integrated\nactions. For each of the demonstration,hand motions,operation pressures,sounds\nof the assembling process,multi-view videos, high-precision motion capture\ninformation,eye gaze with first-person videos,electromyography signals are all\nrecorded. Fine-grained multi-level annotation based on absolute timestamp,and\nsemantic segmentation labelling are performed. Kaiwu dataset aims to facilitate\nrobot learning,dexterous manipulation,human intention investigation and\nhuman-robot collaboration research.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05186v1",
    "title": "Narrating the Video: Boosting Text-Video Retrieval via Comprehensive\n  Utilization of Frame-Level Captions",
    "url": "http://arxiv.org/abs/2503.05186v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05186v1",
    "authors": [
      "Chan hur",
      "Jeong-hun Hong",
      "Dong-hun Lee",
      "Dabin Kang",
      "Semin Myeong",
      "Sang-hyo Park",
      "Hyeyoung Park"
    ],
    "date": "2025-03-07",
    "summary": "In recent text-video retrieval, the use of additional captions from\nvision-language models has shown promising effects on the performance. However,\nexisting models using additional captions often have struggled to capture the\nrich semantics, including temporal changes, inherent in the video. In addition,\nincorrect information caused by generative models can lead to inaccurate\nretrieval. To address these issues, we propose a new framework, Narrating the\nVideo (NarVid), which strategically leverages the comprehensive information\navailable from frame-level captions, the narration. The proposed NarVid\nexploits narration in multiple ways: 1) feature enhancement through cross-modal\ninteractions between narration and video, 2) query-aware adaptive filtering to\nsuppress irrelevant or incorrect information, 3) dual-modal matching score by\nadding query-video similarity and query-narration similarity, and 4)\nhard-negative loss to learn discriminative features from multiple perspectives\nusing the two similarities from different views. Experimental results\ndemonstrate that NarVid achieves state-of-the-art performance on various\nbenchmark datasets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05179v1",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
    "url": "http://arxiv.org/abs/2503.05179v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05179v1",
    "authors": [
      "Simon A. Aytes",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ],
    "date": "2025-03-07",
    "summary": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.05626v1",
    "title": "FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE\n  Framework",
    "url": "http://arxiv.org/abs/2503.05626v1",
    "pdf_url": "http://arxiv.org/pdf/2503.05626v1",
    "authors": [
      "Jingyu Xu",
      "Yang Wang"
    ],
    "date": "2025-03-07",
    "summary": "Artificial intelligence has shown the potential to improve diagnostic\naccuracy through medical image analysis for pneumonia diagnosis. However,\ntraditional multimodal approaches often fail to address real-world challenges\nsuch as incomplete data and modality loss. In this study, a Flexible Multimodal\nTransformer (FMT) was proposed, which uses ResNet-50 and BERT for joint\nrepresentation learning, followed by a dynamic masked attention strategy that\nsimulates clinical modality loss to improve robustness; finally, a sequential\nmixture of experts (MOE) architecture was used to achieve multi-level decision\nrefinement. After evaluation on a small multimodal pneumonia dataset, FMT\nachieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1\nscore, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and the\nmedical benchmark CheXMed (90%), providing a scalable solution for multimodal\ndiagnosis of pneumonia in resource-constrained medical settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.34,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9917808219178083,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04050v1",
    "title": "Underlying Semantic Diffusion for Effective and Efficient In-Context\n  Learning",
    "url": "http://arxiv.org/abs/2503.04050v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04050v1",
    "authors": [
      "Zhong Ji",
      "Weilong Cao",
      "Yan Zhang",
      "Yanwei Pang",
      "Jungong Han",
      "Xuelong Li"
    ],
    "date": "2025-03-06",
    "summary": "Diffusion models has emerged as a powerful framework for tasks like image\ncontrollable generation and dense prediction. However, existing models often\nstruggle to capture underlying semantics (e.g., edges, textures, shapes) and\neffectively utilize in-context learning, limiting their contextual\nunderstanding and image generation quality. Additionally, high computational\ncosts and slow inference speeds hinder their real-time applicability. To\naddress these challenges, we propose Underlying Semantic Diffusion\n(US-Diffusion), an enhanced diffusion model that boosts underlying semantics\nlearning, computational efficiency, and in-context learning capabilities on\nmulti-task scenarios. We introduce Separate & Gather Adapter (SGA), which\ndecouples input conditions for different tasks while sharing the architecture,\nenabling better in-context learning and generalization across diverse visual\ndomains. We also present a Feedback-Aided Learning (FAL) framework, which\nleverages feedback signals to guide the model in capturing semantic details and\ndynamically adapting to task-specific contextual cues. Furthermore, we propose\na plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time\nsteps with high-noise levels, which aims at optimizing training and inference\nefficiency while maintaining strong in-context learning performance.\nExperimental results demonstrate that US-Diffusion outperforms the\nstate-of-the-art method, achieving an average reduction of 7.47 in FID on\nMap2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks,\nwhile achieving approximately 9.45 times faster inference speed. Our method\nalso demonstrates superior training efficiency and in-context learning\ncapabilities, excelling in new datasets and tasks, highlighting its robustness\nand adaptability across diverse visual domains.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04199v1",
    "title": "MASTER: Multimodal Segmentation with Text Prompts",
    "url": "http://arxiv.org/abs/2503.04199v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04199v1",
    "authors": [
      "Fuyang Liu",
      "Shun Lu",
      "Jilin Mei",
      "Yu Hu"
    ],
    "date": "2025-03-06",
    "summary": "RGB-Thermal fusion is a potential solution for various weather and light\nconditions in challenging scenarios. However, plenty of studies focus on\ndesigning complex modules to fuse different modalities. With the widespread\napplication of large language models (LLMs), valuable information can be more\neffectively extracted from natural language. Therefore, we aim to leverage the\nadvantages of large language models to design a structurally simple and highly\nadaptable multimodal fusion model architecture. We proposed MultimodAl\nSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM into\nthe fusion of RGB-Thermal multimodal data and allows complex query text to\nparticipate in the fusion process. Our model utilizes a dual-path structure to\nextract information from different modalities of images. Additionally, we\nemploy LLM as the core module for multimodal fusion, enabling the model to\ngenerate learnable codebook tokens from RGB, thermal images, and textual\ninformation. A lightweight image decoder is used to obtain semantic\nsegmentation results. The proposed MASTER performs exceptionally well in\nbenchmark tests across various automated driving scenarios, yielding promising\nresults.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04308v1",
    "title": "Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses\n  in Human-Robot Bartending Tasks",
    "url": "http://arxiv.org/abs/2503.04308v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04308v1",
    "authors": [
      "Luk\u00e1\u0161 Gajdo\u0161ech",
      "Hassan Ali",
      "Jan-Gerrit Habekost",
      "Martin Madaras",
      "Matthias Kerzel",
      "Stefan Wermter"
    ],
    "date": "2025-03-06",
    "summary": "Datasets for object detection often do not account for enough variety of\nglasses, due to their transparent and reflective properties. Specifically,\nopen-vocabulary object detectors, widely used in embodied robotic agents, fail\nto distinguish subclasses of glasses. This scientific gap poses an issue to\nrobotic applications that suffer from accumulating errors between detection,\nplanning, and action execution. The paper introduces a novel method for the\nacquisition of real-world data from RGB-D sensors that minimizes human effort.\nWe propose an auto-labeling pipeline that generates labels for all the acquired\nframes based on the depth measurements. We provide a novel real-world glass\nobject dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a\nhumanoid robot platform. The data set consists of 7850 images recorded from\nfive different cameras. We show that our trained baseline model outperforms\nstate-of-the-art open-vocabulary approaches. In addition, we deploy our\nbaseline model in an embodied agent approach to the NICOL platform, on which it\nachieves a success rate of 81% in a human-robot bartending scenario.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual understanding"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04528v1",
    "title": "Federated Dynamic Modeling and Learning for Spatiotemporal Data\n  Forecasting",
    "url": "http://arxiv.org/abs/2503.04528v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04528v1",
    "authors": [
      "Thien Pham",
      "Angelo Furno",
      "Fa\u00efcel Chamroukhi",
      "Latifa Oukhellou"
    ],
    "date": "2025-03-06",
    "summary": "This paper presents an advanced Federated Learning (FL) framework for\nforecasting complex spatiotemporal data, improving upon recent state-of-the-art\nmodels. In the proposed approach, the original Gated Recurrent Unit (GRU)\nmodule within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent\nNetwork (DSTGCRN) modeling is first replaced with a Long Short-Term Memory\n(LSTM) network, enabling the resulting model to more effectively capture\nlong-term dependencies inherent to time series data. The resulting architecture\nsignificantly improves the model's capacity to handle complex temporal patterns\nin diverse forecasting applications. Furthermore, the proposed FL framework\nintegrates a novel Client-Side Validation (CSV) mechanism, introducing a\ncritical validation step at the client level before incorporating aggregated\nparameters from the central server into local models. This ensures that only\nthe most effective updates are adopted, improving both the robustness and\naccuracy of the forecasting model across clients. The efficiency of our\napproach is demonstrated through extensive experiments on real-world\napplications, including public datasets for multimodal transport demand\nforecasting and private datasets for Origin-Destination (OD) matrix forecasting\nin urban areas. The results demonstrate substantial improvements over\nconventional methods, highlighting the framework's ability to capture complex\nspatiotemporal dependencies while preserving data privacy. This work not only\nprovides a scalable and privacy-preserving solution for real-time,\nregion-specific forecasting and management but also underscores the potential\nof leveraging distributed data sources in a FL context. We provide our\nalgorithms as open-source on GitHub.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04107v1",
    "title": "Fractional Correspondence Framework in Detection Transformer",
    "url": "http://arxiv.org/abs/2503.04107v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04107v1",
    "authors": [
      "Masoumeh Zareapoor",
      "Pourya Shamsolmoali",
      "Huiyu Zhou",
      "Yue Lu",
      "Salvador Garc\u00eda"
    ],
    "date": "2025-03-06",
    "summary": "The Detection Transformer (DETR), by incorporating the Hungarian algorithm,\nhas significantly simplified the matching process in object detection tasks.\nThis algorithm facilitates optimal one-to-one matching of predicted bounding\nboxes to ground-truth annotations during training. While effective, this strict\nmatching process does not inherently account for the varying densities and\ndistributions of objects, leading to suboptimal correspondences such as failing\nto handle multiple detections of the same object or missing small objects. To\naddress this, we propose the Regularized Transport Plan (RTP). RTP introduces a\nflexible matching strategy that captures the cost of aligning predictions with\nground truths to find the most accurate correspondences between these sets. By\nutilizing the differentiable Sinkhorn algorithm, RTP allows for soft,\nfractional matching rather than strict one-to-one assignments. This approach\nenhances the model's capability to manage varying object densities and\ndistributions effectively. Our extensive evaluations on the MS-COCO and VOC\nbenchmarks demonstrate the effectiveness of our approach. RTP-DETR, surpassing\nthe performance of the Deform-DETR and the recently introduced DINO-DETR,\nachieving absolute gains in mAP of +3.8% and +1.7%, respectively.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "DINO"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04490v1",
    "title": "Large Language Models in Bioinformatics: A Survey",
    "url": "http://arxiv.org/abs/2503.04490v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04490v1",
    "authors": [
      "Zhenyu Wang",
      "Zikang Wang",
      "Jiyue Jiang",
      "Pengan Chen",
      "Xiangyu Shi",
      "Yu Li"
    ],
    "date": "2025-03-06",
    "summary": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04971v1",
    "title": "Incentivizing Multi-Tenant Split Federated Learning for Foundation\n  Models at the Network Edge",
    "url": "http://arxiv.org/abs/2503.04971v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04971v1",
    "authors": [
      "Songyuan Li",
      "Jia Hu",
      "Geyong Min",
      "Haojun Huang"
    ],
    "date": "2025-03-06",
    "summary": "Foundation models (FMs) such as GPT-4 exhibit exceptional generative\ncapabilities across diverse downstream tasks through fine-tuning. Split\nFederated Learning (SFL) facilitates privacy-preserving FM fine-tuning on\nresource-constrained local devices by offloading partial FM computations to\nedge servers, enabling device-edge synergistic fine-tuning. Practical edge\nnetworks often host multiple SFL tenants to support diversified downstream\ntasks. However, existing research primarily focuses on single-tenant SFL\nscenarios, and lacks tailored incentive mechanisms for multi-tenant settings,\nwhich are essential to effectively coordinate self-interested local devices for\nparticipation in various downstream tasks, ensuring that each SFL tenant's\ndistinct FM fine-tuning requirements (e.g., FM types, performance targets, and\nfine-tuning deadlines) are met. To address this gap, we propose a novel\nPrice-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offer\nstrategic price incentives, which solicit high-quality device participation for\nefficient FM fine-tuning. Specifically, we first develop a bias-resilient\nglobal SFL model aggregation scheme to eliminate model biases caused by\nindependent device participation. We then derive a rigorous SFL convergence\nbound to evaluate the contributions of heterogeneous devices to FM performance\nimprovements, guiding the incentive strategies of SFL tenants. Furthermore, we\nmodel inter-tenant device competition as a congestion game for Stackelberg\nequilibrium (SE) analysis, deriving each SFL tenant's optimal incentive\nstrategy. Extensive simulations involving four representative SFL tenant types\n(ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images,\nand audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07x\ncompared to state-of-the-art approaches, while consistently meeting fine-tuning\nperformance targets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04135v1",
    "title": "Biological Sequence with Language Model Prompting: A Survey",
    "url": "http://arxiv.org/abs/2503.04135v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04135v1",
    "authors": [
      "Jiyue Jiang",
      "Zikang Wang",
      "Yuheng Shan",
      "Heyan Chai",
      "Jiayi Li",
      "Zixian Ma",
      "Xinrui Zhang",
      "Yu Li"
    ],
    "date": "2025-03-06",
    "summary": "Large Language models (LLMs) have emerged as powerful tools for addressing\nchallenges across diverse domains. Notably, recent studies have demonstrated\nthat large language models significantly enhance the efficiency of biomolecular\nanalysis and synthesis, attracting widespread attention from academics and\nmedicine. In this paper, we systematically investigate the application of\nprompt-based methods with LLMs to biological sequences, including DNA, RNA,\nproteins, and drug discovery tasks. Specifically, we focus on how prompt\nengineering enables LLMs to tackle domain-specific problems, such as promoter\nsequence prediction, protein structure modeling, and drug-target binding\naffinity prediction, often with limited labeled data. Furthermore, our\ndiscussion highlights the transformative potential of prompting in\nbioinformatics while addressing key challenges such as data scarcity,\nmultimodal fusion, and computational resource limitations. Our aim is for this\npaper to function both as a foundational primer for newcomers and a catalyst\nfor continued innovation within this dynamic field of study.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04900v1",
    "title": "Extracting Symbolic Sequences from Visual Representations via\n  Self-Supervised Learning",
    "url": "http://arxiv.org/abs/2503.04900v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04900v1",
    "authors": [
      "Victor Sebastian Martinez Pozos",
      "Ivan Vladimir Meza Ruiz"
    ],
    "date": "2025-03-06",
    "summary": "This paper explores the potential of abstracting complex visual information\ninto discrete, structured symbolic sequences using self-supervised learning\n(SSL). Inspired by how language abstracts and organizes information to enable\nbetter reasoning and generalization, we propose a novel approach for generating\nsymbolic representations from visual data. To learn these sequences, we extend\nthe DINO framework to handle visual and symbolic information. Initial\nexperiments suggest that the generated symbolic sequences capture a meaningful\nlevel of abstraction, though further refinement is required. An advantage of\nour method is its interpretability: the sequences are produced by a decoder\ntransformer using cross-attention, allowing attention maps to be linked to\nspecific symbols and offering insight into how these representations correspond\nto image regions. This approach lays the foundation for creating interpretable\nsymbolic representations with potential applications in high-level scene\nunderstanding.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "DINO"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04154v1",
    "title": "CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised\n  Monocular 3D Detection",
    "url": "http://arxiv.org/abs/2503.04154v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04154v1",
    "authors": [
      "Chupeng Liu",
      "Runkai Zhao",
      "Weidong Cai"
    ],
    "date": "2025-03-06",
    "summary": "Weakly supervised monocular 3D detection, while less annotation-intensive,\noften struggles to capture the global context required for reliable 3D\nreasoning. Conventional label-efficient methods focus on object-centric\nfeatures, neglecting contextual semantic relationships that are critical in\ncomplex scenes. In this work, we propose a Context-Aware Weak Supervision for\nMonocular 3D object detection, namely CA-W3D, to address this limitation in a\ntwo-stage training paradigm. Specifically, we first introduce a pre-training\nstage employing Region-wise Object Contrastive Matching (ROCM), which aligns\nregional object embeddings derived from a trainable monocular 3D encoder and a\nfrozen open-vocabulary 2D visual grounding model. This alignment encourages the\nmonocular encoder to discriminate scene-specific attributes and acquire richer\ncontextual knowledge. In the second stage, we incorporate a pseudo-label\ntraining process with a Dual-to-One Distillation (D2OD) mechanism, which\neffectively transfers contextual priors into the monocular encoder while\npreserving spatial fidelity and maintaining computational efficiency during\ninference. Extensive experiments conducted on the public KITTI benchmark\ndemonstrate the effectiveness of our approach, surpassing the SoTA method over\nall metrics, highlighting the importance of contextual-aware knowledge in\nweakly-supervised monocular 3D detection.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual grounding"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04170v1",
    "title": "Towards Intelligent Transportation with Pedestrians and Vehicles\n  In-the-Loop: A Surveillance Video-Assisted Federated Digital Twin Framework",
    "url": "http://arxiv.org/abs/2503.04170v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04170v1",
    "authors": [
      "Xiaolong Li",
      "Jianhao Wei",
      "Haidong Wang",
      "Li Dong",
      "Ruoyang Chen",
      "Changyan Yi",
      "Jun Cai",
      "Dusit Niyato",
      "Xuemin",
      "Shen"
    ],
    "date": "2025-03-06",
    "summary": "In intelligent transportation systems (ITSs), incorporating pedestrians and\nvehicles in-the-loop is crucial for developing realistic and safe traffic\nmanagement solutions. However, there is falls short of simulating complex\nreal-world ITS scenarios, primarily due to the lack of a digital twin\nimplementation framework for characterizing interactions between pedestrians\nand vehicles at different locations in different traffic environments. In this\narticle, we propose a surveillance video assisted federated digital twin\n(SV-FDT) framework to empower ITSs with pedestrians and vehicles in-the-loop.\nSpecifically, SVFDT builds comprehensive pedestrian-vehicle interaction models\nby leveraging multi-source traffic surveillance videos. Its architecture\nconsists of three layers: (i) the end layer, which collects traffic\nsurveillance videos from multiple sources; (ii) the edge layer, responsible for\nsemantic segmentation-based visual understanding, twin agent-based interaction\nmodeling, and local digital twin system (LDTS) creation in local regions; and\n(iii) the cloud layer, which integrates LDTSs across different regions to\nconstruct a global DT model in realtime. We analyze key design requirements and\nchallenges and present core guidelines for SVFDT's system implementation. A\ntestbed evaluation demonstrates its effectiveness in optimizing traffic\nmanagement. Comparisons with traditional terminal-server frameworks highlight\nSV-FDT's advantages in mirroring delays, recognition accuracy, and subjective\nevaluation. Finally, we identify some open challenges and discuss future\nresearch directions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual understanding"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04406v1",
    "title": "Training-Free Graph Filtering via Multimodal Feature Refinement for\n  Extremely Fast Multimodal Recommendation",
    "url": "http://arxiv.org/abs/2503.04406v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04406v1",
    "authors": [
      "Yu-Seung Roh",
      "Joo-Young Kim",
      "Jin-Duk Park",
      "Won-Yong Shin"
    ],
    "date": "2025-03-06",
    "summary": "Multimodal recommender systems improve the performance of canonical\nrecommender systems with no item features by utilizing diverse content types\nsuch as text, images, and videos, while alleviating inherent sparsity of\nuser-item interactions and accelerating user engagement. However, current\nneural network-based models often incur significant computational overhead due\nto the complex training process required to learn and integrate information\nfrom multiple modalities. To overcome this limitation, we propose\nMultiModal-Graph Filtering (MM-GF), a training-free method based on the notion\nof graph filtering (GF) for efficient and accurate multimodal recommendations.\nSpecifically, MM-GF first constructs multiple similarity graphs through\nnontrivial multimodal feature refinement such as robust scaling and vector\nshifting by addressing the heterogeneous characteristics across modalities.\nThen, MM-GF optimally fuses multimodal information using linear low-pass\nfilters across different modalities. Extensive experiments on real-world\nbenchmark datasets demonstrate that MM-GF not only improves recommendation\naccuracy by up to 13.35% compared to the best competitor but also dramatically\nreduces computational costs by achieving the runtime of less than 10 seconds.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04444v1",
    "title": "ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task",
    "url": "http://arxiv.org/abs/2503.04444v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04444v1",
    "authors": [
      "Vittorio Pippi",
      "Matthieu Guillaumin",
      "Silvia Cascianelli",
      "Rita Cucchiara",
      "Maximilian Jaritz",
      "Loris Bazzani"
    ],
    "date": "2025-03-06",
    "summary": "Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04252v1",
    "title": "RCRank: Multimodal Ranking of Root Causes of Slow Queries in Cloud\n  Database Systems",
    "url": "http://arxiv.org/abs/2503.04252v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04252v1",
    "authors": [
      "Biao Ouyang",
      "Yingying Zhang",
      "Hanyin Cheng",
      "Yang Shu",
      "Chenjuan Guo",
      "Bin Yang",
      "Qingsong Wen",
      "Lunting Fan",
      "Christian S. Jensen"
    ],
    "date": "2025-03-06",
    "summary": "With the continued migration of storage to cloud database systems,the impact\nof slow queries in such systems on services and user experience is increasing.\nRoot-cause diagnosis plays an indispensable role in facilitating slow-query\ndetection and revision. This paper proposes a method capable of both\nidentifying possible root cause types for slow queries and ranking these\naccording to their potential for accelerating slow queries. This enables\nprioritizing root causes with the highest impact, in turn improving slow-query\nrevision effectiveness. To enable more accurate and detailed diagnoses, we\npropose the multimodal Ranking for the Root Causes of slow queries (RCRank)\nframework, which formulates root cause analysis as a multimodal machine\nlearning problem and leverages multimodal information from query statements,\nexecution plans, execution logs, and key performance indicators. To obtain\nexpressive embeddings from its heterogeneous multimodal input, RCRank\nintegrates self-supervised pre-training that enhances cross-modal alignment and\ntask relevance. Next, the framework integrates root-cause-adaptive cross\nTransformers that enable adaptive fusion of multimodal features with varying\ncharacteristics. Finally, the framework offers a unified model that features an\nimpact-aware training objective for identifying and ranking root causes. We\nreport on experiments on real and synthetic datasets, finding that RCRank is\ncapable of consistently outperforming the state-of-the-art methods at root\ncause identification and ranking according to a range of metrics.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04280v2",
    "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models",
    "url": "http://arxiv.org/abs/2503.04280v2",
    "pdf_url": "http://arxiv.org/pdf/2503.04280v2",
    "authors": [
      "Niccol\u00f2 Turcato",
      "Matteo Iovino",
      "Aris Synodinos",
      "Alberto Dalla Libera",
      "Ruggero Carli",
      "Pietro Falco"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04065v1",
    "title": "PP-DocBee: Improving Multimodal Document Understanding Through a Bag of\n  Tricks",
    "url": "http://arxiv.org/abs/2503.04065v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04065v1",
    "authors": [
      "Feng Ni",
      "Kui Huang",
      "Yao Lu",
      "Wenyu Lv",
      "Guanzhong Wang",
      "Zeyu Chen",
      "Yi Liu"
    ],
    "date": "2025-03-06",
    "summary": "With the rapid advancement of digitalization, various document images are\nbeing applied more extensively in production and daily life, and there is an\nincreasingly urgent need for fast and accurate parsing of the content in\ndocument images. Therefore, this report presents PP-DocBee, a novel multimodal\nlarge language model designed for end-to-end document image understanding.\nFirst, we develop a data synthesis strategy tailored to document scenarios in\nwhich we build a diverse dataset to improve the model generalization. Then, we\napply a few training techniques, including dynamic proportional sampling, data\npreprocessing, and OCR postprocessing strategies. Extensive evaluations\ndemonstrate the superior performance of PP-DocBee, achieving state-of-the-art\nresults on English document understanding benchmarks and even outperforming\nexisting open source and commercial models in Chinese document understanding.\nThe source code and pre-trained models are publicly available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04250v1",
    "title": "An Egocentric Vision-Language Model based Portable Real-time Smart\n  Assistant",
    "url": "http://arxiv.org/abs/2503.04250v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04250v1",
    "authors": [
      "Yifei Huang",
      "Jilan Xu",
      "Baoqi Pei",
      "Yuping He",
      "Guo Chen",
      "Mingfang Zhang",
      "Lijin Yang",
      "Zheng Nie",
      "Jinyao Liu",
      "Guoshun Fan",
      "Dechen Lin",
      "Fang Fang",
      "Kunpeng Li",
      "Chang Yuan",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Yali Wang",
      "Yu Qiao",
      "Limin Wang"
    ],
    "date": "2025-03-06",
    "summary": "We present Vinci, a vision-language system designed to provide real-time,\ncomprehensive AI assistance on portable devices. At its core, Vinci leverages\nEgoVideo-VL, a novel model that integrates an egocentric vision foundation\nmodel with a large language model (LLM), enabling advanced functionalities such\nas scene understanding, temporal grounding, video summarization, and future\nplanning. To enhance its utility, Vinci incorporates a memory module for\nprocessing long video streams in real time while retaining contextual history,\na generation module for producing visual action demonstrations, and a retrieval\nmodule that bridges egocentric and third-person perspectives to provide\nrelevant how-to videos for skill acquisition. Unlike existing systems that\noften depend on specialized hardware, Vinci is hardware-agnostic, supporting\ndeployment across a wide range of devices, including smartphones and wearable\ncameras. In our experiments, we first demonstrate the superior performance of\nEgoVideo-VL on multiple public benchmarks, showcasing its vision-language\nreasoning and contextual understanding capabilities. We then conduct a series\nof user studies to evaluate the real-world effectiveness of Vinci, highlighting\nits adaptability and usability in diverse scenarios. We hope Vinci can\nestablish a new framework for portable, real-time egocentric AI systems,\nempowering users with contextual and actionable insights. Including the\nfrontend, backend, and models, all codes of Vinci are available at\nhttps://github.com/OpenGVLab/vinci.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04470v1",
    "title": "Gate-Shift-Pose: Enhancing Action Recognition in Sports with Skeleton\n  Information",
    "url": "http://arxiv.org/abs/2503.04470v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04470v1",
    "authors": [
      "Edoardo Bianchi",
      "Oswald Lanz"
    ],
    "date": "2025-03-06",
    "summary": "This paper introduces Gate-Shift-Pose, an enhanced version of Gate-Shift-Fuse\nnetworks, designed for athlete fall classification in figure skating by\nintegrating skeleton pose data alongside RGB frames. We evaluate two fusion\nstrategies: early-fusion, which combines RGB frames with Gaussian heatmaps of\npose keypoints at the input stage, and late-fusion, which employs a\nmulti-stream architecture with attention mechanisms to combine RGB and pose\nfeatures. Experiments on the FR-FS dataset demonstrate that Gate-Shift-Pose\nsignificantly outperforms the RGB-only baseline, improving accuracy by up to\n40% with ResNet18 and 20% with ResNet50. Early-fusion achieves the highest\naccuracy (98.08%) with ResNet50, leveraging the model's capacity for effective\nmultimodal integration, while late-fusion is better suited for lighter\nbackbones like ResNet18. These results highlight the potential of multimodal\narchitectures for sports action recognition and the critical role of skeleton\npose information in capturing complex motion patterns.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04110v1",
    "title": "InterChat: Enhancing Generative Visual Analytics using Multimodal\n  Interactions",
    "url": "http://arxiv.org/abs/2503.04110v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04110v1",
    "authors": [
      "Juntong Chen",
      "Jiang Wu",
      "Jiajing Guo",
      "Vikram Mohanty",
      "Xueming Li",
      "Jorge Piazentin Ono",
      "Wenbin He",
      "Liu Ren",
      "Dongyu Liu"
    ],
    "date": "2025-03-06",
    "summary": "The rise of Large Language Models (LLMs) and generative visual analytics\nsystems has transformed data-driven insights, yet significant challenges\npersist in accurately interpreting users' analytical and interaction intents.\nWhile language inputs offer flexibility, they often lack precision, making the\nexpression of complex intents inefficient, error-prone, and time-intensive. To\naddress these limitations, we investigate the design space of multimodal\ninteractions for generative visual analytics through a literature review and\npilot brainstorming sessions. Building on these insights, we introduce a highly\nextensible workflow that integrates multiple LLM agents for intent inference\nand visualization generation. We develop InterChat, a generative visual\nanalytics system that combines direct manipulation of visual elements with\nnatural language inputs. This integration enables precise intent communication\nand supports progressive, visually driven exploratory data analyses. By\nemploying effective prompt engineering, and contextual interaction linking,\nalongside intuitive visualization and interaction designs, InterChat bridges\nthe gap between user interactions and LLM-driven visualizations, enhancing both\ninterpretability and usability. Extensive evaluations, including two usage\nscenarios, a user study, and expert feedback, demonstrate the effectiveness of\nInterChat. Results show significant improvements in the accuracy and efficiency\nof handling complex visual analytics tasks, highlighting the potential of\nmultimodal interactions to redefine user engagement and analytical depth in\ngenerative visual analytics.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04641v1",
    "title": "Simulating the Real World: A Unified Survey of Multimodal Generative\n  Models",
    "url": "http://arxiv.org/abs/2503.04641v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04641v1",
    "authors": [
      "Yuqi Hu",
      "Longguang Wang",
      "Xian Liu",
      "Ling-Hao Chen",
      "Yuwei Guo",
      "Yukai Shi",
      "Ce Liu",
      "Anyi Rao",
      "Zeyu Wang",
      "Hui Xiong"
    ],
    "date": "2025-03-06",
    "summary": "Understanding and replicating the real world is a critical challenge in\nArtificial General Intelligence (AGI) research. To achieve this, many existing\napproaches, such as world models, aim to capture the fundamental principles\ngoverning the physical world, enabling more accurate simulations and meaningful\ninteractions. However, current methods often treat different modalities,\nincluding 2D (images), videos, 3D, and 4D representations, as independent\ndomains, overlooking their interdependencies. Additionally, these methods\ntypically focus on isolated dimensions of reality without systematically\nintegrating their connections. In this survey, we present a unified survey for\nmultimodal generative models that investigate the progression of data\ndimensionality in real-world simulation. Specifically, this survey starts from\n2D generation (appearance), then moves to video (appearance+dynamics) and 3D\ngeneration (appearance+geometry), and finally culminates in 4D generation that\nintegrate all dimensions. To the best of our knowledge, this is the first\nattempt to systematically unify the study of 2D, video, 3D and 4D generation\nwithin a single framework. To guide future research, we provide a comprehensive\nreview of datasets, evaluation metrics and future directions, and fostering\ninsights for newcomers. This survey serves as a bridge to advance the study of\nmultimodal generative models and real-world simulation within a unified\nframework.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04034v1",
    "title": "GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world\n  Scene Understanding",
    "url": "http://arxiv.org/abs/2503.04034v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04034v1",
    "authors": [
      "Xihan Wang",
      "Dianyi Yang",
      "Yu Gao",
      "Yufeng Yue",
      "Yi Yang",
      "Mengyin Fu"
    ],
    "date": "2025-03-06",
    "summary": "Recent advancements in 3D Gaussian Splatting(3DGS) have significantly\nimproved semantic scene understanding, enabling natural language queries to\nlocalize objects within a scene. However, existing methods primarily focus on\nembedding compressed CLIP features to 3D Gaussians, suffering from low object\nsegmentation accuracy and lack spatial reasoning capabilities. To address these\nlimitations, we propose GaussianGraph, a novel framework that enhances\n3DGS-based scene understanding by integrating adaptive semantic clustering and\nscene graph generation. We introduce a \"Control-Follow\" clustering strategy,\nwhich dynamically adapts to scene scale and feature distribution, avoiding\nfeature compression and significantly improving segmentation accuracy.\nAdditionally, we enrich scene representation by integrating object attributes\nand spatial relations extracted from 2D foundation models. To address\ninaccuracies in spatial relationships, we propose 3D correction modules that\nfilter implausible relations through spatial consistency verification, ensuring\nreliable scene graph construction. Extensive experiments on three datasets\ndemonstrate that GaussianGraph outperforms state-of-the-art methods in both\nsemantic segmentation and object grounding tasks, providing a robust solution\nfor complex scene understanding and interaction.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04478v1",
    "title": "Semantic Alignment of Unimodal Medical Text and Vision Representations",
    "url": "http://arxiv.org/abs/2503.04478v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04478v1",
    "authors": [
      "Maxime Di Folco",
      "Emily Chan",
      "Marta Hasny",
      "Cosmin I. Bercea",
      "Julia A. Schnabel"
    ],
    "date": "2025-03-06",
    "summary": "General-purpose AI models, particularly those designed for text and vision,\ndemonstrate impressive versatility across a wide range of deep-learning tasks.\nHowever, they often underperform in specialised domains like medical imaging,\nwhere domain-specific solutions or alternative knowledge transfer approaches\nare typically required. Recent studies have noted that general-purpose models\ncan exhibit similar latent spaces when processing semantically related data,\nalthough this alignment does not occur naturally. Building on this insight, it\nhas been shown that applying a simple transformation - at most affine -\nestimated from a subset of semantically corresponding samples, known as\nanchors, enables model stitching across diverse training paradigms,\narchitectures, and modalities. In this paper, we explore how semantic alignment\n- estimating transformations between anchors - can bridge general-purpose AI\nwith specialised medical knowledge. Using multiple public chest X-ray datasets,\nwe demonstrate that model stitching across model architectures allows general\nmodels to integrate domain-specific knowledge without additional training,\nleading to improved performance on medical tasks. Furthermore, we introduce a\nnovel zero-shot classification approach for unimodal vision encoders that\nleverages semantic alignment across modalities. Our results show that our\nmethod not only outperforms general multimodal models but also approaches the\nperformance levels of fully trained, medical-specific multimodal solutions",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04643v1",
    "title": "Adaptive Prototype Learning for Multimodal Cancer Survival Analysis",
    "url": "http://arxiv.org/abs/2503.04643v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04643v1",
    "authors": [
      "Hong Liu",
      "Haosen Yang",
      "Federica Eduati",
      "Josien P. W. Pluim",
      "Mitko Veta"
    ],
    "date": "2025-03-06",
    "summary": "Leveraging multimodal data, particularly the integration of whole-slide\nhistology images (WSIs) and transcriptomic profiles, holds great promise for\nimproving cancer survival prediction. However, excessive redundancy in\nmultimodal data can degrade model performance. In this paper, we propose\nAdaptive Prototype Learning (APL), a novel and effective approach for\nmultimodal cancer survival analysis. APL adaptively learns representative\nprototypes in a data-driven manner, reducing redundancy while preserving\ncritical information. Our method employs two sets of learnable query vectors\nthat serve as a bridge between high-dimensional representations and survival\nprediction, capturing task-relevant features. Additionally, we introduce a\nmultimodal mixed self-attention mechanism to enable cross-modal interactions,\nfurther enhancing information fusion. Extensive experiments on five benchmark\ncancer datasets demonstrate the superiority of our approach over existing\nmethods. The code is available at https://github.com/HongLiuuuuu/APL.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.32,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.989041095890411,
      "source_weight": 1.0,
      "age_months": 0.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03663v2",
    "title": "LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant",
    "url": "http://arxiv.org/abs/2503.03663v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03663v2",
    "authors": [
      "Wei Li",
      "Bing Hu",
      "Rui Shao",
      "Leyang Shen",
      "Liqiang Nie"
    ],
    "date": "2025-03-05",
    "summary": "First-person video assistants are highly anticipated to enhance our daily\nlives through online video dialogue. However, existing online video assistants\noften sacrifice assistant efficacy for real-time efficiency by processing\nlow-frame-rate videos with coarse-grained visual features.To overcome the\ntrade-off between efficacy and efficiency, we propose \"Fast & Slow\nVideo-Language Thinker\" as an onLIne videO assistaNt, LION-FS, achieving\nreal-time, proactive, temporally accurate, and contextually precise responses.\nLION-FS adopts a two-stage optimization strategy: 1)Fast Path: Routing-Based\nResponse Determination evaluates frame-by-frame whether an immediate response\nis necessary. To enhance response determination accuracy and handle higher\nframe-rate inputs efficiently, we employ Token Aggregation Routing to\ndynamically fuse spatiotemporal features without increasing token numbers,\nwhile utilizing Token Dropping Routing to eliminate redundant features. 2)Slow\nPath: Multi-granularity Keyframe Augmentation optimizes keyframes during\nresponse generation. To provide comprehensive and detailed responses beyond\natomic actions constrained by training data, fine-grained spatial features and\nhuman-environment interaction features are extracted through multi-granular\npooling. These features are further integrated into a meticulously designed\nmultimodal Thinking Template to guide more precise response generation.\nComprehensive evaluations on online video tasks demonstrate that LION-FS\nachieves state-of-the-art efficacy and efficiency.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03579v1",
    "title": "A Generative System for Robot-to-Human Handovers: from Intent Inference\n  to Spatial Configuration Imagery",
    "url": "http://arxiv.org/abs/2503.03579v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03579v1",
    "authors": [
      "Hanxin Zhang",
      "Abdulqader Dhafer",
      "Zhou Daniel Hao",
      "Hongbiao Dong"
    ],
    "date": "2025-03-05",
    "summary": "We propose a novel system for robot-to-human object handover that emulates\nhuman coworker interactions. Unlike most existing studies that focus primarily\non grasping strategies and motion planning, our system focus on 1. inferring\nhuman handover intents, 2. imagining spatial handover configuration. The first\none integrates multimodal perception-combining visual and verbal cues-to infer\nhuman intent. The second one using a diffusion-based model to generate the\nhandover configuration, involving the spacial relationship among robot's\ngripper, the object, and the human hand, thereby mimicking the cognitive\nprocess of motor imagery. Experimental results demonstrate that our approach\neffectively interprets human cues and achieves fluent, human-like handovers,\noffering a promising solution for collaborative robotics. Code, videos, and\ndata are available at: https://i3handover.github.io.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03689v1",
    "title": "DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with\n  Reward Guidance",
    "url": "http://arxiv.org/abs/2503.03689v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03689v1",
    "authors": [
      "Zhao Yang",
      "Zezhong Qian",
      "Xiaofan Li",
      "Weixiang Xu",
      "Gongpeng Zhao",
      "Ruohong Yu",
      "Lingsi Zhu",
      "Longjun Liu"
    ],
    "date": "2025-03-05",
    "summary": "Accurate and high-fidelity driving scene reconstruction demands the effective\nutilization of comprehensive scene information as conditional inputs. Existing\nmethods predominantly rely on 3D bounding boxes and BEV road maps for\nforeground and background control, which fail to capture the full complexity of\ndriving scenes and adequately integrate multimodal information. In this work,\nwe present DualDiff, a dual-branch conditional diffusion model designed to\nenhance driving scene generation across multiple views and video sequences.\nSpecifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional\ninput, offering rich foreground and background semantics alongside 3D spatial\ngeometry to precisely control the generation of both elements. To improve the\nsynthesis of fine-grained foreground objects, particularly complex and distant\nones, we propose a Foreground-Aware Mask (FGM) denoising loss function.\nAdditionally, we develop the Semantic Fusion Attention (SFA) mechanism to\ndynamically prioritize relevant information and suppress noise, enabling more\neffective multimodal fusion. Finally, to ensure high-quality image-to-video\ngeneration, we introduce the Reward-Guided Diffusion (RGD) framework, which\nmaintains global consistency and semantic coherence in generated videos.\nExtensive experiments demonstrate that DualDiff achieves state-of-the-art\n(SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff\nreduces the FID score by 4.09% compared to the best baseline. In downstream\ntasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and\nroad mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP\nincreases by 1.46%. Code will be made available at\nhttps://github.com/yangzhaojason/DualDiff.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03947v1",
    "title": "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.03947v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03947v1",
    "authors": [
      "Aurelio Noca",
      "Xianmei Lei",
      "Jonathan Becktor",
      "Jeffrey Edlund",
      "Anna Sabel",
      "Patrick Spieler",
      "Curtis Padgett",
      "Alexandre Alahi",
      "Deegan Atha"
    ],
    "date": "2025-03-05",
    "summary": "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03743v1",
    "title": "CHOP: Mobile Operating Assistant with Constrained High-frequency\n  Optimized Subtask Planning",
    "url": "http://arxiv.org/abs/2503.03743v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03743v1",
    "authors": [
      "Yuqi Zhou",
      "Shuai Wang",
      "Sunhao Dai",
      "Qinglin Jia",
      "Zhaocheng Du",
      "Zhenhua Dong",
      "Jun Xu"
    ],
    "date": "2025-03-05",
    "summary": "The advancement of visual language models (VLMs) has enhanced mobile device\noperations, allowing simulated human-like actions to address user requirements.\nCurrent VLM-based mobile operating assistants can be structured into three\nlevels: task, subtask, and action. The subtask level, linking high-level goals\nwith low-level executable actions, is crucial for task completion but faces two\nchallenges: ineffective subtasks that lower-level agent cannot execute and\ninefficient subtasks that fail to contribute to the completion of the\nhigher-level task. These challenges stem from VLM's lack of experience in\ndecomposing subtasks within GUI scenarios in multi-agent architecture. To\naddress these, we propose a new mobile assistant architecture with constrained\nhigh-frequency o}ptimized planning (CHOP). Our approach overcomes the VLM's\ndeficiency in GUI scenarios planning by using human-planned subtasks as the\nbasis vector. We evaluate our architecture in both English and Chinese contexts\nacross 20 Apps, demonstrating significant improvements in both effectiveness\nand efficiency. Our dataset and code is available at\nhttps://github.com/Yuqi-Zhou/CHOP",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03848v1",
    "title": "Nexar Dashcam Collision Prediction Dataset and Challenge",
    "url": "http://arxiv.org/abs/2503.03848v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03848v1",
    "authors": [
      "Daniel C. Moura",
      "Shizhan Zhu",
      "Orly Zvitia"
    ],
    "date": "2025-03-05",
    "summary": "This paper presents the Nexar Dashcam Collision Prediction Dataset and\nChallenge, designed to support research in traffic event analysis, collision\nprediction, and autonomous vehicle safety. The dataset consists of 1,500\nannotated video clips, each approximately 40 seconds long, capturing a diverse\nrange of real-world traffic scenarios. Videos are labeled with event type\n(collision/near-collision vs. normal driving), environmental conditions\n(lighting conditions and weather), and scene type (urban, rural, highway,\netc.). For collision and near-collision cases, additional temporal labels are\nprovided, including the precise moment of the event and the alert time, marking\nwhen the collision first becomes predictable.\n  To advance research on accident prediction, we introduce the Nexar Dashcam\nCollision Prediction Challenge, a public competition on top of this dataset.\nParticipants are tasked with developing machine learning models that predict\nthe likelihood of an imminent collision, given an input video. Model\nperformance is evaluated using the average precision (AP) computed across\nmultiple intervals before the accident (i.e. 500 ms, 1000 ms, and 1500 ms prior\nto the event), emphasizing the importance of early and reliable predictions.\n  The dataset is released under an open license with restrictions on unethical\nuse, ensuring responsible research and innovation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.04842v1",
    "title": "Replicating Human Social Perception in Generative AI: Evaluating the\n  Valence-Dominance Model",
    "url": "http://arxiv.org/abs/2503.04842v1",
    "pdf_url": "http://arxiv.org/pdf/2503.04842v1",
    "authors": [
      "Necdet Gurkan",
      "Kimathi Njoki",
      "Jordan W. Suchow"
    ],
    "date": "2025-03-05",
    "summary": "As artificial intelligence (AI) continues to advance--particularly in\ngenerative models--an open question is whether these systems can replicate\nfoundational models of human social perception. A well-established framework in\nsocial cognition suggests that social judgments are organized along two primary\ndimensions: valence (e.g., trustworthiness, warmth) and dominance (e.g., power,\nassertiveness). This study examines whether multimodal generative AI systems\ncan reproduce this valence-dominance structure when evaluating facial images\nand how their representations align with those observed across world regions.\nThrough principal component analysis (PCA), we found that the extracted\ndimensions closely mirrored the theoretical structure of valence and dominance,\nwith trait loadings aligning with established definitions. However, many world\nregions and generative AI models also exhibited a third component, the nature\nand significance of which warrant further investigation. These findings\ndemonstrate that multimodal generative AI systems can replicate key aspects of\nhuman social perception, raising important questions about their implications\nfor AI-driven decision-making and human-AI interactions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503.03644v2",
    "title": "DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating\n  Semantic Understanding of Dongba Pictograms",
    "url": "http://arxiv.org/abs/2503.03644v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03644v2",
    "authors": [
      "Xiaojun Bi",
      "Shuo Li",
      "Ziyue Wang",
      "Fuwen Luo",
      "Weizheng Qiao",
      "Lu Han",
      "Ziwei Sun",
      "Peng Li",
      "Yang Liu"
    ],
    "date": "2025-03-05",
    "summary": "Dongba pictographs are the only pictographs still in use in the world. They\nhave pictorial ideographic features, and their symbols carry rich cultural and\ncontextual information. Due to the lack of relevant datasets, existing research\nhas difficulty in advancing the study of semantic understanding of Dongba\npictographs. To this end, we propose DongbaMIE, the first multimodal dataset\nfor semantic understanding and extraction of Dongba pictographs. The dataset\nconsists of Dongba pictograph images and their corresponding Chinese semantic\nannotations. It contains 23,530 sentence-level and 2,539 paragraph-level\nimages, covering four semantic dimensions: objects, actions, relations, and\nattributes. We systematically evaluate the GPT-4o, Gemini-2.0, and Qwen2-VL\nmodels. Experimental results show that the F1 scores of GPT-4o and Gemini in\nthe best object extraction are only 3.16 and 3.11 respectively. The F1 score of\nQwen2-VL after supervised fine-tuning is only 11.49. These results suggest that\ncurrent large multimodal models still face significant challenges in accurately\nrecognizing the diverse semantic information in Dongba pictographs. The dataset\ncan be obtained from this URL.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 2.3,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.9863013698630136,
      "source_weight": 1.0,
      "age_months": 0.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_MME_Survey__A_Comprehensive_Survey_on_Evaluation_o",
    "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
    "url": "https://paperswithcode.com/paper/mme-survey-a-comprehensive-survey-on",
    "authors": [],
    "date": "2024-03-11",
    "summary": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "multimodal LLM",
      "MLLM",
      "MLLMs"
    ],
    "code_url": null,
    "attention_score": 0.22,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05916v2",
    "title": "GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual\n  Affective Computing",
    "url": "http://arxiv.org/abs/2403.05916v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05916v2",
    "authors": [
      "Hao Lu",
      "Xuesong Niu",
      "Jiyao Wang",
      "Yin Wang",
      "Qingyong Hu",
      "Jiaqi Tang",
      "Yuting Zhang",
      "Kaishen Yuan",
      "Bin Huang",
      "Zitong Yu",
      "Dengbo He",
      "Shuiguang Deng",
      "Hao Chen",
      "Yingcong Chen",
      "Shiguang Shan"
    ],
    "date": "2024-03-09",
    "summary": "Multimodal large language models (MLLMs) are designed to process and\nintegrate information from multiple sources, such as text, speech, images, and\nvideos. Despite its success in language understanding, it is critical to\nevaluate the performance of downstream tasks for better human-centric\napplications. This paper assesses the application of MLLMs with 5 crucial\nabilities for affective computing, spanning from visual affective tasks and\nreasoning tasks. The results show that \\gpt has high accuracy in facial action\nunit recognition and micro-expression detection while its general facial\nexpression recognition performance is not accurate. We also highlight the\nchallenges of achieving fine-grained micro-expression recognition and the\npotential for further study and demonstrate the versatility and potential of\n\\gpt for handling advanced tasks in emotion recognition and related fields by\nintegrating with task-related agents for more complex tasks, such as heart rate\nestimation through signal processing. In conclusion, this paper provides\nvaluable insights into the potential applications and challenges of MLLMs in\nhuman-centric computing. Our interesting examples are at\nhttps://github.com/EnVision-Research/GPT4Affectivity.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "GPT-4V",
      "MLLM",
      "ViT",
      "MLLMs"
    ],
    "attention_score": 0.2,
    "attention_components": {
      "base_score": 2.0,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05381v1",
    "title": "Exploring Robust Features for Few-Shot Object Detection in Satellite\n  Imagery",
    "url": "http://arxiv.org/abs/2403.05381v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05381v1",
    "authors": [
      "Xavier Bou",
      "Gabriele Facciolo",
      "Rafael Grompone von Gioi",
      "Jean-Michel Morel",
      "Thibaud Ehret"
    ],
    "date": "2024-03-08",
    "summary": "The goal of this paper is to perform object detection in satellite imagery\nwith only a few examples, thus enabling users to specify any object class with\nminimal annotation. To this end, we explore recent methods and ideas from\nopen-vocabulary detection for the remote sensing domain. We develop a few-shot\nobject detector based on a traditional two-stage architecture, where the\nclassification block is replaced by a prototype-based classifier. A large-scale\npre-trained model is used to build class-reference embeddings or prototypes,\nwhich are compared to region proposal contents for label prediction. In\naddition, we propose to fine-tune prototypes on available training images to\nboost performance and learn differences between similar classes, such as\naircraft types. We perform extensive evaluations on two remote sensing datasets\ncontaining challenging and rare objects. Moreover, we study the performance of\nboth visual and image-text features, namely DINOv2 and CLIP, including two CLIP\nmodels specifically tailored for remote sensing applications. Results indicate\nthat visual features are largely superior to vision-language models, as the\nlatter lack the necessary domain-specific vocabulary. Lastly, the developed\ndetector outperforms fully supervised and few-shot methods evaluated on the\nSIMD and DIOR datasets, despite minimal training parameters.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "image-text",
      "CLIP",
      "DINO",
      "DINOv2"
    ],
    "attention_score": 0.2,
    "attention_components": {
      "base_score": 2.0,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04640v1",
    "title": "CAT: Enhancing Multimodal Large Language Model to Answer Questions in\n  Dynamic Audio-Visual Scenarios",
    "url": "http://arxiv.org/abs/2403.04640v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04640v1",
    "authors": [
      "Qilang Ye",
      "Zitong Yu",
      "Rui Shao",
      "Xinyu Xie",
      "Philip Torr",
      "Xiaochun Cao"
    ],
    "date": "2024-03-07",
    "summary": "This paper focuses on the challenge of answering questions in scenarios that\nare composed of rich and complex dynamic audio-visual components. Although\nexisting Multimodal Large Language Models (MLLMs) can respond to audio-visual\ncontent, these responses are sometimes ambiguous and fail to describe specific\naudio-visual events. To overcome this limitation, we introduce the CAT, which\nenhances MLLM in three ways: 1) besides straightforwardly bridging audio and\nvideo, we design a clue aggregator that aggregates question-related clues in\ndynamic audio-visual scenarios to enrich the detailed knowledge required for\nlarge language models. 2) CAT is trained on a mixed multimodal dataset,\nallowing direct application in audio-visual scenarios. Notably, we collect an\naudio-visual joint instruction dataset named AVinstruct, to further enhance the\ncapacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted\nambiguity-aware direct preference optimization, a strategy specialized in\nretraining the model to favor the non-ambiguity response and improve the\nability to localize specific audio-visual objects. Extensive experimental\nresults demonstrate that CAT outperforms existing methods on multimodal tasks,\nespecially in Audio-Visual Question Answering (AVQA) tasks. The codes and the\ncollected instructions are released at https://github.com/rikeilong/Bay-CAT.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 0.2,
    "attention_components": {
      "base_score": 2.0,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_BLIP__Bootstrapping_Language_Image_Pre_training_fo",
    "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
    "url": "https://paperswithcode.com/paper/blip-bootstrapping-language-image-pre",
    "authors": [],
    "date": "2024-03-26",
    "summary": "Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language",
      "image-text",
      "BLIP"
    ],
    "code_url": null,
    "attention_score": 0.19,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.6,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_G_LLaVA__Solving_Geometric_Problem_with_Multi_Moda",
    "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
    "url": "https://paperswithcode.com/paper/g-llava-solving-geometric-problem-with-multi",
    "authors": [],
    "date": "2024-03-19",
    "summary": "We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ],
    "code_url": null,
    "attention_score": 0.19,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.9,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_Kosmos_G__Generating_Images_in_Context_with_Multim",
    "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
    "url": "https://paperswithcode.com/paper/kosmos-g-generating-images-in-context-with",
    "authors": [],
    "date": "2024-03-13",
    "summary": "These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\"",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "image generation",
      "Kosmos"
    ],
    "code_url": null,
    "attention_score": 0.19,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_A_Survey_on_Multimodal_Large_Language_Models",
    "title": "A Survey on Multimodal Large Language Models",
    "url": "https://paperswithcode.com/paper/a-survey-on-multimodal-large-language-models",
    "authors": [],
    "date": "2024-03-10",
    "summary": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "GPT-4V",
      "MLLM"
    ],
    "code_url": null,
    "attention_score": 0.19,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_MiniGPT_v2__large_language_model_as_a_unified_inte",
    "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning",
    "url": "https://paperswithcode.com/paper/minigpt-v2-large-language-model-as-a-unified",
    "authors": [],
    "date": "2024-03-04",
    "summary": "Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language",
      "visual question answering",
      "visual grounding"
    ],
    "code_url": null,
    "attention_score": 0.19,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06295v1",
    "title": "A streamlined Approach to Multimodal Few-Shot Class Incremental Learning\n  for Fine-Grained Datasets",
    "url": "http://arxiv.org/abs/2403.06295v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06295v1",
    "authors": [
      "Thang Doan",
      "Sima Behpour",
      "Xin Li",
      "Wenbin He",
      "Liang Gou",
      "Liu Ren"
    ],
    "date": "2024-03-10",
    "summary": "Few-shot Class-Incremental Learning (FSCIL) poses the challenge of retaining\nprior knowledge while learning from limited new data streams, all without\noverfitting. The rise of Vision-Language models (VLMs) has unlocked numerous\napplications, leveraging their existing knowledge to fine-tune on custom data.\nHowever, training the whole model is computationally prohibitive, and VLMs\nwhile being versatile in general domains still struggle with fine-grained\ndatasets crucial for many applications. We tackle these challenges with two\nproposed simple modules. The first, Session-Specific Prompts (SSP), enhances\nthe separability of image-text embeddings across sessions. The second,\nHyperbolic distance, compresses representations of image-text pairs within the\nsame class while expanding those from different classes, leading to better\nrepresentations. Experimental results demonstrate an average 10-point increase\ncompared to baselines while requiring at least 8 times fewer trainable\nparameters. This improvement is further underscored on our three newly\nintroduced fine-grained datasets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language",
      "image-text"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06199v4",
    "title": "Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models",
    "url": "http://arxiv.org/abs/2403.06199v4",
    "pdf_url": "http://arxiv.org/pdf/2403.06199v4",
    "authors": [
      "Minjie Zhu",
      "Yichen Zhu",
      "Xin Liu",
      "Ning Liu",
      "Zhiyuan Xu",
      "Chaomin Shen",
      "Yaxin Peng",
      "Zhicai Ou",
      "Feifei Feng",
      "Jian Tang"
    ],
    "date": "2024-03-10",
    "summary": "Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual understanding",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05131v2",
    "title": "Sora as an AGI World Model? A Complete Survey on Text-to-Video\n  Generation",
    "url": "http://arxiv.org/abs/2403.05131v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05131v2",
    "authors": [
      "Joseph Cho",
      "Fachrina Dewi Puspitasari",
      "Sheng Zheng",
      "Jingyao Zheng",
      "Lik-Hang Lee",
      "Tae-Ho Kim",
      "Choong Seon Hong",
      "Chaoning Zhang"
    ],
    "date": "2024-03-08",
    "summary": "The evolution of video generation from text, starting with animating MNIST\nnumbers to simulating the physical world with Sora, has progressed at a\nbreakneck speed over the past seven years. While often seen as a superficial\nexpansion of the predecessor text-to-image generation model, text-to-video\ngeneration models are developed upon carefully engineered constituents. Here,\nwe systematically discuss these elements consisting of but not limited to core\nbuilding blocks (vision, language, and temporal) and supporting features from\nthe perspective of their contributions to achieving a world model. We employ\nthe PRISMA framework to curate 97 impactful research articles from renowned\nscientific databases primarily studying video synthesis using text conditions.\nUpon minute exploration of these manuscripts, we observe that text-to-video\ngeneration involves more intricate technologies beyond the plain extension of\ntext-to-image generation. Our additional review into the shortcomings of\nSora-generated videos pinpoints the call for more in-depth studies in various\nenabling aspects of video generation such as dataset, evaluation metric,\nefficient architecture, and human-controlled generation. Finally, we conclude\nthat the study of the text-to-video generation may still be in its infancy,\nrequiring contribution from the cross-discipline research community towards its\nadvancement as the first step to realize artificial general intelligence (AGI).",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "text-to-video",
      "Sora",
      "image generation"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04735v1",
    "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with\n  Retrieval Augmented Multimodal LLM",
    "url": "http://arxiv.org/abs/2403.04735v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04735v1",
    "authors": [
      "Jielin Qiu",
      "Andrea Madotto",
      "Zhaojiang Lin",
      "Paul A. Crook",
      "Yifan Ethan Xu",
      "Xin Luna Dong",
      "Christos Faloutsos",
      "Lei Li",
      "Babak Damavandi",
      "Seungwhan Moon"
    ],
    "date": "2024-03-07",
    "summary": "Vision-extended LLMs have made significant strides in Visual Question\nAnswering (VQA). Despite these advancements, VLLMs still encounter substantial\ndifficulties in handling queries involving long-tail entities, with a tendency\nto produce erroneous or hallucinated responses. In this work, we introduce a\nnovel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for\nentity-centric VQA. This task aims to test the models' capabilities in\nidentifying entities and providing detailed, entity-specific knowledge. We have\ndeveloped the \\textbf{SnapNTell Dataset}, distinct from traditional VQA\ndatasets: (1) It encompasses a wide range of categorized entities, each\nrepresented by images and explicitly named in the answers; (2) It features QA\npairs that require extensive knowledge for accurate responses. The dataset is\norganized into 22 major categories, containing 7,568 unique entities in total.\nFor each entity, we curated 10 illustrative images and crafted 10\nknowledge-intensive QA pairs. To address this novel task, we devised a\nscalable, efficient, and transparent retrieval-augmented multimodal LLM. Our\napproach markedly outperforms existing methods on the SnapNTell dataset,\nachieving a 66.5\\% improvement in the BELURT score. We will soon make the\ndataset and the source code publicly accessible.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA",
      "multimodal LLM"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04306v5",
    "title": "Effectiveness Assessment of Recent Large Vision-Language Models",
    "url": "http://arxiv.org/abs/2403.04306v5",
    "pdf_url": "http://arxiv.org/pdf/2403.04306v5",
    "authors": [
      "Yao Jiang",
      "Xinyu Yan",
      "Ge-Peng Ji",
      "Keren Fu",
      "Meijun Sun",
      "Huan Xiong",
      "Deng-Ping Fan",
      "Fahad Shahbaz Khan"
    ],
    "date": "2024-03-07",
    "summary": "The advent of large vision-language models (LVLMs) represents a remarkable\nadvance in the quest for artificial general intelligence. However, the model's\neffectiveness in both specialized and general tasks warrants further\ninvestigation. This paper endeavors to evaluate the competency of popular LVLMs\nin specialized and general tasks, respectively, aiming to offer a comprehensive\nunderstanding of these novel models. To gauge their effectiveness in\nspecialized tasks, we employ six challenging tasks in three different\napplication scenarios: natural, healthcare, and industrial. These six tasks\ninclude salient/camouflaged/transparent object detection, as well as polyp\ndetection, skin lesion detection, and industrial anomaly detection. We examine\nthe performance of three recent open-source LVLMs, including MiniGPT-v2,\nLLaVA-1.5, and Shikra, on both visual recognition and localization in these\ntasks. Moreover, we conduct empirical investigations utilizing the\naforementioned LVLMs together with GPT-4V, assessing their multi-modal\nunderstanding capabilities in general tasks including object counting, absurd\nquestion answering, affordance reasoning, attribute recognition, and spatial\nrelation reasoning. Our investigations reveal that these LVLMs demonstrate\nlimited proficiency not only in specialized tasks but also in general tasks. We\ndelve deep into this inadequacy and uncover several potential factors,\nincluding limited cognition in specialized tasks, object hallucination,\ntext-to-image interference, and decreased robustness in complex problems. We\nhope that this study can provide useful insights for the future development of\nLVLMs, helping researchers improve LVLMs for both general and specialized\napplications.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "GPT-4V",
      "text-to-image"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02865v2",
    "title": "FairSense-AI: Responsible AI Meets Sustainability",
    "url": "http://arxiv.org/abs/2503.02865v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02865v2",
    "authors": [
      "Shaina Raza",
      "Mukund Sayeeganesh Chettiar",
      "Matin Yousefabadi",
      "Tahniat Khan",
      "Marcelo Lotif"
    ],
    "date": "2024-03-04",
    "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "multimodal",
      "vision-language"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02476v1",
    "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA",
    "url": "http://arxiv.org/abs/2503.02476v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02476v1",
    "authors": [
      "Zhengyang Ji",
      "Shang Gao",
      "Li Liu",
      "Yifan Jia",
      "Yutao Yue"
    ],
    "date": "2024-03-04",
    "summary": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image-text",
      "visual question answering",
      "VQA"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02358v1",
    "title": "Are Large Vision Language Models Good Game Players?",
    "url": "http://arxiv.org/abs/2503.02358v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02358v1",
    "authors": [
      "Xinyu Wang",
      "Bohan Zhuang",
      "Qi Wu"
    ],
    "date": "2024-03-04",
    "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable abilities\nin understanding and reasoning about both visual and textual information.\nHowever, existing evaluation methods for LVLMs, primarily based on benchmarks\nlike Visual Question Answering and image captioning, often fail to capture the\nfull scope of LVLMs' capabilities. These benchmarks are limited by issues such\nas inadequate assessment of detailed visual perception, data contamination, and\na lack of focus on multi-turn reasoning. To address these challenges, we\npropose \\method{}, a game-based evaluation framework designed to provide a\ncomprehensive assessment of LVLMs' cognitive and reasoning skills in structured\nenvironments. \\method{} uses a set of games to evaluate LVLMs on four core\ntasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,\nwith each target task designed to assess specific abilities, including visual\nperception, reasoning, decision-making, etc. Based on this framework, we\nconduct extensive experiments that explore the limitations of current LVLMs,\nsuch as handling long structured outputs and perceiving detailed and dense\nelements. Code and data are publicly available at\nhttps://github.com/xinke-wang/LVLM-Playground.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM",
      "image captioning",
      "visual question answering"
    ],
    "attention_score": 0.18,
    "attention_components": {
      "base_score": 1.8,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_ConceptLab__Creative_Concept_Generation_using_VLM_",
    "title": "ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints",
    "url": "https://paperswithcode.com/paper/conceptlab-creative-generation-using",
    "authors": [],
    "date": "2024-03-18",
    "summary": "Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery.",
    "source": "Papers With Code",
    "keywords": [
      "VLM",
      "text-to-image"
    ],
    "code_url": null,
    "attention_score": 0.17,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.9,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_X__2__VLM__All_In_One_Pre_trained_Model_For_Vision",
    "title": "X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
    "url": "https://paperswithcode.com/paper/x-2-vlm-all-in-one-pre-trained-model-for",
    "authors": [],
    "date": "2024-03-17",
    "summary": "Vision language pre-training aims to learn alignments between vision and language from a large amount of data.",
    "source": "Papers With Code",
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "code_url": null,
    "attention_score": 0.17,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.9,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_PaliGemma__A_versatile_3B_VLM_for_transfer",
    "title": "PaliGemma: A versatile 3B VLM for transfer",
    "url": "https://paperswithcode.com/paper/paligemma-a-versatile-3b-vlm-for-transfer",
    "authors": [],
    "date": "2024-03-16",
    "summary": "PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model.",
    "source": "Papers With Code",
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "code_url": null,
    "attention_score": 0.17,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.0,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_MME__A_Comprehensive_Evaluation_Benchmark_for_Mult",
    "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
    "url": "https://paperswithcode.com/paper/mme-a-comprehensive-evaluation-benchmark-for",
    "authors": [],
    "date": "2024-03-14",
    "summary": "Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "MLLM"
    ],
    "code_url": null,
    "attention_score": 0.17,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.0,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_VATT__Transformers_for_Multimodal_Self_Supervised_",
    "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
    "url": "https://paperswithcode.com/paper/vatt-transformers-for-multimodal-self",
    "authors": [],
    "date": "2024-03-12",
    "summary": "We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal",
      "text-to-video"
    ],
    "code_url": null,
    "attention_score": 0.17,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.1,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06269v2",
    "title": "FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video\n  Editing",
    "url": "http://arxiv.org/abs/2403.06269v2",
    "pdf_url": "http://arxiv.org/pdf/2403.06269v2",
    "authors": [
      "Youyuan Zhang",
      "Xuan Ju",
      "James J. Clark"
    ],
    "date": "2024-03-10",
    "summary": "Diffusion models have demonstrated remarkable capabilities in text-to-image\nand text-to-video generation, opening up possibilities for video editing based\non textual input. However, the computational cost associated with sequential\nsampling in diffusion models poses challenges for efficient video editing.\nExisting approaches relying on image generation models for video editing suffer\nfrom time-consuming one-shot fine-tuning, additional condition extraction, or\nDDIM inversion, making real-time applications impractical. In this work, we\npropose FastVideoEdit, an efficient zero-shot video editing approach inspired\nby Consistency Models (CMs). By leveraging the self-consistency property of\nCMs, we eliminate the need for time-consuming inversion or additional condition\nextraction, reducing editing time. Our method enables direct mapping from\nsource video to target video with strong preservation ability utilizing a\nspecial variance schedule. This results in improved speed advantages, as fewer\nsampling steps can be used while maintaining comparable generation quality.\nExperimental results validate the state-of-the-art performance and speed\nadvantages of FastVideoEdit across evaluation metrics encompassing editing\nspeed, temporal consistency, and text-video alignment.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "text-to-video",
      "image generation"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.07944v1",
    "title": "WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text\n  and Image Inputs",
    "url": "http://arxiv.org/abs/2403.07944v1",
    "pdf_url": "http://arxiv.org/pdf/2403.07944v1",
    "authors": [
      "Deshun Yang",
      "Luhui Hu",
      "Yu Tian",
      "Zihao Li",
      "Chris Kelly",
      "Bang Yang",
      "Cindy Yang",
      "Yuexian Zou"
    ],
    "date": "2024-03-10",
    "summary": "Several text-to-video diffusion models have demonstrated commendable\ncapabilities in synthesizing high-quality video content. However, it remains a\nformidable challenge pertaining to maintaining temporal consistency and\nensuring action smoothness throughout the generated sequences. In this paper,\nwe present an innovative video generation AI agent that harnesses the power of\nSora-inspired multimodal learning to build skilled world models framework based\non textual prompts and accompanying images. The framework includes two parts:\nprompt enhancer and full video translation. The first part employs the\ncapabilities of ChatGPT to meticulously distill and proactively construct\nprecise prompts for each subsequent step, thereby guaranteeing the utmost\naccuracy in prompt communication and accurate execution in following model\noperations. The second part employ compatible with existing advanced diffusion\ntechniques to expansively generate and refine the key frame at the conclusion\nof a video. Then we can expertly harness the power of leading and trailing key\nframes to craft videos with enhanced temporal consistency and action\nsmoothness. The experimental results confirm that our method has strong\neffectiveness and novelty in constructing world models from text and image\ninputs over the other methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "text-to-video",
      "Sora"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06098v4",
    "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video\n  Diffusion Models",
    "url": "http://arxiv.org/abs/2403.06098v4",
    "pdf_url": "http://arxiv.org/pdf/2403.06098v4",
    "authors": [
      "Wenhao Wang",
      "Yi Yang"
    ],
    "date": "2024-03-10",
    "summary": "The arrival of Sora marks a new era for text-to-video diffusion models,\nbringing significant advancements in video generation and potential\napplications. However, Sora, along with other text-to-video diffusion models,\nis highly reliant on prompts, and there is no publicly available dataset that\nfeatures a study of text-to-video prompts. In this paper, we introduce VidProM,\nthe first large-scale dataset comprising 1.67 Million unique text-to-Video\nPrompts from real users. Additionally, this dataset includes 6.69 million\nvideos generated by four state-of-the-art diffusion models, alongside some\nrelated data. We initially discuss the curation of this large-scale dataset, a\nprocess that is both time-consuming and costly. Subsequently, we underscore the\nneed for a new prompt dataset specifically designed for text-to-video\ngeneration by illustrating how VidProM differs from DiffusionDB, a large-scale\nprompt-gallery dataset for image generation. Our extensive and diverse dataset\nalso opens up many exciting new research areas. For instance, we suggest\nexploring text-to-video prompt engineering, efficient video generation, and\nvideo copy detection for diffusion models to develop better, more efficient,\nand safer models. The project (including the collected dataset VidProM and\nrelated code) is publicly available at https://vidprom.github.io under the\nCC-BY-NC 4.0 License.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-video",
      "Sora",
      "image generation"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06059v1",
    "title": "Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning",
    "url": "http://arxiv.org/abs/2403.06059v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06059v1",
    "authors": [
      "Yi Zhang",
      "Ce Zhang"
    ],
    "date": "2024-03-10",
    "summary": "Vision-Language Pre-Trained (VLP) models, such as CLIP, have demonstrated\nremarkable effectiveness in learning generic visual representations. Several\napproaches aim to efficiently adapt VLP models to downstream tasks with limited\nsupervision, aiming to leverage the acquired knowledge from VLP models.\nHowever, these methods suffer from either introducing biased representations or\nrequiring high computational complexity, which hinders their effectiveness in\nfine-tuning the CLIP model. Moreover, when a model is trained on data specific\nto a particular domain, its ability to generalize to uncharted domains\ndiminishes. In this work, we propose Test-Time Distribution LearNing Adapter\n(TT-DNA) which directly works during the testing period. Specifically, we\nestimate Gaussian distributions to model visual features of the few-shot\nsupport images to capture the knowledge from the support set. The cosine\nsimilarity between query image and the feature distribution of support images\nis used as the prediction of visual adapter. Subsequently, the visual adapter's\nprediction merges with the original CLIP prediction via a residual connection,\nresulting in the final prediction. Our extensive experimental results on visual\nreasoning for human object interaction demonstrate that our proposed TT-DNA\noutperforms existing state-of-the-art methods by large margins.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP",
      "visual reasoning"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05262v2",
    "title": "Debiasing Multimodal Large Language Models",
    "url": "http://arxiv.org/abs/2403.05262v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05262v2",
    "authors": [
      "Yi-Fan Zhang",
      "Weichen Yu",
      "Qingsong Wen",
      "Xue Wang",
      "Zhang Zhang",
      "Liang Wang",
      "Rong Jin",
      "Tieniu Tan"
    ],
    "date": "2024-03-08",
    "summary": "In the realms of computer vision and natural language processing, Large\nVision-Language Models (LVLMs) have become indispensable tools, proficient in\ngenerating textual descriptions based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias in the generated\ncontent, where the output is primarily influenced by the underlying Large\nLanguage Models (LLMs) prior rather than the input image. Our empirical\nexperiments underscore the persistence of this bias, as LVLMs often provide\nconfident answers even in the absence of relevant images or given incongruent\nvisual input. To rectify these biases and redirect the model's focus toward\nvision information, we introduce two simple, training-free strategies. Firstly,\nfor tasks such as classification or multi-choice question-answering (QA), we\npropose a ``calibration'' step through affine transformation to adjust the\noutput distribution. This ``Post-Hoc debias'' approach ensures uniform scores\nfor each answer when the image is absent, serving as an effective\nregularization technique to alleviate the influence of LLM priors. For more\nintricate open-ended generation tasks, we extend this method to ``Debias\nsampling'', drawing inspirations from contrastive decoding methods.\nFurthermore, our investigation sheds light on the instability of LVLMs across\nvarious decoding configurations. Through systematic exploration of different\nsettings, we significantly enhance performance, surpassing reported results and\nraising concerns about the fairness of existing evaluations. Comprehensive\nexperiments substantiate the effectiveness of our proposed strategies in\nmitigating biases. These strategies not only prove beneficial in minimizing\nhallucinations but also contribute to the generation of more helpful and\nprecise illustrations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "multimodal",
      "vision-language"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05135v1",
    "title": "ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment",
    "url": "http://arxiv.org/abs/2403.05135v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05135v1",
    "authors": [
      "Xiwei Hu",
      "Rui Wang",
      "Yixiao Fang",
      "Bin Fu",
      "Pei Cheng",
      "Gang Yu"
    ],
    "date": "2024-03-08",
    "summary": "Diffusion models have demonstrated remarkable performance in the domain of\ntext-to-image generation. However, most widely used models still employ CLIP as\ntheir text encoder, which constrains their ability to comprehend dense prompts,\nencompassing multiple objects, detailed attributes, complex relationships,\nlong-text alignment, etc. In this paper, we introduce an Efficient Large\nLanguage Model Adapter, termed ELLA, which equips text-to-image diffusion\nmodels with powerful Large Language Models (LLM) to enhance text alignment\nwithout training of either U-Net or LLM. To seamlessly bridge two pre-trained\nmodels, we investigate a range of semantic alignment connector designs and\npropose a novel module, the Timestep-Aware Semantic Connector (TSC), which\ndynamically extracts timestep-dependent conditions from LLM. Our approach\nadapts semantic features at different stages of the denoising process,\nassisting diffusion models in interpreting lengthy and intricate prompts over\nsampling timesteps. Additionally, ELLA can be readily incorporated with\ncommunity models and tools to improve their prompt-following capabilities. To\nassess text-to-image models in dense prompt following, we introduce Dense\nPrompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K\ndense prompts. Extensive experiments demonstrate the superiority of ELLA in\ndense prompt following compared to state-of-the-art methods, particularly in\nmultiple object compositions involving diverse attributes and relationships.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP",
      "text-to-image",
      "image generation"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05124v1",
    "title": "CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model",
    "url": "http://arxiv.org/abs/2403.05124v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05124v1",
    "authors": [
      "Pengwei Yin",
      "Guanzhong Zeng",
      "Jingjing Wang",
      "Di Xie"
    ],
    "date": "2024-03-08",
    "summary": "Gaze estimation methods often experience significant performance degradation\nwhen evaluated across different domains, due to the domain gap between the\ntesting and training data. Existing methods try to address this issue using\nvarious domain generalization approaches, but with little success because of\nthe limited diversity of gaze datasets, such as appearance, wearable, and image\nquality. To overcome these limitations, we propose a novel framework called\nCLIP-Gaze that utilizes a pre-trained vision-language model to leverage its\ntransferable knowledge. Our framework is the first to leverage the\nvision-and-language cross-modality approach for gaze estimation task.\nSpecifically, we extract gaze-relevant feature by pushing it away from\ngaze-irrelevant features which can be flexibly constructed via language\ndescriptions. To learn more suitable prompts, we propose a personalized context\noptimization method for text prompt tuning. Furthermore, we utilize the\nrelationship among gaze samples to refine the distribution of gaze-relevant\nfeatures, thereby improving the generalization capability of the gaze\nestimation model. Extensive experiments demonstrate the excellent performance\nof CLIP-Gaze over existing methods on four cross-domain evaluations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP",
      "vision-and-language"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05056v1",
    "title": "Stealing Stable Diffusion Prior for Robust Monocular Depth Estimation",
    "url": "http://arxiv.org/abs/2403.05056v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05056v1",
    "authors": [
      "Yifan Mao",
      "Jian Liu",
      "Xianming Liu"
    ],
    "date": "2024-03-08",
    "summary": "Monocular depth estimation is a crucial task in computer vision. While\nexisting methods have shown impressive results under standard conditions, they\noften face challenges in reliably performing in scenarios such as low-light or\nrainy conditions due to the absence of diverse training data. This paper\nintroduces a novel approach named Stealing Stable Diffusion (SSD) prior for\nrobust monocular depth estimation. The approach addresses this limitation by\nutilizing stable diffusion to generate synthetic images that mimic challenging\nconditions. Additionally, a self-training mechanism is introduced to enhance\nthe model's depth estimation capability in such challenging environments. To\nenhance the utilization of the stable diffusion prior further, the DINOv2\nencoder is integrated into the depth model architecture, enabling the model to\nleverage rich semantic priors and improve its scene understanding. Furthermore,\na teacher loss is introduced to guide the student models in acquiring\nmeaningful knowledge independently, thus reducing their dependency on the\nteacher models. The effectiveness of the approach is evaluated on nuScenes and\nOxford RobotCar, two challenging public datasets, with the results showing the\nefficacy of the method. Source code and weights are available at:\nhttps://github.com/hitcslj/SSD.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion",
      "DINO",
      "DINOv2"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05049v2",
    "title": "XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution",
    "url": "http://arxiv.org/abs/2403.05049v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05049v2",
    "authors": [
      "Yunpeng Qu",
      "Kun Yuan",
      "Kai Zhao",
      "Qizhi Xie",
      "Jinhua Hao",
      "Ming Sun",
      "Chao Zhou"
    ],
    "date": "2024-03-08",
    "summary": "Diffusion-based methods, endowed with a formidable generative prior, have\nreceived increasing attention in Image Super-Resolution (ISR) recently.\nHowever, as low-resolution (LR) images often undergo severe degradation, it is\nchallenging for ISR models to perceive the semantic and degradation\ninformation, resulting in restoration images with incorrect content or\nunrealistic artifacts. To address these issues, we propose a\n\\textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR,\nto acquire precise and comprehensive semantic conditions for the diffusion\nmodel, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To\nfacilitate better fusion of cross-modal priors, a \\textit{Semantic-Fusion\nAttention} is raised. To distill semantic-preserved information instead of\nundesired degradations, a \\textit{Degradation-Free Constraint} is attached\nbetween LR and its high-resolution (HR) counterpart. Quantitative and\nqualitative results show that XPSR is capable of generating high-fidelity and\nhigh-realism images across synthetic and real-world datasets. Codes are\nreleased at \\url{https://github.com/qyp2000/XPSR}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04732v3",
    "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
    "url": "http://arxiv.org/abs/2403.04732v3",
    "pdf_url": "http://arxiv.org/pdf/2403.04732v3",
    "authors": [
      "Yizhe Zhang",
      "He Bai",
      "Ruixiang Zhang",
      "Jiatao Gu",
      "Shuangfei Zhai",
      "Josh Susskind",
      "Navdeep Jaitly"
    ],
    "date": "2024-03-07",
    "summary": "Vision-Language Models (VLMs) have recently demonstrated incredible strides\non diverse vision language tasks. We dig into vision-based deductive reasoning,\na more sophisticated but less explored realm, and find previously unexposed\nblindspots in the current SOTA VLMs. Specifically, we leverage Raven's\nProgressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop\nrelational and deductive reasoning relying solely on visual clues. We perform\ncomprehensive evaluations of several popular VLMs employing standard strategies\nsuch as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on\nthree diverse datasets, including the Mensa IQ test, IntelligenceTest, and\nRAVEN. The results reveal that despite the impressive capabilities of LLMs in\ntext-based reasoning, we are still far from achieving comparable proficiency in\nvisual deductive reasoning. We found that certain standard strategies that are\neffective when applied to LLMs do not seamlessly translate to the challenges\npresented by visual reasoning tasks. A detailed analysis reveals that VLMs\nstruggle to solve these tasks mainly because they are unable to perceive and\ncomprehend multiple, confounding abstract patterns in RPM examples.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "visual reasoning"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04652v3",
    "title": "Yi: Open Foundation Models by 01.AI",
    "url": "http://arxiv.org/abs/2403.04652v3",
    "pdf_url": "http://arxiv.org/pdf/2403.04652v3",
    "authors": [
      "01. AI",
      ":",
      "Alex Young",
      "Bei Chen",
      "Chao Li",
      "Chengen Huang",
      "Ge Zhang",
      "Guanwei Zhang",
      "Guoyin Wang",
      "Heng Li",
      "Jiangcheng Zhu",
      "Jianqun Chen",
      "Jing Chang",
      "Kaidong Yu",
      "Peng Liu",
      "Qiang Liu",
      "Shawn Yue",
      "Senbin Yang",
      "Shiming Yang",
      "Wen Xie",
      "Wenhao Huang",
      "Xiaohui Hu",
      "Xiaoyi Ren",
      "Xinyao Niu",
      "Pengcheng Nie",
      "Yanpeng Li",
      "Yuchi Xu",
      "Yudong Liu",
      "Yue Wang",
      "Yuxuan Cai",
      "Zhenyu Gu",
      "Zhiyuan Liu",
      "Zonghong Dai"
    ],
    "date": "2024-03-07",
    "summary": "We introduce the Yi model family, a series of language and multimodal models\nthat demonstrate strong multi-dimensional capabilities. The Yi model family is\nbased on 6B and 34B pretrained language models, then we extend them to chat\nmodels, 200K long context models, depth-upscaled models, and vision-language\nmodels. Our base models achieve strong performance on a wide range of\nbenchmarks like MMLU, and our finetuned chat models deliver strong human\npreference rate on major evaluation platforms like AlpacaEval and Chatbot\nArena. Building upon our scalable super-computing infrastructure and the\nclassical transformer architecture, we attribute the performance of Yi models\nprimarily to its data quality resulting from our data-engineering efforts. For\npretraining, we construct 3.1 trillion tokens of English and Chinese corpora\nusing a cascaded data deduplication and quality filtering pipeline. For\nfinetuning, we polish a small scale (less than 10K) instruction dataset over\nmultiple iterations such that every single instance has been verified directly\nby our machine learning engineers. For vision-language, we combine the chat\nlanguage model with a vision transformer encoder and train the model to align\nvisual representations to the semantic space of the language model. We further\nextend the context length to 200K through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. We show that\nextending the depth of the pretrained checkpoint through continual pretraining\nfurther improves performance. We believe that given our current results,\ncontinuing to scale up model parameters using thoroughly optimized data will\nlead to even stronger frontier models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language",
      "vision transformer"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04547v1",
    "title": "CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?",
    "url": "http://arxiv.org/abs/2403.04547v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04547v1",
    "authors": [
      "Ibrahim Alabdulmohsin",
      "Xiao Wang",
      "Andreas Steiner",
      "Priya Goyal",
      "Alexander D'Amour",
      "Xiaohua Zhai"
    ],
    "date": "2024-03-07",
    "summary": "We study the effectiveness of data-balancing for mitigating biases in\ncontrastive language-image pretraining (CLIP), identifying areas of strength\nand limitation. First, we reaffirm prior conclusions that CLIP models can\ninadvertently absorb societal stereotypes. To counter this, we present a novel\nalgorithm, called Multi-Modal Moment Matching (M4), designed to reduce both\nrepresentation and association biases (i.e. in first- and second-order\nstatistics) in multimodal data. We use M4 to conduct an in-depth analysis\ntaking into account various factors, such as the model, representation, and\ndata size. Our study also explores the dynamic nature of how CLIP learns and\nunlearns biases. In particular, we find that fine-tuning is effective in\ncountering representation biases, though its impact diminishes for association\nbiases. Also, data balancing has a mixed impact on quality: it tends to improve\nclassification but can hurt retrieval. Interestingly, data and architectural\nimprovements seem to mitigate the negative impact of data balancing on\nperformance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves\nCOCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and\nImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with\nrecommendations for improving the efficacy of data balancing in multimodal\nsystems.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "CLIP",
      "image-to-text"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04473v2",
    "title": "TextMonkey: An OCR-Free Large Multimodal Model for Understanding\n  Document",
    "url": "http://arxiv.org/abs/2403.04473v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04473v2",
    "authors": [
      "Yuliang Liu",
      "Biao Yang",
      "Qiang Liu",
      "Zhang Li",
      "Zhiyin Ma",
      "Shuo Zhang",
      "Xiang Bai"
    ],
    "date": "2024-03-07",
    "summary": "We present TextMonkey, a large multimodal model (LMM) tailored for\ntext-centric tasks. Our approach introduces enhancement across several\ndimensions: By adopting Shifted Window Attention with zero-initialization, we\nachieve cross-window connectivity at higher input resolutions and stabilize\nearly training; We hypothesize that images may contain redundant tokens, and by\nusing similarity to filter out significant tokens, we can not only streamline\nthe token length but also enhance the model's performance. Moreover, by\nexpanding our model's capabilities to encompass text spotting and grounding,\nand incorporating positional information into responses, we enhance\ninterpretability. It also learns to perform screenshot tasks through\nfinetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in\nScene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in\nDocument-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister\nCharity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks\n(comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with\na 10.9\\% increase and sets a new standard on OCRBench, a comprehensive\nbenchmark consisting of 29 OCR-related assessments, with a score of 561,\nsurpassing previous open-sourced large multimodal models for document\nunderstanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "VQA",
      "ViT"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03285v2",
    "title": "Enhancing Vietnamese VQA through Curriculum Learning on Raw and\n  Augmented Text Representations",
    "url": "http://arxiv.org/abs/2503.03285v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03285v2",
    "authors": [
      "Khoi Anh Nguyen",
      "Linh Yen Vu",
      "Thang Dinh Duong",
      "Thuan Nguyen Duong",
      "Huy Thanh Nguyen",
      "Vinh Quang Dinh"
    ],
    "date": "2024-03-05",
    "summary": "Visual Question Answering (VQA) is a multimodal task requiring reasoning\nacross textual and visual inputs, which becomes particularly challenging in\nlow-resource languages like Vietnamese due to linguistic variability and the\nlack of high-quality datasets. Traditional methods often rely heavily on\nextensive annotated datasets, computationally expensive pipelines, and large\npre-trained models, specifically in the domain of Vietnamese VQA, limiting\ntheir applicability in such scenarios. To address these limitations, we propose\na training framework that combines a paraphrase-based feature augmentation\nmodule with a dynamic curriculum learning strategy. Explicitly, augmented\nsamples are considered \"easy\" while raw samples are regarded as \"hard\". The\nframework then utilizes a mechanism that dynamically adjusts the ratio of easy\nto hard samples during training, progressively modifying the same dataset to\nincrease its difficulty level. By enabling gradual adaptation to task\ncomplexity, this approach helps the Vietnamese VQA model generalize well, thus\nimproving overall performance. Experimental results show consistent\nimprovements on the OpenViVQA dataset and mixed outcomes on the ViVQA dataset,\nhighlighting both the potential and challenges of our approach in advancing VQA\nfor Vietnamese language.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual question answering",
      "VQA"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03202v1",
    "title": "Variance-Aware Loss Scheduling for Multimodal Alignment in Low-Data\n  Settings",
    "url": "http://arxiv.org/abs/2503.03202v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03202v1",
    "authors": [
      "Sneh Pillai"
    ],
    "date": "2024-03-05",
    "summary": "Training vision-language models for image-text alignment typically requires\nlarge datasets to achieve robust performance. In low-data scenarios, standard\ncontrastive learning can struggle to align modalities effectively due to\noverfitting and unstable training dynamics. In this paper, we propose a\nvariance-aware loss scheduling approach that dynamically adjusts the weighting\nof the contrastive loss based on the statistical variability (uncertainty) in\nthe model's alignment predictions. Using a subset of the Flickr8k image-caption\ndataset to simulate limited data conditions, we demonstrate that our approach\nimproves image-text retrieval accuracy compared to a fixed-weight baseline. We\nalso compare against other adaptive weighting strategies (using output entropy\nand cosine similarity spread) and find that variance-aware scheduling provides\nthe best overall trade-off. Qualitatively, our method yields more distinct\nmultimodal embeddings as shown by t-SNE visualizations. Moreover, in a stress\ntest with noise-injected captions and images, the variance-guided loss proves\nmore robust, maintaining higher recall when random perturbations are\nintroduced. These results highlight the benefit of adaptive loss weighting for\nmultimodal alignment in low-data regimes.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language",
      "image-text"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02393v1",
    "title": "Vision-Language Model IP Protection via Prompt-based Learning",
    "url": "http://arxiv.org/abs/2503.02393v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02393v1",
    "authors": [
      "Lianyu Wang",
      "Meng Wang",
      "Huazhu Fu",
      "Daoqiang Zhang"
    ],
    "date": "2024-03-04",
    "summary": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image\nPre-Training) have seen remarkable success in visual recognition, highlighting\nthe increasing need to safeguard the intellectual property (IP) of well-trained\nmodels. Effective IP protection extends beyond ensuring authorized usage; it\nalso necessitates restricting model deployment to authorized data domains,\nparticularly when the model is fine-tuned for specific target domains. However,\ncurrent IP protection methods often rely solely on the visual backbone, which\nmay lack sufficient semantic richness. To bridge this gap, we introduce\nIP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a\nprompt-based learning approach. By leveraging the frozen visual backbone of\nCLIP, we extract both image style and content information, incorporating them\ninto the learning of IP prompt. This strategy acts as a robust barrier,\neffectively preventing the unauthorized transfer of features from authorized\ndomains to unauthorized ones. Additionally, we propose a style-enhancement\nbranch that constructs feature banks for both authorized and unauthorized\ndomains. This branch integrates self-enhanced and cross-domain features,\nfurther strengthening IP-CLIP's capability to block features from unauthorized\ndomains. Finally, we present new three metrics designed to better balance the\nperformance degradation of authorized and unauthorized domains. Comprehensive\nexperiments in various scenarios demonstrate its promising potential for\napplication in IP protection tasks for VLMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language",
      "CLIP"
    ],
    "attention_score": 0.16,
    "attention_components": {
      "base_score": 1.6,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_ArtVLM__Attribute_Recognition_Through_Vision_Based",
    "title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling",
    "url": "https://paperswithcode.com/paper/artvlm-attribute-recognition-through-vision",
    "authors": [],
    "date": "2024-03-28",
    "summary": "Recognizing and disentangling visual attributes from objects is a foundation to many computer vision applications.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.6,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_ViLT__Vision_and_Language_Transformer_Without_Conv",
    "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
    "url": "https://paperswithcode.com/paper/vilt-vision-and-language-transformer-without",
    "authors": [],
    "date": "2024-03-27",
    "summary": "Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks.",
    "source": "Papers With Code",
    "keywords": [
      "vision-and-language"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.6,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_LlavaGuard__An_Open_VLM_based_Framework_for_Safegu",
    "title": "LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models",
    "url": "https://paperswithcode.com/paper/llavaguard-vlm-based-safeguards-for-vision",
    "authors": [],
    "date": "2024-03-23",
    "summary": "This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that address the critical need for reliable guardrails in the era of large-scale data and models.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.7,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_Open6DOR__Benchmarking_Open_instruction_6_DoF_Obje",
    "title": "Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach",
    "url": "https://paperswithcode.com/paper/open6dor-benchmarking-open-instruction-6-dof",
    "authors": [],
    "date": "2024-03-22",
    "summary": "In this work, we propel the pioneer construction of the benchmark and approach for table-top Open-instruction 6-DoF Object Rearrangement (Open6DOR).",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.8,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_RL_VLM_F__Reinforcement_Learning_from_Vision_Langu",
    "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
    "url": "https://paperswithcode.com/paper/rl-vlm-f-reinforcement-learning-from-vision",
    "authors": [],
    "date": "2024-03-21",
    "summary": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.8,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_An_Image_Grid_Can_Be_Worth_a_Video__Zero_shot_Vide",
    "title": "An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM",
    "url": "https://paperswithcode.com/paper/an-image-grid-can-be-worth-a-video-zero-shot",
    "authors": [],
    "date": "2024-03-20",
    "summary": "Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 11.8,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_VLM__Task_agnostic_Video_Language_Model_Pre_traini",
    "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
    "url": "https://paperswithcode.com/paper/vlm-task-agnostic-video-language-model-pre",
    "authors": [],
    "date": "2024-03-15",
    "summary": "We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.0,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06292v1",
    "title": "Transformer based Multitask Learning for Image Captioning and Object\n  Detection",
    "url": "http://arxiv.org/abs/2403.06292v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06292v1",
    "authors": [
      "Debolena Basak",
      "P. K. Srijith",
      "Maunendra Sankar Desarkar"
    ],
    "date": "2024-03-10",
    "summary": "In several real-world scenarios like autonomous navigation and mobility, to\nobtain a better visual understanding of the surroundings, image captioning and\nobject detection play a crucial role. This work introduces a novel multitask\nlearning framework that combines image captioning and object detection into a\njoint model. We propose TICOD, Transformer-based Image Captioning and Object\ndetection model for jointly training both tasks by combining the losses\nobtained from image captioning and object detection networks. By leveraging\njoint training, the model benefits from the complementary information shared\nbetween the two tasks, leading to improved performance for image captioning.\nOur approach utilizes a transformer-based architecture that enables end-to-end\nnetwork integration for image captioning and object detection and performs both\ntasks jointly. We evaluate the effectiveness of our approach through\ncomprehensive experiments on the MS-COCO dataset. Our model outperforms the\nbaselines from image captioning literature by achieving a 3.65% improvement in\nBERTScore.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image captioning",
      "visual understanding"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06136v1",
    "title": "RESTORE: Towards Feature Shift for Vision-Language Prompt Learning",
    "url": "http://arxiv.org/abs/2403.06136v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06136v1",
    "authors": [
      "Yuncheng Yang",
      "Chuyan Zhang",
      "Zuopeng Yang",
      "Yuting Gao",
      "Yulei Qin",
      "Ke Li",
      "Xing Sun",
      "Jie Yang",
      "Yun Gu"
    ],
    "date": "2024-03-10",
    "summary": "Prompt learning is effective for fine-tuning foundation models to improve\ntheir generalization across a variety of downstream tasks. However, the prompts\nthat are independently optimized along a single modality path, may sacrifice\nthe vision-language alignment of pre-trained models in return for improved\nperformance on specific tasks and classes, leading to poorer generalization. In\nthis paper, we first demonstrate that prompt tuning along only one single\nbranch of CLIP (e.g., language or vision) is the reason why the misalignment\noccurs. Without proper regularization across the learnable parameters in\ndifferent modalities, prompt learning violates the original pre-training\nconstraints inherent in the two-tower architecture. To address such\nmisalignment, we first propose feature shift, which is defined as the variation\nof embeddings after introducing the learned prompts, to serve as an explanatory\ntool. We dive into its relation with generalizability and thereafter propose\nRESTORE, a multi-modal prompt learning method that exerts explicit constraints\non cross-modal consistency. To be more specific, to prevent feature\nmisalignment, a feature shift consistency is introduced to synchronize\ninter-modal feature shifts by measuring and regularizing the magnitude of\ndiscrepancy during prompt tuning. In addition, we propose a \"surgery\" block to\navoid short-cut hacking, where cross-modal misalignment can still be severe if\nthe feature shift of each modality varies drastically at the same rate. It is\nimplemented as feed-forward adapters upon both modalities to alleviate the\nmisalignment problem. Extensive experiments on 15 datasets demonstrate that our\nmethod outperforms the state-of-the-art prompt tuning methods without\ncompromising feature alignment.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06126v2",
    "title": "In-context Prompt Learning for Test-time Vision Recognition with Frozen\n  Vision-language Model",
    "url": "http://arxiv.org/abs/2403.06126v2",
    "pdf_url": "http://arxiv.org/pdf/2403.06126v2",
    "authors": [
      "Junhui Yin",
      "Xinyu Zhang",
      "Lin Wu",
      "Xiaojie Wang"
    ],
    "date": "2024-03-10",
    "summary": "Current pre-trained vision-language models, such as CLIP, have demonstrated\nremarkable zero-shot generalization capabilities across various downstream\ntasks. However, their performance significantly degrades when test inputs\nexhibit different distributions. In this paper, we explore the concept of\ntest-time prompt tuning (TTPT), which facilitates the adaptation of the CLIP\nmodel to novel downstream tasks through a one-step unsupervised optimization\nthat involves only test samples. Inspired by in-context learning in natural\nlanguage processing (NLP), we propose In-Context Prompt Learning (InCPL) for\ntest-time visual recognition tasks, which empowers a pre-trained\nvision-language model with labeled examples as context information on\ndownstream task. Specifically, InCPL associates a new test sample with very few\nlabeled examples (sometimes just one) as context information, enabling reliable\nlabel estimation for the test sample and facilitating model adaptation. To\nachieve this, InCPL employs an efficient language-to-vision translator to\nexplore the textual prior information for visual prompt learning. Further, we\nintroduce a context-aware unsupervised loss to optimize visual prompts tailored\nto test samples. Finally, we design a cyclic learning strategy for visual and\ntextual prompts to ensure mutual synergy across different modalities. This\nenables a pre-trained, frozen CLIP model to adapt to any task using its learned\nadaptive prompt. Our method demonstrates superior performance and achieves\nstate-of-the-art results across various downstream datasets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06088v1",
    "title": "Towards In-Vehicle Multi-Task Facial Attribute Recognition:\n  Investigating Synthetic Data and Vision Foundation Models",
    "url": "http://arxiv.org/abs/2403.06088v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06088v1",
    "authors": [
      "Esmaeil Seraj",
      "Walter Talamonti"
    ],
    "date": "2024-03-10",
    "summary": "In the burgeoning field of intelligent transportation systems, enhancing\nvehicle-driver interaction through facial attribute recognition, such as facial\nexpression, eye gaze, age, etc., is of paramount importance for safety,\npersonalization, and overall user experience. However, the scarcity of\ncomprehensive large-scale, real-world datasets poses a significant challenge\nfor training robust multi-task models. Existing literature often overlooks the\npotential of synthetic datasets and the comparative efficacy of\nstate-of-the-art vision foundation models in such constrained settings. This\npaper addresses these gaps by investigating the utility of synthetic datasets\nfor training complex multi-task models that recognize facial attributes of\npassengers of a vehicle, such as gaze plane, age, and facial expression.\nUtilizing transfer learning techniques with both pre-trained Vision Transformer\n(ViT) and Residual Network (ResNet) models, we explore various training and\nadaptation methods to optimize performance, particularly when data availability\nis limited. We provide extensive post-evaluation analysis, investigating the\neffects of synthetic data distributions on model performance in in-distribution\ndata and out-of-distribution inference. Our study unveils counter-intuitive\nfindings, notably the superior performance of ResNet over ViTs in our specific\nmulti-task context, which is attributed to the mismatch in model complexity\nrelative to task complexity. Our results highlight the challenges and\nopportunities for enhancing the use of synthetic data and vision foundation\nmodels in practical applications.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.07942v1",
    "title": "Attacking Transformers with Feature Diversity Adversarial Perturbation",
    "url": "http://arxiv.org/abs/2403.07942v1",
    "pdf_url": "http://arxiv.org/pdf/2403.07942v1",
    "authors": [
      "Chenxing Gao",
      "Hang Zhou",
      "Junqing Yu",
      "YuTeng Ye",
      "Jiale Cai",
      "Junle Wang",
      "Wei Yang"
    ],
    "date": "2024-03-10",
    "summary": "Understanding the mechanisms behind Vision Transformer (ViT), particularly\nits vulnerability to adversarial perturba tions, is crucial for addressing\nchallenges in its real-world applications. Existing ViT adversarial attackers\nrely on la bels to calculate the gradient for perturbation, and exhibit low\ntransferability to other structures and tasks. In this paper, we present a\nlabel-free white-box attack approach for ViT-based models that exhibits strong\ntransferability to various black box models, including most ViT variants, CNNs,\nand MLPs, even for models developed for other modalities. Our inspira tion\ncomes from the feature collapse phenomenon in ViTs, where the critical\nattention mechanism overly depends on the low-frequency component of features,\ncausing the features in middle-to-end layers to become increasingly similar and\neventually collapse. We propose the feature diversity attacker to naturally\naccelerate this process and achieve remarkable performance and transferability.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_Screen2Words__Automatic_Mobile_UI_Summarization_wi",
    "title": "Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning",
    "url": "https://paperswithcode.com/paper/screen2words-automatic-mobile-ui",
    "authors": [],
    "date": "2024-03-09",
    "summary": "Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05949v3",
    "title": "General surgery vision transformer: A video pre-trained foundation model\n  for general surgery",
    "url": "http://arxiv.org/abs/2403.05949v3",
    "pdf_url": "http://arxiv.org/pdf/2403.05949v3",
    "authors": [
      "Samuel Schmidgall",
      "Ji Woong Kim",
      "Jeffrey Jopling",
      "Axel Krieger"
    ],
    "date": "2024-03-09",
    "summary": "The absence of openly accessible data and specialized foundation models is a\nmajor barrier for computational research in surgery. Toward this, (i) we\nopen-source the largest dataset of general surgery videos to-date, consisting\nof 680 hours of surgical videos, including data from robotic and laparoscopic\ntechniques across 28 procedures; (ii) we propose a technique for video\npre-training a general surgery vision transformer (GSViT) on surgical videos\nbased on forward video prediction that can run in real-time for surgical\napplications, toward which we open-source the code and weights of GSViT; (iii)\nwe also release code and weights for procedure-specific fine-tuned versions of\nGSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the\nCholec80 phase annotation task, displaying improved performance over\nstate-of-the-art single frame predictors.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05846v2",
    "title": "Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines",
    "url": "http://arxiv.org/abs/2403.05846v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05846v2",
    "authors": [
      "Michael Toker",
      "Hadas Orgad",
      "Mor Ventura",
      "Dana Arad",
      "Yonatan Belinkov"
    ],
    "date": "2024-03-09",
    "summary": "Text-to-image diffusion models (T2I) use a latent representation of a text\nprompt to guide the image generation process. However, the process by which the\nencoder produces the text representation is unknown. We propose the Diffusion\nLens, a method for analyzing the text encoder of T2I models by generating\nimages from its intermediate representations. Using the Diffusion Lens, we\nperform an extensive analysis of two recent T2I models. Exploring compound\nprompts, we find that complex scenes describing multiple objects are composed\nprogressively and more slowly compared to simple scenes; Exploring knowledge\nretrieval, we find that representation of uncommon concepts requires further\ncomputation compared to common concepts, and that knowledge retrieval is\ngradual across layers. Overall, our findings provide valuable insights into the\ntext encoder component in T2I pipelines.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05794v2",
    "title": "Privacy-Preserving Diffusion Model Using Homomorphic Encryption",
    "url": "http://arxiv.org/abs/2403.05794v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05794v2",
    "authors": [
      "Yaojian Chen",
      "Qiben Yan"
    ],
    "date": "2024-03-09",
    "summary": "In this paper, we introduce a privacy-preserving stable diffusion framework\nleveraging homomorphic encryption, called HE-Diffusion, which primarily focuses\non protecting the denoising phase of the diffusion process. HE-Diffusion is a\ntailored encryption framework specifically designed to align with the unique\narchitecture of stable diffusion, ensuring both privacy and functionality. To\naddress the inherent computational challenges, we propose a novel\nmin-distortion method that enables efficient partial image encryption,\nsignificantly reducing the overhead without compromising the model's output\nquality. Furthermore, we adopt a sparse tensor representation to expedite\ncomputational operations, enhancing the overall efficiency of the\nprivacy-preserving diffusion process. We successfully implement HE-based\nprivacy-preserving stable diffusion inference. The experimental results show\nthat HE-Diffusion achieves 500 times speedup compared with the baseline method,\nand reduces time cost of the homomorphically encrypted inference to the minute\nlevel. Both the performance and accuracy of the HE-Diffusion are on par with\nthe plaintext counterpart. Our approach marks a significant step towards\nintegrating advanced cryptographic techniques with state-of-the-art generative\nmodels, paving the way for privacy-preserving and efficient image generation in\ncritical applications.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_Multimodal_Latent_Language_Modeling_with_Next_Toke",
    "title": "Multimodal Latent Language Modeling with Next-Token Diffusion",
    "url": "https://paperswithcode.com/paper/multimodal-latent-language-modeling-with-next",
    "authors": [],
    "date": "2024-03-08",
    "summary": "In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05680v2",
    "title": "How Well Do Multi-modal LLMs Interpret CT Scans? An Auto-Evaluation\n  Framework for Analyses",
    "url": "http://arxiv.org/abs/2403.05680v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05680v2",
    "authors": [
      "Qingqing Zhu",
      "Benjamin Hou",
      "Tejas S. Mathai",
      "Pritam Mukherjee",
      "Qiao Jin",
      "Xiuying Chen",
      "Zhizheng Wang",
      "Ruida Cheng",
      "Ronald M. Summers",
      "Zhiyong Lu"
    ],
    "date": "2024-03-08",
    "summary": "Automatically interpreting CT scans can ease the workload of radiologists.\nHowever, this is challenging mainly due to the scarcity of adequate datasets\nand reference standards for evaluation. This study aims to bridge this gap by\nintroducing a novel evaluation framework, named ``GPTRadScore''. This framework\nassesses the capabilities of multi-modal LLMs, such as GPT-4 with Vision\n(GPT-4V), Gemini Pro Vision, LLaVA-Med, and RadFM, in generating descriptions\nfor prospectively-identified findings. By employing a decomposition technique\nbased on GPT-4, GPTRadScore compares these generated descriptions with\ngold-standard report sentences, analyzing their accuracy in terms of body part,\nlocation, and type of finding. Evaluations demonstrated a high correlation with\nclinician assessments and highlighted its potential over traditional metrics,\nsuch as BLEU, METEOR, and ROUGE. Furthermore, to contribute to future studies,\nwe plan to release a benchmark dataset annotated by clinicians. Using\nGPTRadScore, we found that while GPT-4V and Gemini Pro Vision fare better,\ntheir performance revealed significant areas for improvement, primarily due to\nlimitations in the dataset used for training these models. To demonstrate this\npotential, RadFM was fine-tuned and it resulted in significant accuracy\nimprovements: location accuracy rose from 3.41\\% to 12.8\\%, body part accuracy\nfrom 29.12\\% to 53\\%, and type accuracy from 9.24\\% to 30\\%, thereby validating\nour hypothesis.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "GPT-4V",
      "Gemini Pro Vision"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05435v8",
    "title": "OmniCount: Multi-label Object Counting with Semantic-Geometric Priors",
    "url": "http://arxiv.org/abs/2403.05435v8",
    "pdf_url": "http://arxiv.org/pdf/2403.05435v8",
    "authors": [
      "Anindya Mondal",
      "Sauradip Nag",
      "Xiatian Zhu",
      "Anjan Dutta"
    ],
    "date": "2024-03-08",
    "summary": "Object counting is pivotal for understanding the composition of scenes.\nPreviously, this task was dominated by class-specific methods, which have\ngradually evolved into more adaptable class-agnostic strategies. However, these\nstrategies come with their own set of limitations, such as the need for manual\nexemplar input and multiple passes for multiple categories, resulting in\nsignificant inefficiencies. This paper introduces a more practical approach\nenabling simultaneous counting of multiple object categories using an\nopen-vocabulary framework. Our solution, OmniCount, stands out by using\nsemantic and geometric insights (priors) from pre-trained models to count\nmultiple categories of objects as specified by users, all without additional\ntraining. OmniCount distinguishes itself by generating precise object masks and\nleveraging varied interactive prompts via the Segment Anything Model for\nefficient counting. To evaluate OmniCount, we created the OmniCount-191\nbenchmark, a first-of-its-kind dataset with multi-label object counts,\nincluding points, bounding boxes, and VQA annotations. Our comprehensive\nevaluation in OmniCount-191, alongside other leading benchmarks, demonstrates\nOmniCount's exceptional performance, significantly outpacing existing\nsolutions. The project webpage is available at\nhttps://mondalanindya.github.io/OmniCount.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VQA",
      "Segment Anything"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05346v3",
    "title": "VLM-PL: Advanced Pseudo Labeling Approach for Class Incremental Object\n  Detection via Vision-Language Model",
    "url": "http://arxiv.org/abs/2403.05346v3",
    "pdf_url": "http://arxiv.org/pdf/2403.05346v3",
    "authors": [
      "Junsu Kim",
      "Yunhoe Ku",
      "Jihyeon Kim",
      "Junuk Cha",
      "Seungryul Baek"
    ],
    "date": "2024-03-08",
    "summary": "In the field of Class Incremental Object Detection (CIOD), creating models\nthat can continuously learn like humans is a major challenge. Pseudo-labeling\nmethods, although initially powerful, struggle with multi-scenario incremental\nlearning due to their tendency to forget past knowledge. To overcome this, we\nintroduce a new approach called Vision-Language Model assisted Pseudo-Labeling\n(VLM-PL). This technique uses Vision-Language Model (VLM) to verify the\ncorrectness of pseudo ground-truths (GTs) without requiring additional model\ntraining. VLM-PL starts by deriving pseudo GTs from a pre-trained detector.\nThen, we generate custom queries for each pseudo GT using carefully designed\nprompt templates that combine image and text features. This allows the VLM to\nclassify the correctness through its responses. Furthermore, VLM-PL integrates\nrefined pseudo and real GTs from upcoming training, effectively combining new\nand old knowledge. Extensive experiments conducted on the Pascal VOC and MS\nCOCO datasets not only highlight VLM-PL's exceptional performance in\nmulti-scenario but also illuminate its effectiveness in dual-scenario by\nachieving state-of-the-art results in both.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05239v1",
    "title": "Towards Effective Usage of Human-Centric Priors in Diffusion Models for\n  Text-based Human Image Generation",
    "url": "http://arxiv.org/abs/2403.05239v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05239v1",
    "authors": [
      "Junyan Wang",
      "Zhenhong Sun",
      "Zhiyu Tan",
      "Xuanbai Chen",
      "Weihua Chen",
      "Hao Li",
      "Cheng Zhang",
      "Yang Song"
    ],
    "date": "2024-03-08",
    "summary": "Vanilla text-to-image diffusion models struggle with generating accurate\nhuman images, commonly resulting in imperfect anatomies such as unnatural\npostures or disproportionate limbs.Existing methods address this issue mostly\nby fine-tuning the model with extra images or adding additional controls --\nhuman-centric priors such as pose or depth maps -- during the image generation\nphase. This paper explores the integration of these human-centric priors\ndirectly into the model fine-tuning stage, essentially eliminating the need for\nextra conditions at the inference stage. We realize this idea by proposing a\nhuman-centric alignment loss to strengthen human-related information from the\ntextual prompts within the cross-attention maps. To ensure semantic detail\nrichness and human structural accuracy during fine-tuning, we introduce\nscale-aware and step-wise constraints within the diffusion process, according\nto an in-depth analysis of the cross-attention layer. Extensive experiments\nshow that our method largely improves over state-of-the-art text-to-image\nmodels to synthesize high-quality human images based on user-written prompts.\nProject page: \\url{https://hcplayercvpr2024.github.io}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05220v1",
    "title": "Synthetic Privileged Information Enhances Medical Image Representation\n  Learning",
    "url": "http://arxiv.org/abs/2403.05220v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05220v1",
    "authors": [
      "Lucas Farndale",
      "Chris Walsh",
      "Robert Insall",
      "Ke Yuan"
    ],
    "date": "2024-03-08",
    "summary": "Multimodal self-supervised representation learning has consistently proven to\nbe a highly effective method in medical image analysis, offering strong task\nperformance and producing biologically informed insights. However, these\nmethods heavily rely on large, paired datasets, which is prohibitive for their\nuse in scenarios where paired data does not exist, or there is only a small\namount available. In contrast, image generation methods can work well on very\nsmall datasets, and can find mappings between unpaired datasets, meaning an\neffectively unlimited amount of paired synthetic data can be generated. In this\nwork, we demonstrate that representation learning can be significantly improved\nby synthetically generating paired information, both compared to training on\neither single-modality (up to 4.4x error reduction) or authentic multi-modal\npaired datasets (up to 5.6x error reduction).",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05168v1",
    "title": "Unlocking the Potential of Multimodal Unified Discrete Representation\n  through Training-Free Codebook Optimization and Hierarchical Alignment",
    "url": "http://arxiv.org/abs/2403.05168v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05168v1",
    "authors": [
      "Hai Huang",
      "Yan Xia",
      "Shengpeng Ji",
      "Shulei Wang",
      "Hanting Wang",
      "Jieming Zhu",
      "Zhenhua Dong",
      "Zhou Zhao"
    ],
    "date": "2024-03-08",
    "summary": "Recent advances in representation learning have demonstrated the significance\nof multimodal alignment. The Dual Cross-modal Information Disentanglement\n(DCID) model, utilizing a unified codebook, shows promising results in\nachieving fine-grained representation and cross-modal generalization. However,\nit is still hindered by equal treatment of all channels and neglect of minor\nevent information, resulting in interference from irrelevant channels and\nlimited performance in fine-grained tasks. Thus, in this work, We propose a\nTraining-free Optimization of Codebook (TOC) method to enhance model\nperformance by selecting important channels in the unified space without\nretraining. Additionally, we introduce the Hierarchical Dual Cross-modal\nInformation Disentanglement (H-DCID) approach to extend information separation\nand alignment to two levels, capturing more cross-modal details. The experiment\nresults demonstrate significant improvements across various downstream tasks,\nwith TOC contributing to an average improvement of 1.70% for DCID on four\ntasks, and H-DCID surpassing DCID by an average of 3.64%. The combination of\nTOC and H-DCID further enhances performance, exceeding DCID by 4.43%. These\nfindings highlight the effectiveness of our methods in facilitating robust and\nnuanced cross-modal learning, opening avenues for future enhancements. The\nsource code and pre-trained models can be accessed at\nhttps://github.com/haihuangcode/TOC_H-DCID.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "multimodal alignment"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05141v1",
    "title": "Med3DInsight: Enhancing 3D Medical Image Understanding with 2D\n  Multi-Modal Large Language Models",
    "url": "http://arxiv.org/abs/2403.05141v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05141v1",
    "authors": [
      "Qiuhui Chen",
      "Huping Ye",
      "Yi Hong"
    ],
    "date": "2024-03-08",
    "summary": "Understanding 3D medical image volumes is a critical task in the medical\ndomain. However, existing 3D convolution and transformer-based methods have\nlimited semantic understanding of an image volume and also need a large set of\nvolumes for training. Recent advances in multi-modal large language models\n(MLLMs) provide a new and promising way to understand images with the help of\ntext descriptions. However, most current MLLMs are designed for 2D natural\nimages. To enhance the 3D medical image understanding with 2D MLLMs, we propose\na novel pre-training framework called Med3DInsight, which marries existing 3D\nimage encoders with 2D MLLMs and bridges them via a designed Plane-Slice-Aware\nTransformer (PSAT) module. Extensive experiments demonstrate our SOTA\nperformance on two downstream segmentation and classification tasks, including\nthree public datasets with CT and MRI modalities and comparison to more than\nten baselines. Med3DInsight can be easily integrated into any current 3D\nmedical image understanding network and improves its performance by a good\nmargin.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "MLLM",
      "MLLMs"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05125v2",
    "title": "Evaluating Text-to-Image Generative Models: An Empirical Study on Human\n  Image Synthesis",
    "url": "http://arxiv.org/abs/2403.05125v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05125v2",
    "authors": [
      "Muxi Chen",
      "Yi Liu",
      "Jian Yi",
      "Changran Xu",
      "Qiuxia Lai",
      "Hongliang Wang",
      "Tsung-Yi Ho",
      "Qiang Xu"
    ],
    "date": "2024-03-08",
    "summary": "In this paper, we present an empirical study introducing a nuanced evaluation\nframework for text-to-image (T2I) generative models, applied to human image\nsynthesis. Our framework categorizes evaluations into two distinct groups:\nfirst, focusing on image qualities such as aesthetics and realism, and second,\nexamining text conditions through concept coverage and fairness. We introduce\nan innovative aesthetic score prediction model that assesses the visual appeal\nof generated images and unveils the first dataset marked with low-quality\nregions in generated human images to facilitate automatic defect detection. Our\nexploration into concept coverage probes the model's effectiveness in\ninterpreting and rendering text-based concepts accurately, while our analysis\nof fairness reveals biases in model outputs, with an emphasis on gender, race,\nand age. While our study is grounded in human imagery, this dual-faceted\napproach is designed with the flexibility to be applicable to other forms of\nimage generation, enhancing our understanding of generative models and paving\nthe way to the next generation of more sophisticated, contextually aware, and\nethically attuned generative models. Code and data, including the dataset\nannotated with defective areas, are available at\n\\href{https://github.com/cure-lab/EvaluateAIGC}{https://github.com/cure-lab/EvaluateAIGC}.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05121v1",
    "title": "CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion",
    "url": "http://arxiv.org/abs/2403.05121v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05121v1",
    "authors": [
      "Wendi Zheng",
      "Jiayan Teng",
      "Zhuoyi Yang",
      "Weihan Wang",
      "Jidong Chen",
      "Xiaotao Gu",
      "Yuxiao Dong",
      "Ming Ding",
      "Jie Tang"
    ],
    "date": "2024-03-08",
    "summary": "Recent advancements in text-to-image generative systems have been largely\ndriven by diffusion models. However, single-stage text-to-image diffusion\nmodels still face challenges, in terms of computational efficiency and the\nrefinement of image details. To tackle the issue, we propose CogView3, an\ninnovative cascaded framework that enhances the performance of text-to-image\ndiffusion. CogView3 is the first model implementing relay diffusion in the\nrealm of text-to-image generation, executing the task by first creating\nlow-resolution images and subsequently applying relay-based super-resolution.\nThis methodology not only results in competitive text-to-image outputs but also\ngreatly reduces both training and inference costs. Our experimental results\ndemonstrate that CogView3 outperforms SDXL, the current state-of-the-art\nopen-source text-to-image diffusion model, by 77.0\\% in human evaluations, all\nwhile requiring only about 1/2 of the inference time. The distilled variant of\nCogView3 achieves comparable performance while only utilizing 1/10 of the\ninference time by SDXL.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05105v1",
    "title": "Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval",
    "url": "http://arxiv.org/abs/2403.05105v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05105v1",
    "authors": [
      "Haochen Han",
      "Qinghua Zheng",
      "Guang Dai",
      "Minnan Luo",
      "Jingdong Wang"
    ],
    "date": "2024-03-08",
    "summary": "Collecting well-matched multimedia datasets is crucial for training\ncross-modal retrieval models. However, in real-world scenarios, massive\nmultimodal data are harvested from the Internet, which inevitably contains\nPartially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data\nwill remarkably harm the cross-modal retrieval performance. Previous efforts\ntend to mitigate this problem by estimating a soft correspondence to\ndown-weight the contribution of PMPs. In this paper, we aim to address this\nchallenge from a new perspective: the potential semantic similarity among\nunpaired samples makes it possible to excavate useful knowledge from mismatched\npairs. To achieve this, we propose L2RM, a general framework based on Optimal\nTransport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to\ngenerate refined alignments by seeking a minimal-cost transport plan across\ndifferent modalities. To formalize the rematching idea in OT, first, we propose\na self-supervised cost function that automatically learns from explicit\nsimilarity-cost mapping relation. Second, we present to model a partial OT\nproblem while restricting the transport among false positives to further boost\nrefined alignments. Extensive experiments on three benchmarks demonstrate our\nL2RM significantly improves the robustness against PMPs for existing models.\nThe code is available at https://github.com/hhc1997/L2RM.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05029v2",
    "title": "BjTT: A Large-scale Multimodal Dataset for Traffic Prediction",
    "url": "http://arxiv.org/abs/2403.05029v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05029v2",
    "authors": [
      "Chengyang Zhang",
      "Yong Zhang",
      "Qitan Shao",
      "Jiangtao Feng",
      "Bo Li",
      "Yisheng Lv",
      "Xinglin Piao",
      "Baocai Yin"
    ],
    "date": "2024-03-08",
    "summary": "Traffic prediction is one of the most significant foundations in Intelligent\nTransportation Systems (ITS). Traditional traffic prediction methods rely only\non historical traffic data to predict traffic trends and face two main\nchallenges. 1) insensitivity to unusual events. 2) limited performance in\nlong-term prediction. In this work, we explore how generative models combined\nwith text describing the traffic system can be applied for traffic generation,\nand name the task Text-to-Traffic Generation (TTG). The key challenge of the\nTTG task is how to associate text with the spatial structure of the road\nnetwork and traffic data for generating traffic situations. To this end, we\npropose ChatTraffic, the first diffusion model for text-to-traffic generation.\nTo guarantee the consistency between synthetic and real data, we augment a\ndiffusion model with the Graph Convolutional Network (GCN) to extract spatial\ncorrelations of traffic data. In addition, we construct a large dataset\ncontaining text-traffic pairs for the TTG task. We benchmarked our model\nqualitatively and quantitatively on the released dataset. The experimental\nresults indicate that ChatTraffic can generate realistic traffic situations\nfrom the text. Our code and dataset are available at\nhttps://github.com/ChyaZhang/ChatTraffic.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_Socratic_Models__Composing_Zero_Shot_Multimodal_Re",
    "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
    "url": "https://paperswithcode.com/paper/socratic-models-composing-zero-shot",
    "authors": [],
    "date": "2024-03-07",
    "summary": "Large pretrained (e. g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04908v3",
    "title": "Self-Adapting Large Visual-Language Models to Edge Devices across Visual\n  Modalities",
    "url": "http://arxiv.org/abs/2403.04908v3",
    "pdf_url": "http://arxiv.org/pdf/2403.04908v3",
    "authors": [
      "Kaiwen Cai",
      "Zhekai Duan",
      "Gaowen Liu",
      "Charles Fleming",
      "Chris Xiaoxuan Lu"
    ],
    "date": "2024-03-07",
    "summary": "Recent advancements in Vision-Language (VL) models have sparked interest in\ntheir deployment on edge devices, yet challenges in handling diverse visual\nmodalities, manual annotation, and computational constraints remain. We\nintroduce EdgeVL, a novel framework that bridges this gap by seamlessly\nintegrating dual-modality knowledge distillation and quantization-aware\ncontrastive learning. This approach enables the adaptation of large VL models,\nlike CLIP, for efficient use with both RGB and non-RGB images on\nresource-limited devices without the need for manual annotations. EdgeVL not\nonly transfers visual language alignment capabilities to compact models but\nalso maintains feature quality post-quantization, significantly enhancing\nopen-vocabulary classification performance across various visual modalities.\nOur work represents the first systematic effort to adapt large VL models for\nedge deployment, showcasing up to 15.4% accuracy improvements on multiple\ndatasets and up to 93-fold reduction in model size.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language",
      "CLIP"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04724v1",
    "title": "Masked Capsule Autoencoders",
    "url": "http://arxiv.org/abs/2403.04724v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04724v1",
    "authors": [
      "Miles Everett",
      "Mingjun Zhong",
      "Georgios Leontidis"
    ],
    "date": "2024-03-07",
    "summary": "We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that\nutilises pretraining in a self-supervised manner. Capsule Networks have emerged\nas a powerful alternative to Convolutional Neural Networks (CNNs), and have\nshown favourable properties when compared to Vision Transformers (ViT), but\nhave struggled to effectively learn when presented with more complex data,\nleading to Capsule Network models that do not scale to modern tasks. Our\nproposed MCAE model alleviates this issue by reformulating the Capsule Network\nto use masked image modelling as a pretraining stage before finetuning in a\nsupervised manner. Across several experiments and ablations studies we\ndemonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefit\nfrom self-supervised pretraining, paving the way for further advancements in\nthis neural network domain. For instance, pretraining on the Imagenette\ndataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only\nstate-of-the-art results for Capsule Networks but also a 9% improvement\ncompared to purely supervised training. Thus we propose that Capsule Networks\nbenefit from and should be trained within a masked image modelling framework,\nwith a novel capsule decoder, to improve a Capsule Network's performance on\nrealistic-sized images.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04697v2",
    "title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit\n  Detectors",
    "url": "http://arxiv.org/abs/2403.04697v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04697v2",
    "authors": [
      "Kaishen Yuan",
      "Zitong Yu",
      "Xin Liu",
      "Weicheng Xie",
      "Huanjing Yue",
      "Jingyu Yang"
    ],
    "date": "2024-03-07",
    "summary": "Facial Action Units (AU) is a vital concept in the realm of affective\ncomputing, and AU detection has always been a hot research topic. Existing\nmethods suffer from overfitting issues due to the utilization of a large number\nof learnable parameters on scarce AU-annotated datasets or heavy reliance on\nsubstantial additional relevant data. Parameter-Efficient Transfer Learning\n(PETL) provides a promising paradigm to address these challenges, whereas its\nexisting methods lack design for AU characteristics. Therefore, we innovatively\ninvestigate PETL paradigm to AU detection, introducing AUFormer and proposing a\nnovel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual\nMoKE specific to a certain AU with minimal learnable parameters first\nintegrates personalized multi-scale and correlation knowledge. Then the MoKE\ncollaborates with other MoKEs in the expert group to obtain aggregated\ninformation and inject it into the frozen Vision Transformer (ViT) to achieve\nparameter-efficient AU detection. Additionally, we design a Margin-truncated\nDifficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the\nmodel to focus more on activated AUs, differentiate the difficulty of\nunactivated AUs, and discard potential mislabeled samples. Extensive\nexperiments from various perspectives, including within-domain, cross-domain,\ndata efficiency, and micro-expression domain, demonstrate AUFormer's\nstate-of-the-art performance and robust generalization abilities without\nrelying on additional relevant data. The code for AUFormer is available at\nhttps://github.com/yuankaishen2001/AUFormer.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04692v2",
    "title": "PixArt-\u03a3: Weak-to-Strong Training of Diffusion Transformer for 4K\n  Text-to-Image Generation",
    "url": "http://arxiv.org/abs/2403.04692v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04692v2",
    "authors": [
      "Junsong Chen",
      "Chongjian Ge",
      "Enze Xie",
      "Yue Wu",
      "Lewei Yao",
      "Xiaozhe Ren",
      "Zhongdao Wang",
      "Ping Luo",
      "Huchuan Lu",
      "Zhenguo Li"
    ],
    "date": "2024-03-07",
    "summary": "In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer\nmodel~(DiT) capable of directly generating images at 4K resolution.\nPixArt-\\Sigma represents a significant advancement over its predecessor,\nPixArt-\\alpha, offering images of markedly higher fidelity and improved\nalignment with text prompts. A key feature of PixArt-\\Sigma is its training\nefficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it\nevolves from the `weaker' baseline to a `stronger' model via incorporating\nhigher quality data, a process we term \"weak-to-strong training\". The\nadvancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data:\nPixArt-\\Sigma incorporates superior-quality image data, paired with more\nprecise and detailed image captions. (2) Efficient Token Compression: we\npropose a novel attention module within the DiT framework that compresses both\nkeys and values, significantly improving efficiency and facilitating\nultra-high-resolution image generation. Thanks to these improvements,\nPixArt-\\Sigma achieves superior image quality and user prompt adherence\ncapabilities with significantly smaller model size (0.6B parameters) than\nexisting text-to-image diffusion models, such as SDXL (2.6B parameters) and SD\nCascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K\nimages supports the creation of high-resolution posters and wallpapers,\nefficiently bolstering the production of high-quality visual content in\nindustries such as film and gaming.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04626v2",
    "title": "MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training\n  with Masked Autoencoder",
    "url": "http://arxiv.org/abs/2403.04626v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04626v2",
    "authors": [
      "Lei Li",
      "Tianfang Zhang",
      "Xinglin Zhang",
      "Jiaqi Liu",
      "Bingqi Ma",
      "Yan Luo",
      "Tao Chen"
    ],
    "date": "2024-03-07",
    "summary": "Within the domain of medical analysis, extensive research has explored the\npotential of mutual learning between Masked Autoencoders(MAEs) and multimodal\ndata. However, the impact of MAEs on intermodality remains a key challenge. We\nintroduce MedFLIP, a Fast Language-Image Pre-training method for Medical\nanalysis. We explore MAEs for zero-shot learning with crossed domains, which\nenhances the model's ability to learn from limited data, a common scenario in\nmedical diagnostics. We verify that masking an image does not affect\ninter-modal learning. Furthermore, we propose the SVD loss to enhance the\nrepresentation learning for characteristics of medical images, aiming to\nimprove classification accuracy by leveraging the structural intricacies of\nsuch data. Our theory posits that masking encourages semantic preservation,\nrobust feature extraction, regularization, domain adaptation, and invariance\nlearning. Lastly, we validate using language will improve the zero-shot\nperformance for the medical image analysis. MedFLIP's scaling of the masking\nprocess marks an advancement in the field, offering a pathway to rapid and\nprecise medical image analysis without the traditional computational\nbottlenecks. Through experiments and validation, MedFLIP demonstrates efficient\nperformance improvements, helps for future research and application in medical\ndiagnostics.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-and-language"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04593v1",
    "title": "Embodied Understanding of Driving Scenarios",
    "url": "http://arxiv.org/abs/2403.04593v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04593v1",
    "authors": [
      "Yunsong Zhou",
      "Linyan Huang",
      "Qingwen Bu",
      "Jia Zeng",
      "Tianyu Li",
      "Hang Qiu",
      "Hongzi Zhu",
      "Minyi Guo",
      "Yu Qiao",
      "Hongyang Li"
    ],
    "date": "2024-03-07",
    "summary": "Embodied scene understanding serves as the cornerstone for autonomous agents\nto perceive, interpret, and respond to open driving scenarios. Such\nunderstanding is typically founded upon Vision-Language Models (VLMs).\nNevertheless, existing VLMs are restricted to the 2D domain, devoid of spatial\nawareness and long-horizon extrapolation proficiencies. We revisit the key\naspects of autonomous driving and formulate appropriate rubrics. Hereby, we\nintroduce the Embodied Language Model (ELM), a comprehensive framework tailored\nfor agents' understanding of driving scenes with large spatial and temporal\nspans. ELM incorporates space-aware pre-training to endow the agent with robust\nspatial localization capabilities. Besides, the model employs time-aware token\nselection to accurately inquire about temporal cues. We instantiate ELM on the\nreformulated multi-faced benchmark, and it surpasses previous state-of-the-art\napproaches in all aspects. All code, data, and models will be publicly shared.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM",
      "vision-language"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04588v1",
    "title": "Zero-shot cross-modal transfer of Reinforcement Learning policies\n  through a Global Workspace",
    "url": "http://arxiv.org/abs/2403.04588v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04588v1",
    "authors": [
      "L\u00e9opold Mayti\u00e9",
      "Benjamin Devillers",
      "Alexandre Arnold",
      "Rufin VanRullen"
    ],
    "date": "2024-03-07",
    "summary": "Humans perceive the world through multiple senses, enabling them to create a\ncomprehensive representation of their surroundings and to generalize\ninformation across domains. For instance, when a textual description of a scene\nis given, humans can mentally visualize it. In fields like robotics and\nReinforcement Learning (RL), agents can also access information about the\nenvironment through multiple sensors; yet redundancy and complementarity\nbetween sensors is difficult to exploit as a source of robustness (e.g. against\nsensor failure) or generalization (e.g. transfer across domains). Prior\nresearch demonstrated that a robust and flexible multimodal representation can\nbe efficiently constructed based on the cognitive science notion of a 'Global\nWorkspace': a unique representation trained to combine information across\nmodalities, and to broadcast its signal back to each modality. Here, we explore\nwhether such a brain-inspired multimodal representation could be advantageous\nfor RL agents. First, we train a 'Global Workspace' to exploit information\ncollected about the environment via two input modalities (a visual input, or an\nattribute vector representing the state of the agent and/or its environment).\nThen, we train a RL agent policy using this frozen Global Workspace. In two\ndistinct environments and tasks, our results reveal the model's ability to\nperform zero-shot cross-modal transfer between input modalities, i.e. to apply\nto image inputs a policy previously trained on attribute vectors (and\nvice-versa), without additional training or fine-tuning. Variants and ablations\nof the full Global Workspace (including a CLIP-like multimodal representation\ntrained via contrastive learning) did not display the same generalization\nabilities.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "CLIP"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04523v1",
    "title": "T-TAME: Trainable Attention Mechanism for Explaining Convolutional\n  Networks and Vision Transformers",
    "url": "http://arxiv.org/abs/2403.04523v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04523v1",
    "authors": [
      "Mariano V. Ntrougkas",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ],
    "date": "2024-03-07",
    "summary": "The development and adoption of Vision Transformers and other deep-learning\narchitectures for image classification tasks has been rapid. However, the\n\"black box\" nature of neural networks is a barrier to adoption in applications\nwhere explainability is essential. While some techniques for generating\nexplanations have been proposed, primarily for Convolutional Neural Networks,\nadapting such techniques to the new paradigm of Vision Transformers is\nnon-trivial. This paper presents T-TAME, Transformer-compatible Trainable\nAttention Mechanism for Explanations, a general methodology for explaining deep\nneural networks used in image classification tasks. The proposed architecture\nand training technique can be easily applied to any convolutional or Vision\nTransformer-like neural network, using a streamlined training approach. After\ntraining, explanation maps can be computed in a single forward pass; these\nexplanation maps are comparable to or outperform the outputs of computationally\nexpensive perturbation-based explainability techniques, achieving SOTA\nperformance. We apply T-TAME to three popular deep learning classifier\narchitectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet\ndataset, and we demonstrate improvements over existing state-of-the-art\nexplainability methods. A detailed analysis of the results and an ablation\nstudy provide insights into how the T-TAME design choices affect the quality of\nthe generated explanation maps.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer",
      "ViT"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04343v1",
    "title": "CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction\n  Tuning",
    "url": "http://arxiv.org/abs/2403.04343v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04343v1",
    "authors": [
      "Yanqi Dai",
      "Dong Jing",
      "Nanyi Fei",
      "Zhiwu Lu"
    ],
    "date": "2024-03-07",
    "summary": "Visual instruction tuning is a key training stage of large multimodal models\n(LMMs). Nevertheless, the common practice of indiscriminately mixing\ninstruction-following data from various tasks may result in suboptimal overall\nperformance due to different instruction formats and knowledge domains across\ntasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing\n(CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our\nknowledge, this is the first work that explores multi-task optimization in\nvisual instruction tuning. Specifically, we consider two key dimensions for\ntask balancing: (1) Inter-Task Contribution, the phenomenon where learning one\ntask potentially enhances the performance in other tasks, attributable to the\noverlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to\nthe learning difficulty within a single task. By quantifying these two\ndimensions with performance-based metrics, task balancing is thus enabled by\nassigning more weights to tasks that offer substantial contributions to others,\nreceive minimal contributions from others, and also have great intra-task\ndifficulties. Experiments show that our CoTBal leads to superior overall\nperformance in multi-task visual instruction tuning.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "visual instruction tuning"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04321v2",
    "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
    "url": "http://arxiv.org/abs/2403.04321v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04321v2",
    "authors": [
      "Leigang Qu",
      "Wenjie Wang",
      "Yongqi Li",
      "Hanwang Zhang",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ],
    "date": "2024-03-07",
    "summary": "Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image",
      "image generation"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04204v1",
    "title": "On the Essence and Prospect: An Investigation of Alignment Approaches\n  for Big Models",
    "url": "http://arxiv.org/abs/2403.04204v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04204v1",
    "authors": [
      "Xinpeng Wang",
      "Shitong Duan",
      "Xiaoyuan Yi",
      "Jing Yao",
      "Shanlin Zhou",
      "Zhihua Wei",
      "Peng Zhang",
      "Dongkuan Xu",
      "Maosong Sun",
      "Xing Xie"
    ],
    "date": "2024-03-07",
    "summary": "Big models have achieved revolutionary breakthroughs in the field of AI, but\nthey might also pose potential concerns. Addressing such concerns, alignment\ntechnologies were introduced to make these models conform to human preferences\nand values. Despite considerable advancements in the past year, various\nchallenges lie in establishing the optimal alignment strategy, such as data\ncost and scalable oversight, and how to align remains an open question. In this\nsurvey paper, we comprehensively investigate value alignment approaches. We\nfirst unpack the historical context of alignment tracing back to the 1920s\n(where it comes from), then delve into the mathematical essence of alignment\n(what it is), shedding light on the inherent challenges. Following this\nfoundation, we provide a detailed examination of existing alignment methods,\nwhich fall into three categories: Reinforcement Learning, Supervised\nFine-Tuning, and In-context Learning, and demonstrate their intrinsic\nconnections, strengths, and limitations, helping readers better understand this\nresearch area. In addition, two emerging topics, personal alignment, and\nmultimodal alignment, are also discussed as novel frontiers in this field.\nLooking forward, we discuss potential alignment paradigms and how they could\nhandle remaining challenges, prospecting where future alignment will go.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "multimodal alignment"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_Supervised_Multimodal_Bitransformers_for_Classifyi",
    "title": "Supervised Multimodal Bitransformers for Classifying Images and Text",
    "url": "https://paperswithcode.com/paper/supervised-multimodal-bitransformers-for",
    "authors": [],
    "date": "2024-03-06",
    "summary": "Self-supervised bidirectional transformer models such as BERT have led to dramatic improvements in a wide variety of textual classification tasks.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03321v1",
    "title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
    "url": "http://arxiv.org/abs/2503.03321v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03321v1",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ],
    "date": "2024-03-05",
    "summary": "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03278v1",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with\n  Knowledge Descriptions",
    "url": "http://arxiv.org/abs/2503.03278v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03278v1",
    "authors": [
      "Jun Li",
      "Che Liu",
      "Wenjia Bai",
      "Rossella Arcucci",
      "Cosmin I. Bercea",
      "Julia A. Schnabel"
    ],
    "date": "2024-03-05",
    "summary": "Visual Language Models (VLMs) have demonstrated impressive capabilities in\nvisual grounding tasks. However, their effectiveness in the medical domain,\nparticularly for abnormality detection and localization within medical images,\nremains underexplored. A major challenge is the complex and abstract nature of\nmedical terminology, which makes it difficult to directly associate\npathological anomaly terms with their corresponding visual features. In this\nwork, we introduce a novel approach to enhance VLM performance in medical\nabnormality detection and localization by leveraging decomposed medical\nknowledge. Instead of directly prompting models to recognize specific\nabnormalities, we focus on breaking down medical concepts into fundamental\nattributes and common visual patterns. This strategy promotes a stronger\nalignment between textual descriptions and visual features, improving both the\nrecognition and localization of abnormalities in medical images.We evaluate our\nmethod on the 0.23B Florence-2 base model and demonstrate that it achieves\ncomparable performance in abnormality grounding to significantly larger 7B\nLLaVA-based medical VLMs, despite being trained on only 1.5% of the data used\nfor such models. Experimental results also demonstrate the effectiveness of our\napproach in both known and previously unseen abnormalities, suggesting its\nstrong generalization capabilities.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03196v1",
    "title": "SpiritSight Agent: Advanced GUI Agent with One Look",
    "url": "http://arxiv.org/abs/2503.03196v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03196v1",
    "authors": [
      "Zhiyuan Huang",
      "Ziming Cheng",
      "Junting Pan",
      "Zhaohui Hou",
      "Mingjie Zhan"
    ],
    "date": "2024-03-05",
    "summary": "Graphical User Interface (GUI) agents show amazing abilities in assisting\nhuman-computer interaction, automating human user's navigation on digital\ndevices. An ideal GUI agent is expected to achieve high accuracy, low latency,\nand compatibility for different GUI platforms. Recent vision-based approaches\nhave shown promise by leveraging advanced Vision Language Models (VLMs). While\nthey generally meet the requirements of compatibility and low latency, these\nvision-based GUI agents tend to have low accuracy due to their limitations in\nelement grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a\nvision-based, end-to-end GUI agent that excels in GUI navigation tasks across\nvarious GUI platforms. First, we create a multi-level, large-scale,\nhigh-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods,\nempowering SpiritSight with robust GUI understanding and grounding\ncapabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$\nmethod to resolve the ambiguity problem in dynamic high-resolution of visual\ninputs, further enhancing SpiritSight's ability to ground GUI objects. Through\nthese efforts, SpiritSight agent outperforms other advanced methods on diverse\nGUI benchmarks, demonstrating its superior capability and compatibility in GUI\nnavigation tasks. Models are available at\n$\\href{https://huggingface.co/SenseLLM/SpiritSight-Agent-8B}{this\\ URL}$.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_LayoutXLM__Multimodal_Pre_training_for_Multilingua",
    "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding",
    "url": "https://paperswithcode.com/paper/layoutxlm-multimodal-pre-training-for",
    "authors": [],
    "date": "2024-03-05",
    "summary": "In this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding.",
    "source": "Papers With Code",
    "keywords": [
      "multimodal"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02824v1",
    "title": "Developing a PET/CT Foundation Model for Cross-Modal Anatomical and\n  Functional Imaging",
    "url": "http://arxiv.org/abs/2503.02824v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02824v1",
    "authors": [
      "Yujin Oh",
      "Robert Seifert",
      "Yihan Cao",
      "Christoph Clement",
      "Justin Ferdinandus",
      "Constantin Lapa",
      "Alessandro Liebich",
      "Michelle Amon",
      "Johanna Enke",
      "Sifan Song",
      "Runqi Meng",
      "Fang Zeng",
      "Ning Guo",
      "Xiang Li",
      "Pedram Heidari",
      "Axel Rominger",
      "Kuangyu Shi",
      "Quanzheng Li"
    ],
    "date": "2024-03-04",
    "summary": "In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision transformer"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02597v1",
    "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs",
    "url": "http://arxiv.org/abs/2503.02597v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02597v1",
    "authors": [
      "Wei-Yao Wang",
      "Zhao Wang",
      "Helen Suzuki",
      "Yoshiyuki Kobayashi"
    ],
    "date": "2024-03-04",
    "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02579v1",
    "title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic\n  Understanding of High-Intensity Surgical Environments",
    "url": "http://arxiv.org/abs/2503.02579v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02579v1",
    "authors": [
      "Ege \u00d6zsoy",
      "Chantal Pellegrini",
      "Tobias Czempiel",
      "Felix Tristram",
      "Kun Yuan",
      "David Bani-Harouni",
      "Ulrich Eck",
      "Benjamin Busam",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "date": "2024-03-04",
    "summary": "Operating rooms (ORs) are complex, high-stakes environments requiring precise\nunderstanding of interactions among medical staff, tools, and equipment for\nenhancing surgical assistance, situational awareness, and patient safety.\nCurrent datasets fall short in scale, realism and do not capture the multimodal\nnature of OR scenes, limiting progress in OR modeling. To this end, we\nintroduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR\ndataset, and the first dataset to enable multimodal scene graph generation.\nMM-OR captures comprehensive OR scenes containing RGB-D data, detail views,\naudio, speech transcripts, robotic logs, and tracking data and is annotated\nwith panoptic segmentations, semantic scene graphs, and downstream task labels.\nFurther, we propose MM2SG, the first multimodal large vision-language model for\nscene graph generation, and through extensive experiments, demonstrate its\nability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG\nestablish a new benchmark for holistic OR understanding, and open the path\ntowards multimodal scene analysis in complex, high-stakes environments. Our\ncode, and data is available at https://github.com/egeozsoy/MM-OR.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal",
      "vision-language"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02334v1",
    "title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language\n  Models",
    "url": "http://arxiv.org/abs/2503.02334v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02334v1",
    "authors": [
      "Sonnet Xu",
      "Joseph Janizek",
      "Yixing Jiang",
      "Roxana Daneshjou"
    ],
    "date": "2024-03-04",
    "summary": "Vision language models (VLMs) show promise in medical diagnosis, but their\nperformance across demographic subgroups when using in-context learning (ICL)\nremains poorly understood. We examine how the demographic composition of\ndemonstration examples affects VLM performance in two medical imaging tasks:\nskin lesion malignancy prediction and pneumothorax detection from chest\nradiographs. Our analysis reveals that ICL influences model predictions through\nmultiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease\nbase rates from prompts and (2) ICL leads VLMs to make predictions that perform\ndifferently across demographic groups, even after controlling for\nsubgroup-specific disease base rates. Our empirical results inform\nbest-practices for prompting current VLMs (specifically examining demographic\nsubgroup performance, and matching base rates of labels to target distribution\nat a bulk level and within subgroups), while also suggesting next steps for\nimproving our theoretical understanding of these models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision language model",
      "VLM"
    ],
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.4,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_F_VLM__Open_Vocabulary_Object_Detection_upon_Froze",
    "title": "F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models",
    "url": "https://paperswithcode.com/paper/f-vlm-open-vocabulary-object-detection-upon",
    "authors": [],
    "date": "2024-03-03",
    "summary": "We present F-VLM, a simple open-vocabulary object detection method built upon Frozen Vision and Language Models.",
    "source": "Papers With Code",
    "keywords": [
      "VLM"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_VILA__Learning_Image_Aesthetics_from_User_Comments",
    "title": "VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining",
    "url": "https://paperswithcode.com/paper/vila-learning-image-aesthetics-from-user",
    "authors": [],
    "date": "2024-03-02",
    "summary": "Our results show that our pretrained aesthetic vision-language model outperforms prior works on image aesthetic captioning over the AVA-Captions dataset, and it has powerful zero-shot capability for aesthetic tasks such as zero-shot style classification and zero-shot IAA, surpassing many supervised baselines.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "paperswithcode_MiniGPT_4__Enhancing_Vision_Language_Understanding",
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "url": "https://paperswithcode.com/paper/minigpt-4-enhancing-vision-language",
    "authors": [],
    "date": "2024-03-01",
    "summary": "Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts.",
    "source": "Papers With Code",
    "keywords": [
      "vision-language"
    ],
    "code_url": null,
    "attention_score": 0.14,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.2,
      "age_months": 12.5,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06339v1",
    "title": "FOAA: Flattened Outer Arithmetic Attention For Multimodal Tumor\n  Classification",
    "url": "http://arxiv.org/abs/2403.06339v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06339v1",
    "authors": [
      "Omnia Alwazzan",
      "Ioannis Patras",
      "Gregory Slabaugh"
    ],
    "date": "2024-03-10",
    "summary": "Fusion of multimodal healthcare data holds great promise to provide a\nholistic view of a patient's health, taking advantage of the complementarity of\ndifferent modalities while leveraging their correlation. This paper proposes a\nsimple and effective approach, inspired by attention, to fuse discriminative\nfeatures from different modalities. We propose a novel attention mechanism,\ncalled Flattened Outer Arithmetic Attention (FOAA), which relies on outer\narithmetic operators (addition, subtraction, product, and division) to compute\nattention scores from keys, queries and values derived from flattened\nembeddings of each modality. We demonstrate how FOAA can be implemented for\nself-attention and cross-attention, providing a reusable component in neural\nnetwork architectures. We evaluate FOAA on two datasets for multimodal tumor\nclassification and achieve state-of-the-art results, and we demonstrate that\nfeatures enriched by FOAA are superior to those derived from other fusion\napproaches. The code is publicly available at\n\\href{https://github.com/omniaalwazzan/FOAA}{https://github.com/omniaalwazzan/FOAA}",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06338v1",
    "title": "Disentangling shared and private latent factors in multimodal\n  Variational Autoencoders",
    "url": "http://arxiv.org/abs/2403.06338v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06338v1",
    "authors": [
      "Kaspar M\u00e4rtens",
      "Christopher Yau"
    ],
    "date": "2024-03-10",
    "summary": "Generative models for multimodal data permit the identification of latent\nfactors that may be associated with important determinants of observed data\nheterogeneity. Common or shared factors could be important for explaining\nvariation across modalities whereas other factors may be private and important\nonly for the explanation of a single modality. Multimodal Variational\nAutoencoders, such as MVAE and MMVAE, are a natural choice for inferring those\nunderlying latent factors and separating shared variation from private. In this\nwork, we investigate their capability to reliably perform this disentanglement.\nIn particular, we highlight a challenging problem setting where\nmodality-specific variation dominates the shared signal. Taking a cross-modal\nprediction perspective, we demonstrate limitations of existing models, and\npropose a modification how to make them more robust to modality-specific\nvariation. Our findings are supported by experiments on synthetic as well as\nvarious real-world multi-omics data sets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06302v1",
    "title": "Nonparametric Automatic Differentiation Variational Inference with\n  Spline Approximation",
    "url": "http://arxiv.org/abs/2403.06302v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06302v1",
    "authors": [
      "Yuda Shao",
      "Shan Yu",
      "Tianshu Feng"
    ],
    "date": "2024-03-10",
    "summary": "Automatic Differentiation Variational Inference (ADVI) is efficient in\nlearning probabilistic models. Classic ADVI relies on the parametric approach\nto approximate the posterior. In this paper, we develop a spline-based\nnonparametric approximation approach that enables flexible posterior\napproximation for distributions with complicated structures, such as skewness,\nmultimodality, and bounded support. Compared with widely-used nonparametric\nvariational inference methods, the proposed method is easy to implement and\nadaptive to various data structures. By adopting the spline approximation, we\nderive a lower bound of the importance weighted autoencoder and establish the\nasymptotic consistency. Experiments demonstrate the efficiency of the proposed\nmethod in approximating complex posterior distributions and improving the\nperformance of generative models with incomplete data.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06247v2",
    "title": "Text-Guided Variational Image Generation for Industrial Anomaly\n  Detection and Segmentation",
    "url": "http://arxiv.org/abs/2403.06247v2",
    "pdf_url": "http://arxiv.org/pdf/2403.06247v2",
    "authors": [
      "Mingyu Lee",
      "Jongwon Choi"
    ],
    "date": "2024-03-10",
    "summary": "We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06213v1",
    "title": "$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections",
    "url": "http://arxiv.org/abs/2403.06213v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06213v1",
    "authors": [
      "Roy Miles",
      "Ismail Elezi",
      "Jiankang Deng"
    ],
    "date": "2024-03-10",
    "summary": "Knowledge distillation is an effective method for training small and\nefficient deep learning models. However, the efficacy of a single method can\ndegenerate when transferring to other tasks, modalities, or even other\narchitectures. To address this limitation, we propose a novel constrained\nfeature distillation method. This method is derived from a small set of core\nprinciples, which results in two emerging components: an orthogonal projection\nand a task-specific normalisation. Equipped with both of these components, our\ntransformer models can outperform all previous methods on ImageNet and reach up\nto a 4.4% relative improvement over the previous state-of-the-art methods. To\nfurther demonstrate the generality of our method, we apply it to object\ndetection and image generation, whereby we obtain consistent and substantial\nperformance improvements over state-of-the-art. Code and models are publicly\navailable: https://github.com/roymiles/vkd",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06071v1",
    "title": "Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised\n  Semantic Hashing",
    "url": "http://arxiv.org/abs/2403.06071v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06071v1",
    "authors": [
      "Liyang He",
      "Zhenya Huang",
      "Jiayu Liu",
      "Enhong Chen",
      "Fei Wang",
      "Jing Sha",
      "Shijin Wang"
    ],
    "date": "2024-03-10",
    "summary": "Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06027v1",
    "title": "Multimodal deep learning approach to predicting neurological recovery\n  from coma after cardiac arrest",
    "url": "http://arxiv.org/abs/2403.06027v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06027v1",
    "authors": [
      "Felix H. Krones",
      "Ben Walker",
      "Guy Parsons",
      "Terry Lyons",
      "Adam Mahdi"
    ],
    "date": "2024-03-09",
    "summary": "This work showcases our team's (The BEEGees) contributions to the 2023 George\nB. Moody PhysioNet Challenge. The aim was to predict neurological recovery from\ncoma following cardiac arrest using clinical data and time-series such as\nmulti-channel EEG and ECG signals. Our modelling approach is multimodal, based\non two-dimensional spectrogram representations derived from numerous EEG\nchannels, alongside the integration of clinical data and features extracted\ndirectly from EEG recordings. Our submitted model achieved a Challenge score of\n$0.53$ on the hidden test set for predictions made $72$ hours after return of\nspontaneous circulation. Our study shows the efficacy and limitations of\nemploying transfer learning in medical classification. With regard to\nprospective implementation, our analysis reveals that the performance of the\nmodel is strongly linked to the selection of a decision threshold and exhibits\nstrong variability across data splits.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.06024v1",
    "title": "Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis\n  Diagnosis",
    "url": "http://arxiv.org/abs/2403.06024v1",
    "pdf_url": "http://arxiv.org/pdf/2403.06024v1",
    "authors": [
      "Zhe Huang",
      "Xiaowei Yu",
      "Benjamin S. Wessler",
      "Michael C. Hughes"
    ],
    "date": "2024-03-09",
    "summary": "Automated interpretation of ultrasound imaging of the heart (echocardiograms)\ncould improve the detection and treatment of aortic stenosis (AS), a deadly\nheart disease. However, existing deep learning pipelines for assessing AS from\nechocardiograms have two key limitations. First, most methods rely on limited\n2D cineloops, thereby ignoring widely available Doppler imaging that contains\nimportant complementary information about pressure gradients and blood flow\nabnormalities associated with AS. Second, obtaining labeled data is difficult.\nThere are often far more unlabeled echocardiogram recordings available, but\nthese remain underutilized by existing methods. To overcome these limitations,\nwe introduce Semi-supervised Multimodal Multiple-Instance Learning (SMMIL), a\nnew deep learning framework for automatic interpretation for structural heart\ndiseases like AS. When deployed, SMMIL can combine information from two input\nmodalities, spectral Dopplers and 2D cineloops, to produce a study-level AS\ndiagnosis. During training, SMMIL can combine a smaller labeled set and an\nabundant unlabeled set of both modalities to improve its classifier.\nExperiments demonstrate that SMMIL outperforms recent alternatives at 3-level\nAS severity classification as well as several clinically relevant AS detection\ntasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05912v2",
    "title": "Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic\n  Segmentation",
    "url": "http://arxiv.org/abs/2403.05912v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05912v2",
    "authors": [
      "Hairong Shi",
      "Songhao Han",
      "Shaofei Huang",
      "Yue Liao",
      "Guanbin Li",
      "Xiangxing Kong",
      "Hua Zhu",
      "Xiaomu Wang",
      "Si Liu"
    ],
    "date": "2024-03-09",
    "summary": "Tumor lesion segmentation on CT or MRI images plays a critical role in cancer\ndiagnosis and treatment planning. Considering the inherent differences in tumor\nlesion segmentation data across various medical imaging modalities and\nequipment, integrating medical knowledge into the Segment Anything Model (SAM)\npresents promising capability due to its versatility and generalization\npotential. Recent studies have attempted to enhance SAM with medical expertise\nby pre-training on large-scale medical segmentation datasets. However,\nchallenges still exist in 3D tumor lesion segmentation owing to tumor\ncomplexity and the imbalance in foreground and background regions. Therefore,\nwe introduce Mask-Enhanced SAM (M-SAM), an innovative architecture tailored for\n3D tumor lesion segmentation. We propose a novel Mask-Enhanced Adapter (MEA)\nwithin M-SAM that enriches the semantic information of medical images with\npositional data from coarse segmentation masks, facilitating the generation of\nmore precise segmentation masks. Furthermore, an iterative refinement scheme is\nimplemented in M-SAM to refine the segmentation masks progressively, leading to\nimproved performance. Extensive experiments on seven tumor lesion segmentation\ndatasets indicate that our M-SAM not only achieves high segmentation accuracy\nbut also exhibits robust generalization. The code is available at\nhttps://github.com/nanase1025/M-SAM.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Segment Anything"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05906v1",
    "title": "Segmentation Guided Sparse Transformer for Under-Display Camera Image\n  Restoration",
    "url": "http://arxiv.org/abs/2403.05906v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05906v1",
    "authors": [
      "Jingyun Xue",
      "Tao Wang",
      "Jun Wang",
      "Kaihao Zhang",
      "Wenhan Luo",
      "Wenqi Ren",
      "Zikun Liu",
      "Hyunhee Park",
      "Xiaochun Cao"
    ],
    "date": "2024-03-09",
    "summary": "Under-Display Camera (UDC) is an emerging technology that achieves\nfull-screen display via hiding the camera under the display panel. However, the\ncurrent implementation of UDC causes serious degradation. The incident light\nrequired for camera imaging undergoes attenuation and diffraction when passing\nthrough the display panel, leading to various artifacts in UDC imaging.\nPresently, the prevailing UDC image restoration methods predominantly utilize\nconvolutional neural network architectures, whereas Transformer-based methods\nhave exhibited superior performance in the majority of image restoration tasks.\nThis is attributed to the Transformer's capability to sample global features\nfor the local reconstruction of images, thereby achieving high-quality image\nrestoration. In this paper, we observe that when using the Vision Transformer\nfor UDC degraded image restoration, the global attention samples a large amount\nof redundant information and noise. Furthermore, compared to the ordinary\nTransformer employing dense attention, the Transformer utilizing sparse\nattention can alleviate the adverse impact of redundant information and noise.\nBuilding upon this discovery, we propose a Segmentation Guided Sparse\nTransformer method (SGSFormer) for the task of restoring high-quality images\nfrom UDC degraded images. Specifically, we utilize sparse self-attention to\nfilter out redundant information and noise, directing the model's attention to\nfocus on the features more relevant to the degraded regions in need of\nreconstruction. Moreover, we integrate the instance segmentation map as prior\ninformation to guide the sparse self-attention in filtering and focusing on the\ncorrect regions.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05842v1",
    "title": "Hufu: A Modality-Agnositc Watermarking System for Pre-Trained\n  Transformers via Permutation Equivariance",
    "url": "http://arxiv.org/abs/2403.05842v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05842v1",
    "authors": [
      "Hengyuan Xu",
      "Liyao Xiang",
      "Xingjun Ma",
      "Borui Yang",
      "Baochun Li"
    ],
    "date": "2024-03-09",
    "summary": "With the blossom of deep learning models and services, it has become an\nimperative concern to safeguard the valuable model parameters from being\nstolen. Watermarking is considered an important tool for ownership\nverification. However, current watermarking schemes are customized for\ndifferent models and tasks, hard to be integrated as an integrated intellectual\nprotection service. We propose Hufu, a modality-agnostic watermarking system\nfor pre-trained Transformer-based models, relying on the permutation\nequivariance property of Transformers. Hufu embeds watermark by fine-tuning the\npre-trained model on a set of data samples specifically permuted, and the\nembedded model essentially contains two sets of weights -- one for normal use\nand the other for watermark extraction which is triggered on permuted inputs.\nThe permutation equivariance ensures minimal interference between these two\nsets of model weights and thus high fidelity on downstream tasks. Since our\nmethod only depends on the model itself, it is naturally modality-agnostic,\ntask-independent, and trigger-sample-free. Extensive experiments on the\nstate-of-the-art vision Transformers, BERT, and GPT2 have demonstrated Hufu's\nsuperiority in meeting watermarking requirements including effectiveness,\nefficiency, fidelity, and robustness, showing its great potential to be\ndeployed as a uniform ownership verification service for various Transformers.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05808v2",
    "title": "Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with\n  Diffusion Model for Blind Image Super-Resolution",
    "url": "http://arxiv.org/abs/2403.05808v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05808v2",
    "authors": [
      "Junxiong Lin",
      "Yan Wang",
      "Zeng Tao",
      "Boyang Wang",
      "Qing Zhao",
      "Haorang Wang",
      "Xuan Tong",
      "Xinji Mai",
      "Yuxuan Lin",
      "Wei Song",
      "Jiawen Yu",
      "Shaoqi Yan",
      "Wenqiang Zhang"
    ],
    "date": "2024-03-09",
    "summary": "Pre-trained diffusion models utilized for image generation encapsulate a\nsubstantial reservoir of a priori knowledge pertaining to intricate textures.\nHarnessing the potential of leveraging this a priori knowledge in the context\nof image super-resolution presents a compelling avenue. Nonetheless, prevailing\ndiffusion-based methodologies presently overlook the constraints imposed by\ndegradation information on the diffusion process. Furthermore, these methods\nfail to consider the spatial variability inherent in the estimated blur kernel,\nstemming from factors such as motion jitter and out-of-focus elements in\nopen-environment scenarios. This oversight results in a notable deviation of\nthe image super-resolution effect from fundamental realities. To address these\nconcerns, we introduce a framework known as Adaptive Multi-modal Fusion of\n\\textbf{S}patially Variant Kernel Refinement with Diffusion Model for Blind\nImage \\textbf{S}uper-\\textbf{R}esolution (SSR). Within the SSR framework, we\npropose a Spatially Variant Kernel Refinement (SVKR) module. SVKR estimates a\nDepth-Informed Kernel, which takes the depth information into account and is\nspatially variant. Additionally, SVKR enhance the accuracy of depth information\nacquired from LR images, allowing for mutual enhancement between the depth map\nand blur kernel estimates. Finally, we introduce the Adaptive Multi-Modal\nFusion (AMF) module to align the information from three modalities:\nlow-resolution images, depth maps, and blur kernels. This alignment can\nconstrain the diffusion model to generate more authentic SR results.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05783v1",
    "title": "Large Generative Model Assisted 3D Semantic Communication",
    "url": "http://arxiv.org/abs/2403.05783v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05783v1",
    "authors": [
      "Feibo Jiang",
      "Yubo Peng",
      "Li Dong",
      "Kezhi Wang",
      "Kun Yang",
      "Cunhua Pan",
      "Xiaohu You"
    ],
    "date": "2024-03-09",
    "summary": "Semantic Communication (SC) is a novel paradigm for data transmission in 6G.\nHowever, there are several challenges posed when performing SC in 3D scenarios:\n1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain\nchannel estimation. To address these issues, we propose a Generative AI Model\nassisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor\n(3DSE), which employs generative AI models, including Segment Anything Model\n(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D\nscenario based on user requirements. The extracted 3D semantics are represented\nas multi-perspective images of the goal-oriented 3D object. Then, we present an\nAdaptive Semantic Compression Model (ASCM) for encoding these multi-perspective\nimages, in which we use a semantic encoder with two output heads to perform\nsemantic encoding and mask redundant semantics in the latent semantic space,\nrespectively. Next, we design a conditional Generative adversarial network and\nDiffusion model aided-Channel Estimation (GDCE) to estimate and refine the\nChannel State Information (CSI) of physical channels. Finally, simulation\nresults demonstrate the advantages of the proposed GAM-3DSC system in\neffectively transmitting the goal-oriented 3D scenario.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Segment Anything"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05702v1",
    "title": "Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis\n  from 3D OCT Imaging",
    "url": "http://arxiv.org/abs/2403.05702v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05702v1",
    "authors": [
      "Mona Ashtari-Majlan",
      "Mohammad Mahdi Dehshibi",
      "David Masip"
    ],
    "date": "2024-03-08",
    "summary": "Glaucoma, a leading cause of irreversible blindness, necessitates early\ndetection for accurate and timely intervention to prevent irreversible vision\nloss. In this study, we present a novel deep learning framework that leverages\nthe diagnostic value of 3D Optical Coherence Tomography (OCT) imaging for\nautomated glaucoma detection. In this framework, we integrate a pre-trained\nVision Transformer on retinal data for rich slice-wise feature extraction and a\nbidirectional Gated Recurrent Unit for capturing inter-slice spatial\ndependencies. This dual-component approach enables comprehensive analysis of\nlocal nuances and global structural integrity, crucial for accurate glaucoma\ndiagnosis. Experimental results on a large dataset demonstrate the superior\nperformance of the proposed method over state-of-the-art ones, achieving an\nF1-score of 93.58%, Matthews Correlation Coefficient (MCC) of 73.54%, and AUC\nof 95.24%. The framework's ability to leverage the valuable information in 3D\nOCT data holds significant potential for enhancing clinical decision support\nsystems and improving patient outcomes in glaucoma management.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05659v2",
    "title": "Audio-Synchronized Visual Animation",
    "url": "http://arxiv.org/abs/2403.05659v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05659v2",
    "authors": [
      "Lin Zhang",
      "Shentong Mo",
      "Yijing Zhang",
      "Pedro Morgado"
    ],
    "date": "2024-03-08",
    "summary": "Current visual generation methods can produce high quality videos guided by\ntexts. However, effectively controlling object dynamics remains a challenge.\nThis work explores audio as a cue to generate temporally synchronized image\nanimations. We introduce Audio Synchronized Visual Animation (ASVA), a task\nanimating a static image to demonstrate motion dynamics, temporally guided by\naudio clips across multiple classes. To this end, we present AVSync15, a\ndataset curated from VGGSound with videos featuring synchronized audio visual\nevents across 15 categories. We also present a diffusion model, AVSyncD,\ncapable of generating dynamic animations guided by audios. Extensive\nevaluations validate AVSync15 as a reliable benchmark for synchronized\ngeneration and demonstrate our models superior performance. We further explore\nAVSyncDs potential in a variety of audio synchronized generation tasks, from\ngenerating full videos without a base image to controlling object motions with\nvarious sounds. We hope our established benchmark can open new avenues for\ncontrollable visual generation. More videos on project webpage\nhttps://lzhangbj.github.io/projects/asva/asva.html.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05530v5",
    "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context",
    "url": "http://arxiv.org/abs/2403.05530v5",
    "pdf_url": "http://arxiv.org/pdf/2403.05530v5",
    "authors": [
      "Gemini Team",
      "Petko Georgiev",
      "Ving Ian Lei",
      "Ryan Burnell",
      "Libin Bai",
      "Anmol Gulati",
      "Garrett Tanzer",
      "Damien Vincent",
      "Zhufeng Pan",
      "Shibo Wang",
      "Soroosh Mariooryad",
      "Yifan Ding",
      "Xinyang Geng",
      "Fred Alcober",
      "Roy Frostig",
      "Mark Omernick",
      "Lexi Walker",
      "Cosmin Paduraru",
      "Christina Sorokin",
      "Andrea Tacchetti",
      "Colin Gaffney",
      "Samira Daruki",
      "Olcan Sercinoglu",
      "Zach Gleicher",
      "Juliette Love",
      "Paul Voigtlaender",
      "Rohan Jain",
      "Gabriela Surita",
      "Kareem Mohamed",
      "Rory Blevins",
      "Junwhan Ahn",
      "Tao Zhu",
      "Kornraphop Kawintiranon",
      "Orhan Firat",
      "Yiming Gu",
      "Yujing Zhang",
      "Matthew Rahtz",
      "Manaal Faruqui",
      "Natalie Clay",
      "Justin Gilmer",
      "JD Co-Reyes",
      "Ivo Penchev",
      "Rui Zhu",
      "Nobuyuki Morioka",
      "Kevin Hui",
      "Krishna Haridasan",
      "Victor Campos",
      "Mahdis Mahdieh",
      "Mandy Guo",
      "Samer Hassan",
      "Kevin Kilgour",
      "Arpi Vezer",
      "Heng-Tze Cheng",
      "Raoul de Liedekerke",
      "Siddharth Goyal",
      "Paul Barham",
      "DJ Strouse",
      "Seb Noury",
      "Jonas Adler",
      "Mukund Sundararajan",
      "Sharad Vikram",
      "Dmitry Lepikhin",
      "Michela Paganini",
      "Xavier Garcia",
      "Fan Yang",
      "Dasha Valter",
      "Maja Trebacz",
      "Kiran Vodrahalli",
      "Chulayuth Asawaroengchai",
      "Roman Ring",
      "Norbert Kalb",
      "Livio Baldini Soares",
      "Siddhartha Brahma",
      "David Steiner",
      "Tianhe Yu",
      "Fabian Mentzer",
      "Antoine He",
      "Lucas Gonzalez",
      "Bibo Xu",
      "Raphael Lopez Kaufman",
      "Laurent El Shafey",
      "Junhyuk Oh",
      "Tom Hennigan",
      "George van den Driessche",
      "Seth Odoom",
      "Mario Lucic",
      "Becca Roelofs",
      "Sid Lall",
      "Amit Marathe",
      "Betty Chan",
      "Santiago Ontanon",
      "Luheng He",
      "Denis Teplyashin",
      "Jonathan Lai",
      "Phil Crone",
      "Bogdan Damoc",
      "Lewis Ho",
      "Sebastian Riedel",
      "Karel Lenc",
      "Chih-Kuan Yeh",
      "Aakanksha Chowdhery",
      "Yang Xu",
      "Mehran Kazemi",
      "Ehsan Amid",
      "Anastasia Petrushkina",
      "Kevin Swersky",
      "Ali Khodaei",
      "Gowoon Chen",
      "Chris Larkin",
      "Mario Pinto",
      "Geng Yan",
      "Adria Puigdomenech Badia",
      "Piyush Patil",
      "Steven Hansen",
      "Dave Orr",
      "Sebastien M. R. Arnold",
      "Jordan Grimstad",
      "Andrew Dai",
      "Sholto Douglas",
      "Rishika Sinha",
      "Vikas Yadav",
      "Xi Chen",
      "Elena Gribovskaya",
      "Jacob Austin",
      "Jeffrey Zhao",
      "Kaushal Patel",
      "Paul Komarek",
      "Sophia Austin",
      "Sebastian Borgeaud",
      "Linda Friso",
      "Abhimanyu Goyal",
      "Ben Caine",
      "Kris Cao",
      "Da-Woon Chung",
      "Matthew Lamm",
      "Gabe Barth-Maron",
      "Thais Kagohara",
      "Kate Olszewska",
      "Mia Chen",
      "Kaushik Shivakumar",
      "Rishabh Agarwal",
      "Harshal Godhia",
      "Ravi Rajwar",
      "Javier Snaider",
      "Xerxes Dotiwalla",
      "Yuan Liu",
      "Aditya Barua",
      "Victor Ungureanu",
      "Yuan Zhang",
      "Bat-Orgil Batsaikhan",
      "Mateo Wirth",
      "James Qin",
      "Ivo Danihelka",
      "Tulsee Doshi",
      "Martin Chadwick",
      "Jilin Chen",
      "Sanil Jain",
      "Quoc Le",
      "Arjun Kar",
      "Madhu Gurumurthy",
      "Cheng Li",
      "Ruoxin Sang",
      "Fangyu Liu",
      "Lampros Lamprou",
      "Rich Munoz",
      "Nathan Lintz",
      "Harsh Mehta",
      "Heidi Howard",
      "Malcolm Reynolds",
      "Lora Aroyo",
      "Quan Wang",
      "Lorenzo Blanco",
      "Albin Cassirer",
      "Jordan Griffith",
      "Dipanjan Das",
      "Stephan Lee",
      "Jakub Sygnowski",
      "Zach Fisher",
      "James Besley",
      "Richard Powell",
      "Zafarali Ahmed",
      "Dominik Paulus",
      "David Reitter",
      "Zalan Borsos",
      "Rishabh Joshi",
      "Aedan Pope",
      "Steven Hand",
      "Vittorio Selo",
      "Vihan Jain",
      "Nikhil Sethi",
      "Megha Goel",
      "Takaki Makino",
      "Rhys May",
      "Zhen Yang",
      "Johan Schalkwyk",
      "Christina Butterfield",
      "Anja Hauth",
      "Alex Goldin",
      "Will Hawkins",
      "Evan Senter",
      "Sergey Brin",
      "Oliver Woodman",
      "Marvin Ritter",
      "Eric Noland",
      "Minh Giang",
      "Vijay Bolina",
      "Lisa Lee",
      "Tim Blyth",
      "Ian Mackinnon",
      "Machel Reid",
      "Obaid Sarvana",
      "David Silver",
      "Alexander Chen",
      "Lily Wang",
      "Loren Maggiore",
      "Oscar Chang",
      "Nithya Attaluri",
      "Gregory Thornton",
      "Chung-Cheng Chiu",
      "Oskar Bunyan",
      "Nir Levine",
      "Timothy Chung",
      "Evgenii Eltyshev",
      "Xiance Si",
      "Timothy Lillicrap",
      "Demetra Brady",
      "Vaibhav Aggarwal",
      "Boxi Wu",
      "Yuanzhong Xu",
      "Ross McIlroy",
      "Kartikeya Badola",
      "Paramjit Sandhu",
      "Erica Moreira",
      "Wojciech Stokowiec",
      "Ross Hemsley",
      "Dong Li",
      "Alex Tudor",
      "Pranav Shyam",
      "Elahe Rahimtoroghi",
      "Salem Haykal",
      "Pablo Sprechmann",
      "Xiang Zhou",
      "Diana Mincu",
      "Yujia Li",
      "Ravi Addanki",
      "Kalpesh Krishna",
      "Xiao Wu",
      "Alexandre Frechette",
      "Matan Eyal",
      "Allan Dafoe",
      "Dave Lacey",
      "Jay Whang",
      "Thi Avrahami",
      "Ye Zhang",
      "Emanuel Taropa",
      "Hanzhao Lin",
      "Daniel Toyama",
      "Eliza Rutherford",
      "Motoki Sano",
      "HyunJeong Choe",
      "Alex Tomala",
      "Chalence Safranek-Shrader",
      "Nora Kassner",
      "Mantas Pajarskas",
      "Matt Harvey",
      "Sean Sechrist",
      "Meire Fortunato",
      "Christina Lyu",
      "Gamaleldin Elsayed",
      "Chenkai Kuang",
      "James Lottes",
      "Eric Chu",
      "Chao Jia",
      "Chih-Wei Chen",
      "Peter Humphreys",
      "Kate Baumli",
      "Connie Tao",
      "Rajkumar Samuel",
      "Cicero Nogueira dos Santos",
      "Anders Andreassen",
      "Nemanja Raki\u0107evi\u0107",
      "Dominik Grewe",
      "Aviral Kumar",
      "Stephanie Winkler",
      "Jonathan Caton",
      "Andrew Brock",
      "Sid Dalmia",
      "Hannah Sheahan",
      "Iain Barr",
      "Yingjie Miao",
      "Paul Natsev",
      "Jacob Devlin",
      "Feryal Behbahani",
      "Flavien Prost",
      "Yanhua Sun",
      "Artiom Myaskovsky",
      "Thanumalayan Sankaranarayana Pillai",
      "Dan Hurt",
      "Angeliki Lazaridou",
      "Xi Xiong",
      "Ce Zheng",
      "Fabio Pardo",
      "Xiaowei Li",
      "Dan Horgan",
      "Joe Stanton",
      "Moran Ambar",
      "Fei Xia",
      "Alejandro Lince",
      "Mingqiu Wang",
      "Basil Mustafa",
      "Albert Webson",
      "Hyo Lee",
      "Rohan Anil",
      "Martin Wicke",
      "Timothy Dozat",
      "Abhishek Sinha",
      "Enrique Piqueras",
      "Elahe Dabir",
      "Shyam Upadhyay",
      "Anudhyan Boral",
      "Lisa Anne Hendricks",
      "Corey Fry",
      "Josip Djolonga",
      "Yi Su",
      "Jake Walker",
      "Jane Labanowski",
      "Ronny Huang",
      "Vedant Misra",
      "Jeremy Chen",
      "RJ Skerry-Ryan",
      "Avi Singh",
      "Shruti Rijhwani",
      "Dian Yu",
      "Alex Castro-Ros",
      "Beer Changpinyo",
      "Romina Datta",
      "Sumit Bagri",
      "Arnar Mar Hrafnkelsson",
      "Marcello Maggioni",
      "Daniel Zheng",
      "Yury Sulsky",
      "Shaobo Hou",
      "Tom Le Paine",
      "Antoine Yang",
      "Jason Riesa",
      "Dominika Rogozinska",
      "Dror Marcus",
      "Dalia El Badawy",
      "Qiao Zhang",
      "Luyu Wang",
      "Helen Miller",
      "Jeremy Greer",
      "Lars Lowe Sjos",
      "Azade Nova",
      "Heiga Zen",
      "Rahma Chaabouni",
      "Mihaela Rosca",
      "Jiepu Jiang",
      "Charlie Chen",
      "Ruibo Liu",
      "Tara Sainath",
      "Maxim Krikun",
      "Alex Polozov",
      "Jean-Baptiste Lespiau",
      "Josh Newlan",
      "Zeyncep Cankara",
      "Soo Kwak",
      "Yunhan Xu",
      "Phil Chen",
      "Andy Coenen",
      "Clemens Meyer",
      "Katerina Tsihlas",
      "Ada Ma",
      "Juraj Gottweis",
      "Jinwei Xing",
      "Chenjie Gu",
      "Jin Miao",
      "Christian Frank",
      "Zeynep Cankara",
      "Sanjay Ganapathy",
      "Ishita Dasgupta",
      "Steph Hughes-Fitt",
      "Heng Chen",
      "David Reid",
      "Keran Rong",
      "Hongmin Fan",
      "Joost van Amersfoort",
      "Vincent Zhuang",
      "Aaron Cohen",
      "Shixiang Shane Gu",
      "Anhad Mohananey",
      "Anastasija Ilic",
      "Taylor Tobin",
      "John Wieting",
      "Anna Bortsova",
      "Phoebe Thacker",
      "Emma Wang",
      "Emily Caveness",
      "Justin Chiu",
      "Eren Sezener",
      "Alex Kaskasoli",
      "Steven Baker",
      "Katie Millican",
      "Mohamed Elhawaty",
      "Kostas Aisopos",
      "Carl Lebsack",
      "Nathan Byrd",
      "Hanjun Dai",
      "Wenhao Jia",
      "Matthew Wiethoff",
      "Elnaz Davoodi",
      "Albert Weston",
      "Lakshman Yagati",
      "Arun Ahuja",
      "Isabel Gao",
      "Golan Pundak",
      "Susan Zhang",
      "Michael Azzam",
      "Khe Chai Sim",
      "Sergi Caelles",
      "James Keeling",
      "Abhanshu Sharma",
      "Andy Swing",
      "YaGuang Li",
      "Chenxi Liu",
      "Carrie Grimes Bostock",
      "Yamini Bansal",
      "Zachary Nado",
      "Ankesh Anand",
      "Josh Lipschultz",
      "Abhijit Karmarkar",
      "Lev Proleev",
      "Abe Ittycheriah",
      "Soheil Hassas Yeganeh",
      "George Polovets",
      "Aleksandra Faust",
      "Jiao Sun",
      "Alban Rrustemi",
      "Pen Li",
      "Rakesh Shivanna",
      "Jeremiah Liu",
      "Chris Welty",
      "Federico Lebron",
      "Anirudh Baddepudi",
      "Sebastian Krause",
      "Emilio Parisotto",
      "Radu Soricut",
      "Zheng Xu",
      "Dawn Bloxwich",
      "Melvin Johnson",
      "Behnam Neyshabur",
      "Justin Mao-Jones",
      "Renshen Wang",
      "Vinay Ramasesh",
      "Zaheer Abbas",
      "Arthur Guez",
      "Constant Segal",
      "Duc Dung Nguyen",
      "James Svensson",
      "Le Hou",
      "Sarah York",
      "Kieran Milan",
      "Sophie Bridgers",
      "Wiktor Gworek",
      "Marco Tagliasacchi",
      "James Lee-Thorp",
      "Michael Chang",
      "Alexey Guseynov",
      "Ale Jakse Hartman",
      "Michael Kwong",
      "Ruizhe Zhao",
      "Sheleem Kashem",
      "Elizabeth Cole",
      "Antoine Miech",
      "Richard Tanburn",
      "Mary Phuong",
      "Filip Pavetic",
      "Sebastien Cevey",
      "Ramona Comanescu",
      "Richard Ives",
      "Sherry Yang",
      "Cosmo Du",
      "Bo Li",
      "Zizhao Zhang",
      "Mariko Iinuma",
      "Clara Huiyi Hu",
      "Aurko Roy",
      "Shaan Bijwadia",
      "Zhenkai Zhu",
      "Danilo Martins",
      "Rachel Saputro",
      "Anita Gergely",
      "Steven Zheng",
      "Dawei Jia",
      "Ioannis Antonoglou",
      "Adam Sadovsky",
      "Shane Gu",
      "Yingying Bi",
      "Alek Andreev",
      "Sina Samangooei",
      "Mina Khan",
      "Tomas Kocisky",
      "Angelos Filos",
      "Chintu Kumar",
      "Colton Bishop",
      "Adams Yu",
      "Sarah Hodkinson",
      "Sid Mittal",
      "Premal Shah",
      "Alexandre Moufarek",
      "Yong Cheng",
      "Adam Bloniarz",
      "Jaehoon Lee",
      "Pedram Pejman",
      "Paul Michel",
      "Stephen Spencer",
      "Vladimir Feinberg",
      "Xuehan Xiong",
      "Nikolay Savinov",
      "Charlotte Smith",
      "Siamak Shakeri",
      "Dustin Tran",
      "Mary Chesus",
      "Bernd Bohnet",
      "George Tucker",
      "Tamara von Glehn",
      "Carrie Muir",
      "Yiran Mao",
      "Hideto Kazawa",
      "Ambrose Slone",
      "Kedar Soparkar",
      "Disha Shrivastava",
      "James Cobon-Kerr",
      "Michael Sharman",
      "Jay Pavagadhi",
      "Carlos Araya",
      "Karolis Misiunas",
      "Nimesh Ghelani",
      "Michael Laskin",
      "David Barker",
      "Qiujia Li",
      "Anton Briukhov",
      "Neil Houlsby",
      "Mia Glaese",
      "Balaji Lakshminarayanan",
      "Nathan Schucher",
      "Yunhao Tang",
      "Eli Collins",
      "Hyeontaek Lim",
      "Fangxiaoyu Feng",
      "Adria Recasens",
      "Guangda Lai",
      "Alberto Magni",
      "Nicola De Cao",
      "Aditya Siddhant",
      "Zoe Ashwood",
      "Jordi Orbay",
      "Mostafa Dehghani",
      "Jenny Brennan",
      "Yifan He",
      "Kelvin Xu",
      "Yang Gao",
      "Carl Saroufim",
      "James Molloy",
      "Xinyi Wu",
      "Seb Arnold",
      "Solomon Chang",
      "Julian Schrittwieser",
      "Elena Buchatskaya",
      "Soroush Radpour",
      "Martin Polacek",
      "Skye Giordano",
      "Ankur Bapna",
      "Simon Tokumine",
      "Vincent Hellendoorn",
      "Thibault Sottiaux",
      "Sarah Cogan",
      "Aliaksei Severyn",
      "Mohammad Saleh",
      "Shantanu Thakoor",
      "Laurent Shefey",
      "Siyuan Qiao",
      "Meenu Gaba",
      "Shuo-yiin Chang",
      "Craig Swanson",
      "Biao Zhang",
      "Benjamin Lee",
      "Paul Kishan Rubenstein",
      "Gan Song",
      "Tom Kwiatkowski",
      "Anna Koop",
      "Ajay Kannan",
      "David Kao",
      "Parker Schuh",
      "Axel Stjerngren",
      "Golnaz Ghiasi",
      "Gena Gibson",
      "Luke Vilnis",
      "Ye Yuan",
      "Felipe Tiengo Ferreira",
      "Aishwarya Kamath",
      "Ted Klimenko",
      "Ken Franko",
      "Kefan Xiao",
      "Indro Bhattacharya",
      "Miteyan Patel",
      "Rui Wang",
      "Alex Morris",
      "Robin Strudel",
      "Vivek Sharma",
      "Peter Choy",
      "Sayed Hadi Hashemi",
      "Jessica Landon",
      "Mara Finkelstein",
      "Priya Jhakra",
      "Justin Frye",
      "Megan Barnes",
      "Matthew Mauger",
      "Dennis Daun",
      "Khuslen Baatarsukh",
      "Matthew Tung",
      "Wael Farhan",
      "Henryk Michalewski",
      "Fabio Viola",
      "Felix de Chaumont Quitry",
      "Charline Le Lan",
      "Tom Hudson",
      "Qingze Wang",
      "Felix Fischer",
      "Ivy Zheng",
      "Elspeth White",
      "Anca Dragan",
      "Jean-baptiste Alayrac",
      "Eric Ni",
      "Alexander Pritzel",
      "Adam Iwanicki",
      "Michael Isard",
      "Anna Bulanova",
      "Lukas Zilka",
      "Ethan Dyer",
      "Devendra Sachan",
      "Srivatsan Srinivasan",
      "Hannah Muckenhirn",
      "Honglong Cai",
      "Amol Mandhane",
      "Mukarram Tariq",
      "Jack W. Rae",
      "Gary Wang",
      "Kareem Ayoub",
      "Nicholas FitzGerald",
      "Yao Zhao",
      "Woohyun Han",
      "Chris Alberti",
      "Dan Garrette",
      "Kashyap Krishnakumar",
      "Mai Gimenez",
      "Anselm Levskaya",
      "Daniel Sohn",
      "Josip Matak",
      "Inaki Iturrate",
      "Michael B. Chang",
      "Jackie Xiang",
      "Yuan Cao",
      "Nishant Ranka",
      "Geoff Brown",
      "Adrian Hutter",
      "Vahab Mirrokni",
      "Nanxin Chen",
      "Kaisheng Yao",
      "Zoltan Egyed",
      "Francois Galilee",
      "Tyler Liechty",
      "Praveen Kallakuri",
      "Evan Palmer",
      "Sanjay Ghemawat",
      "Jasmine Liu",
      "David Tao",
      "Chloe Thornton",
      "Tim Green",
      "Mimi Jasarevic",
      "Sharon Lin",
      "Victor Cotruta",
      "Yi-Xuan Tan",
      "Noah Fiedel",
      "Hongkun Yu",
      "Ed Chi",
      "Alexander Neitz",
      "Jens Heitkaemper",
      "Anu Sinha",
      "Denny Zhou",
      "Yi Sun",
      "Charbel Kaed",
      "Brice Hulse",
      "Swaroop Mishra",
      "Maria Georgaki",
      "Sneha Kudugunta",
      "Clement Farabet",
      "Izhak Shafran",
      "Daniel Vlasic",
      "Anton Tsitsulin",
      "Rajagopal Ananthanarayanan",
      "Alen Carin",
      "Guolong Su",
      "Pei Sun",
      "Shashank V",
      "Gabriel Carvajal",
      "Josef Broder",
      "Iulia Comsa",
      "Alena Repina",
      "William Wong",
      "Warren Weilun Chen",
      "Peter Hawkins",
      "Egor Filonov",
      "Lucia Loher",
      "Christoph Hirnschall",
      "Weiyi Wang",
      "Jingchen Ye",
      "Andrea Burns",
      "Hardie Cate",
      "Diana Gage Wright",
      "Federico Piccinini",
      "Lei Zhang",
      "Chu-Cheng Lin",
      "Ionel Gog",
      "Yana Kulizhskaya",
      "Ashwin Sreevatsa",
      "Shuang Song",
      "Luis C. Cobo",
      "Anand Iyer",
      "Chetan Tekur",
      "Guillermo Garrido",
      "Zhuyun Xiao",
      "Rupert Kemp",
      "Huaixiu Steven Zheng",
      "Hui Li",
      "Ananth Agarwal",
      "Christel Ngani",
      "Kati Goshvadi",
      "Rebeca Santamaria-Fernandez",
      "Wojciech Fica",
      "Xinyun Chen",
      "Chris Gorgolewski",
      "Sean Sun",
      "Roopal Garg",
      "Xinyu Ye",
      "S. M. Ali Eslami",
      "Nan Hua",
      "Jon Simon",
      "Pratik Joshi",
      "Yelin Kim",
      "Ian Tenney",
      "Sahitya Potluri",
      "Lam Nguyen Thiet",
      "Quan Yuan",
      "Florian Luisier",
      "Alexandra Chronopoulou",
      "Salvatore Scellato",
      "Praveen Srinivasan",
      "Minmin Chen",
      "Vinod Koverkathu",
      "Valentin Dalibard",
      "Yaming Xu",
      "Brennan Saeta",
      "Keith Anderson",
      "Thibault Sellam",
      "Nick Fernando",
      "Fantine Huot",
      "Junehyuk Jung",
      "Mani Varadarajan",
      "Michael Quinn",
      "Amit Raul",
      "Maigo Le",
      "Ruslan Habalov",
      "Jon Clark",
      "Komal Jalan",
      "Kalesha Bullard",
      "Achintya Singhal",
      "Thang Luong",
      "Boyu Wang",
      "Sujeevan Rajayogam",
      "Julian Eisenschlos",
      "Johnson Jia",
      "Daniel Finchelstein",
      "Alex Yakubovich",
      "Daniel Balle",
      "Michael Fink",
      "Sameer Agarwal",
      "Jing Li",
      "Dj Dvijotham",
      "Shalini Pal",
      "Kai Kang",
      "Jaclyn Konzelmann",
      "Jennifer Beattie",
      "Olivier Dousse",
      "Diane Wu",
      "Remi Crocker",
      "Chen Elkind",
      "Siddhartha Reddy Jonnalagadda",
      "Jong Lee",
      "Dan Holtmann-Rice",
      "Krystal Kallarackal",
      "Rosanne Liu",
      "Denis Vnukov",
      "Neera Vats",
      "Luca Invernizzi",
      "Mohsen Jafari",
      "Huanjie Zhou",
      "Lilly Taylor",
      "Jennifer Prendki",
      "Marcus Wu",
      "Tom Eccles",
      "Tianqi Liu",
      "Kavya Kopparapu",
      "Francoise Beaufays",
      "Christof Angermueller",
      "Andreea Marzoca",
      "Shourya Sarcar",
      "Hilal Dib",
      "Jeff Stanway",
      "Frank Perbet",
      "Nejc Trdin",
      "Rachel Sterneck",
      "Andrey Khorlin",
      "Dinghua Li",
      "Xihui Wu",
      "Sonam Goenka",
      "David Madras",
      "Sasha Goldshtein",
      "Willi Gierke",
      "Tong Zhou",
      "Yaxin Liu",
      "Yannie Liang",
      "Anais White",
      "Yunjie Li",
      "Shreya Singh",
      "Sanaz Bahargam",
      "Mark Epstein",
      "Sujoy Basu",
      "Li Lao",
      "Adnan Ozturel",
      "Carl Crous",
      "Alex Zhai",
      "Han Lu",
      "Zora Tung",
      "Neeraj Gaur",
      "Alanna Walton",
      "Lucas Dixon",
      "Ming Zhang",
      "Amir Globerson",
      "Grant Uy",
      "Andrew Bolt",
      "Olivia Wiles",
      "Milad Nasr",
      "Ilia Shumailov",
      "Marco Selvi",
      "Francesco Piccinno",
      "Ricardo Aguilar",
      "Sara McCarthy",
      "Misha Khalman",
      "Mrinal Shukla",
      "Vlado Galic",
      "John Carpenter",
      "Kevin Villela",
      "Haibin Zhang",
      "Harry Richardson",
      "James Martens",
      "Matko Bosnjak",
      "Shreyas Rammohan Belle",
      "Jeff Seibert",
      "Mahmoud Alnahlawi",
      "Brian McWilliams",
      "Sankalp Singh",
      "Annie Louis",
      "Wen Ding",
      "Dan Popovici",
      "Lenin Simicich",
      "Laura Knight",
      "Pulkit Mehta",
      "Nishesh Gupta",
      "Chongyang Shi",
      "Saaber Fatehi",
      "Jovana Mitrovic",
      "Alex Grills",
      "Joseph Pagadora",
      "Tsendsuren Munkhdalai",
      "Dessie Petrova",
      "Danielle Eisenbud",
      "Zhishuai Zhang",
      "Damion Yates",
      "Bhavishya Mittal",
      "Nilesh Tripuraneni",
      "Yannis Assael",
      "Thomas Brovelli",
      "Prateek Jain",
      "Mihajlo Velimirovic",
      "Canfer Akbulut",
      "Jiaqi Mu",
      "Wolfgang Macherey",
      "Ravin Kumar",
      "Jun Xu",
      "Haroon Qureshi",
      "Gheorghe Comanici",
      "Jeremy Wiesner",
      "Zhitao Gong",
      "Anton Ruddock",
      "Matthias Bauer",
      "Nick Felt",
      "Anirudh GP",
      "Anurag Arnab",
      "Dustin Zelle",
      "Jonas Rothfuss",
      "Bill Rosgen",
      "Ashish Shenoy",
      "Bryan Seybold",
      "Xinjian Li",
      "Jayaram Mudigonda",
      "Goker Erdogan",
      "Jiawei Xia",
      "Jiri Simsa",
      "Andrea Michi",
      "Yi Yao",
      "Christopher Yew",
      "Steven Kan",
      "Isaac Caswell",
      "Carey Radebaugh",
      "Andre Elisseeff",
      "Pedro Valenzuela",
      "Kay McKinney",
      "Kim Paterson",
      "Albert Cui",
      "Eri Latorre-Chimoto",
      "Solomon Kim",
      "William Zeng",
      "Ken Durden",
      "Priya Ponnapalli",
      "Tiberiu Sosea",
      "Christopher A. Choquette-Choo",
      "James Manyika",
      "Brona Robenek",
      "Harsha Vashisht",
      "Sebastien Pereira",
      "Hoi Lam",
      "Marko Velic",
      "Denese Owusu-Afriyie",
      "Katherine Lee",
      "Tolga Bolukbasi",
      "Alicia Parrish",
      "Shawn Lu",
      "Jane Park",
      "Balaji Venkatraman",
      "Alice Talbert",
      "Lambert Rosique",
      "Yuchung Cheng",
      "Andrei Sozanschi",
      "Adam Paszke",
      "Praveen Kumar",
      "Jessica Austin",
      "Lu Li",
      "Khalid Salama",
      "Bartek Perz",
      "Wooyeol Kim",
      "Nandita Dukkipati",
      "Anthony Baryshnikov",
      "Christos Kaplanis",
      "XiangHai Sheng",
      "Yuri Chervonyi",
      "Caglar Unlu",
      "Diego de Las Casas",
      "Harry Askham",
      "Kathryn Tunyasuvunakool",
      "Felix Gimeno",
      "Siim Poder",
      "Chester Kwak",
      "Matt Miecnikowski",
      "Vahab Mirrokni",
      "Alek Dimitriev",
      "Aaron Parisi",
      "Dangyi Liu",
      "Tomy Tsai",
      "Toby Shevlane",
      "Christina Kouridi",
      "Drew Garmon",
      "Adrian Goedeckemeyer",
      "Adam R. Brown",
      "Anitha Vijayakumar",
      "Ali Elqursh",
      "Sadegh Jazayeri",
      "Jin Huang",
      "Sara Mc Carthy",
      "Jay Hoover",
      "Lucy Kim",
      "Sandeep Kumar",
      "Wei Chen",
      "Courtney Biles",
      "Garrett Bingham",
      "Evan Rosen",
      "Lisa Wang",
      "Qijun Tan",
      "David Engel",
      "Francesco Pongetti",
      "Dario de Cesare",
      "Dongseong Hwang",
      "Lily Yu",
      "Jennifer Pullman",
      "Srini Narayanan",
      "Kyle Levin",
      "Siddharth Gopal",
      "Megan Li",
      "Asaf Aharoni",
      "Trieu Trinh",
      "Jessica Lo",
      "Norman Casagrande",
      "Roopali Vij",
      "Loic Matthey",
      "Bramandia Ramadhana",
      "Austin Matthews",
      "CJ Carey",
      "Matthew Johnson",
      "Kremena Goranova",
      "Rohin Shah",
      "Shereen Ashraf",
      "Kingshuk Dasgupta",
      "Rasmus Larsen",
      "Yicheng Wang",
      "Manish Reddy Vuyyuru",
      "Chong Jiang",
      "Joana Ijazi",
      "Kazuki Osawa",
      "Celine Smith",
      "Ramya Sree Boppana",
      "Taylan Bilal",
      "Yuma Koizumi",
      "Ying Xu",
      "Yasemin Altun",
      "Nir Shabat",
      "Ben Bariach",
      "Alex Korchemniy",
      "Kiam Choo",
      "Olaf Ronneberger",
      "Chimezie Iwuanyanwu",
      "Shubin Zhao",
      "David Soergel",
      "Cho-Jui Hsieh",
      "Irene Cai",
      "Shariq Iqbal",
      "Martin Sundermeyer",
      "Zhe Chen",
      "Elie Bursztein",
      "Chaitanya Malaviya",
      "Fadi Biadsy",
      "Prakash Shroff",
      "Inderjit Dhillon",
      "Tejasi Latkar",
      "Chris Dyer",
      "Hannah Forbes",
      "Massimo Nicosia",
      "Vitaly Nikolaev",
      "Somer Greene",
      "Marin Georgiev",
      "Pidong Wang",
      "Nina Martin",
      "Hanie Sedghi",
      "John Zhang",
      "Praseem Banzal",
      "Doug Fritz",
      "Vikram Rao",
      "Xuezhi Wang",
      "Jiageng Zhang",
      "Viorica Patraucean",
      "Dayou Du",
      "Igor Mordatch",
      "Ivan Jurin",
      "Lewis Liu",
      "Ayush Dubey",
      "Abhi Mohan",
      "Janek Nowakowski",
      "Vlad-Doru Ion",
      "Nan Wei",
      "Reiko Tojo",
      "Maria Abi Raad",
      "Drew A. Hudson",
      "Vaishakh Keshava",
      "Shubham Agrawal",
      "Kevin Ramirez",
      "Zhichun Wu",
      "Hoang Nguyen",
      "Ji Liu",
      "Madhavi Sewak",
      "Bryce Petrini",
      "DongHyun Choi",
      "Ivan Philips",
      "Ziyue Wang",
      "Ioana Bica",
      "Ankush Garg",
      "Jarek Wilkiewicz",
      "Priyanka Agrawal",
      "Xiaowei Li",
      "Danhao Guo",
      "Emily Xue",
      "Naseer Shaik",
      "Andrew Leach",
      "Sadh MNM Khan",
      "Julia Wiesinger",
      "Sammy Jerome",
      "Abhishek Chakladar",
      "Alek Wenjiao Wang",
      "Tina Ornduff",
      "Folake Abu",
      "Alireza Ghaffarkhah",
      "Marcus Wainwright",
      "Mario Cortes",
      "Frederick Liu",
      "Joshua Maynez",
      "Andreas Terzis",
      "Pouya Samangouei",
      "Riham Mansour",
      "Tomasz K\u0119pa",
      "Fran\u00e7ois-Xavier Aubet",
      "Anton Algymr",
      "Dan Banica",
      "Agoston Weisz",
      "Andras Orban",
      "Alexandre Senges",
      "Ewa Andrejczuk",
      "Mark Geller",
      "Niccolo Dal Santo",
      "Valentin Anklin",
      "Majd Al Merey",
      "Martin Baeuml",
      "Trevor Strohman",
      "Junwen Bai",
      "Slav Petrov",
      "Yonghui Wu",
      "Demis Hassabis",
      "Koray Kavukcuoglu",
      "Jeff Dean",
      "Oriol Vinyals"
    ],
    "date": "2024-03-08",
    "summary": "In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05525v2",
    "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
    "url": "http://arxiv.org/abs/2403.05525v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05525v2",
    "authors": [
      "Haoyu Lu",
      "Wen Liu",
      "Bo Zhang",
      "Bingxuan Wang",
      "Kai Dong",
      "Bo Liu",
      "Jingxiang Sun",
      "Tongzheng Ren",
      "Zhuoshu Li",
      "Hao Yang",
      "Yaofeng Sun",
      "Chengqi Deng",
      "Hanwei Xu",
      "Zhenda Xie",
      "Chong Ruan"
    ],
    "date": "2024-03-08",
    "summary": "We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed\nfor real-world vision and language understanding applications. Our approach is\nstructured around three key dimensions:\n  We strive to ensure our data is diverse, scalable, and extensively covers\nreal-world scenarios including web screenshots, PDFs, OCR, charts, and\nknowledge-based content, aiming for a comprehensive representation of practical\ncontexts. Further, we create a use case taxonomy from real user scenarios and\nconstruct an instruction tuning dataset accordingly. The fine-tuning with this\ndataset substantially improves the model's user experience in practical\napplications. Considering efficiency and the demands of most real-world\nscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently\nprocesses high-resolution images (1024 x 1024), while maintaining a relatively\nlow computational overhead. This design choice ensures the model's ability to\ncapture critical semantic and detailed information across various visual tasks.\nWe posit that a proficient Vision-Language Model should, foremost, possess\nstrong language abilities. To ensure the preservation of LLM capabilities\nduring pretraining, we investigate an effective VL pretraining strategy by\nintegrating LLM training from the beginning and carefully managing the\ncompetitive dynamics observed between vision and language modalities.\n  The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user\nexperiences as a vision-language chatbot in real-world applications, achieving\nstate-of-the-art or competitive performance across a wide range of\nvisual-language benchmarks at the same model size while maintaining robust\nperformance on language-centric benchmarks. We have made both 1.3B and 7B\nmodels publicly accessible to foster innovations based on this foundation\nmodel.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05523v2",
    "title": "Beyond Finite Data: Towards Data-free Out-of-distribution Generalization\n  via Extrapolation",
    "url": "http://arxiv.org/abs/2403.05523v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05523v2",
    "authors": [
      "Yijiang Li",
      "Sucheng Ren",
      "Weipeng Deng",
      "Yuzhi Xu",
      "Ying Gao",
      "Edith Ngai",
      "Haohan Wang"
    ],
    "date": "2024-03-08",
    "summary": "Out-of-distribution (OOD) generalization is a favorable yet challenging\nproperty for deep neural networks. The core challenges lie in the limited\navailability of source domains that help models learn an invariant\nrepresentation from the spurious features. Various domain augmentation have\nbeen proposed but largely rely on interpolating existing domains and frequently\nface difficulties in creating truly \"novel\" domains. Humans, on the other hand,\ncan easily extrapolate novel domains, thus, an intriguing question arises: How\ncan neural networks extrapolate like humans and achieve OOD generalization?\n  We introduce a novel approach to domain extrapolation that leverages\nreasoning ability and the extensive knowledge encapsulated within large\nlanguage models (LLMs) to synthesize entirely new domains. Starting with the\nclass of interest, we query the LLMs to extract relevant knowledge for these\nnovel domains. We then bridge the gap between the text-centric knowledge\nderived from LLMs and the pixel input space of the model using text-to-image\ngeneration techniques. By augmenting the training set of domain generalization\ndatasets with high-fidelity, photo-realistic images of these new domains, we\nachieve significant improvements over all existing methods, as demonstrated in\nboth single and multi-domain generalization across various benchmarks.\n  With the ability to extrapolate any domains for any class, our method has the\npotential to learn a generalized model for any task without any data. To\nillustrate, we put forth a much more difficult setting termed, data-free domain\ngeneralization, that aims to learn a generalized model in the absence of any\ncollected data. Our empirical findings support the above argument and our\nmethods exhibit commendable performance in this setting, even surpassing the\nsupervised setting by approximately 1-2\\% on datasets such as VLCS.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05489v2",
    "title": "JointMotion: Joint Self-Supervision for Joint Motion Prediction",
    "url": "http://arxiv.org/abs/2403.05489v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05489v2",
    "authors": [
      "Royden Wagner",
      "Omer Sahin Tas",
      "Marvin Klemp",
      "Carlos Fernandez"
    ],
    "date": "2024-03-08",
    "summary": "We present JointMotion, a self-supervised pre-training method for joint\nmotion prediction in self-driving vehicles. Our method jointly optimizes a\nscene-level objective connecting motion and environments, and an instance-level\nobjective to refine learned representations. Scene-level representations are\nlearned via non-contrastive similarity learning of past motion sequences and\nenvironment context. At the instance level, we use masked autoencoding to\nrefine multimodal polyline representations. We complement this with an adaptive\npre-training decoder that enables JointMotion to generalize across different\nenvironment representations, fusion mechanisms, and dataset characteristics.\nNotably, our method reduces the joint final displacement error of Wayformer,\nHPTR, and Scene Transformer models by 3\\%, 8\\%, and 12\\%, respectively; and\nenables transfer learning between the Waymo Open Motion and the Argoverse 2\nMotion Forecasting datasets. Code: https://github.com/kit-mrt/future-motion",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05465v2",
    "title": "Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit\n  Encodings for Efficient DNN Inference",
    "url": "http://arxiv.org/abs/2403.05465v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05465v2",
    "authors": [
      "Akshat Ramachandran",
      "Zishen Wan",
      "Geonhwa Jeong",
      "John Gustafson",
      "Tushar Krishna"
    ],
    "date": "2024-03-08",
    "summary": "Traditional Deep Neural Network (DNN) quantization methods using integer,\nfixed-point, or floating-point data types struggle to capture diverse DNN\nparameter distributions at low precision, and often require large silicon\noverhead and intensive quantization-aware training. In this study, we introduce\nLogarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by\nposits that dynamically adapts to DNN weight/activation distributions by\nparameterizing LP bit fields. We also develop a novel genetic-algorithm based\nframework, LP Quantization (LPQ), to find optimal layer-wise LP parameters\nwhile reducing representational divergence between quantized and full-precision\nmodels through a novel global-local contrastive objective. Additionally, we\ndesign a unified mixed-precision LP accelerator (LPA) architecture comprising\nof processing elements (PEs) incorporating LP in the computational datapath.\nOur algorithm-hardware co-design demonstrates on average <1% drop in top-1\naccuracy across various CNN and ViT models. It also achieves ~ 2x improvements\nin performance per unit area and 2.2x gains in energy efficiency compared to\nstate-of-the-art quantization accelerators using different data types.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05433v1",
    "title": "Part-aware Personalized Segment Anything Model for Patient-Specific\n  Segmentation",
    "url": "http://arxiv.org/abs/2403.05433v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05433v1",
    "authors": [
      "Chenhui Zhao",
      "Liyue Shen"
    ],
    "date": "2024-03-08",
    "summary": "Precision medicine, such as patient-adaptive treatments utilizing medical\nimages, poses new challenges for image segmentation algorithms due to (1) the\nlarge variability across different patients and (2) the limited availability of\nannotated data for each patient. In this work, we propose a data-efficient\nsegmentation method to address these challenges, namely Part-aware Personalized\nSegment Anything Model (P^2SAM). Without any model fine-tuning, P^2SAM enables\nseamless adaptation to any new patients relying only on one-shot\npatient-specific data. We introduce a novel part-aware prompt mechanism to\nselect multiple-point prompts based on part-level features of the one-shot\ndata. To further promote the robustness of the selected prompt, we propose a\nretrieval approach to handle outlier prompts. Extensive experiments demonstrate\nthat P^2SAM improves the performance by +8.0% and +2.0% mean Dice score within\ntwo patient-specific segmentation settings, and exhibits impressive generality\nacross different application domains, e.g., +6.4% mIoU on the PerSeg benchmark.\nCode will be released upon acceptance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Segment Anything"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05408v2",
    "title": "FedFMS: Exploring Federated Foundation Models for Medical Image\n  Segmentation",
    "url": "http://arxiv.org/abs/2403.05408v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05408v2",
    "authors": [
      "Yuxi Liu",
      "Guibo Luo",
      "Yuesheng Zhu"
    ],
    "date": "2024-03-08",
    "summary": "Medical image segmentation is crucial for clinical diagnosis. The\nSegmentation Anything Model (SAM) serves as a powerful foundation model for\nvisual segmentation and can be adapted for medical image segmentation. However,\nmedical imaging data typically contain privacy-sensitive information, making it\nchallenging to train foundation models with centralized storage and sharing. To\ndate, there are few foundation models tailored for medical image deployment\nwithin the federated learning framework, and the segmentation performance, as\nwell as the efficiency of communication and training, remain unexplored. In\nresponse to these issues, we developed Federated Foundation models for Medical\nimage Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a\ncommunication and training-efficient Federated SAM with Medical SAM Adapter\n(FedMSA). Comprehensive experiments on diverse datasets are conducted to\ninvestigate the performance disparities between centralized training and\nfederated learning across various configurations of FedFMS. The experiments\nrevealed that FedFMS could achieve performance comparable to models trained via\ncentralized training methods while maintaining privacy. Furthermore, FedMSA\ndemonstrated the potential to enhance communication and training efficiency.\nOur model implementation codes are available at\nhttps://github.com/LIU-YUXI/FedFMS.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "visual segmentation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05384v1",
    "title": "A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of\n  3D Echocardiography Images using a GAN",
    "url": "http://arxiv.org/abs/2403.05384v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05384v1",
    "authors": [
      "Cristiana Tiago",
      "Andrew Gilbert",
      "Ahmed S. Beela",
      "Svein Arne Aase",
      "Sten Roar Snare",
      "Jurica Sprem"
    ],
    "date": "2024-03-08",
    "summary": "Due to privacy issues and limited amount of publicly available labeled\ndatasets in the domain of medical imaging, we propose an image generation\npipeline to synthesize 3D echocardiographic images with corresponding ground\ntruth labels, to alleviate the need for data collection and for laborious and\nerror-prone human labeling of images for subsequent Deep Learning (DL) tasks.\nThe proposed method utilizes detailed anatomical segmentations of the heart as\nground truth label sources. This initial dataset is combined with a second\ndataset made up of real 3D echocardiographic images to train a Generative\nAdversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasound\nimages paired with ground truth labels. To generate the synthetic 3D dataset,\nthe trained GAN uses high resolution anatomical models from Computed Tomography\n(CT) as input. A qualitative analysis of the synthesized images showed that the\nmain structures of the heart are well delineated and closely follow the labels\nobtained from the anatomical models. To assess the usability of these synthetic\nimages for DL tasks, segmentation algorithms were trained to delineate the left\nventricle, left atrium, and myocardium. A quantitative analysis of the 3D\nsegmentations given by the models trained with the synthetic images indicated\nthe potential use of this GAN approach to generate 3D synthetic data, use the\ndata to train DL models for different clinical tasks, and therefore tackle the\nproblem of scarcity of 3D labeled echocardiography datasets.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05380v1",
    "title": "Spectrogram-Based Detection of Auto-Tuned Vocals in Music Recordings",
    "url": "http://arxiv.org/abs/2403.05380v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05380v1",
    "authors": [
      "Mahyar Gohari",
      "Paolo Bestagini",
      "Sergio Benini",
      "Nicola Adami"
    ],
    "date": "2024-03-08",
    "summary": "In the domain of music production and audio processing, the implementation of\nautomatic pitch correction of the singing voice, also known as Auto-Tune, has\nsignificantly transformed the landscape of vocal performance. While auto-tuning\ntechnology has offered musicians the ability to tune their vocal pitches and\nachieve a desired level of precision, its use has also sparked debates\nregarding its impact on authenticity and artistic integrity. As a result,\ndetecting and analyzing Auto-Tuned vocals in music recordings has become\nessential for music scholars, producers, and listeners. However, to the best of\nour knowledge, no prior effort has been made in this direction. This study\nintroduces a data-driven approach leveraging triplet networks for the detection\nof Auto-Tuned songs, backed by the creation of a dataset composed of original\nand Auto-Tuned audio clips. The experimental results demonstrate the\nsuperiority of the proposed method in both accuracy and robustness compared to\nRawnet2, an end-to-end model proposed for anti-spoofing and widely used for\nother audio forensic tasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05379v2",
    "title": "Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia\n  Classification",
    "url": "http://arxiv.org/abs/2403.05379v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05379v2",
    "authors": [
      "Salome Kazeminia",
      "Max Joosten",
      "Dragan Bosnacki",
      "Carsten Marr"
    ],
    "date": "2024-03-08",
    "summary": "Automated disease diagnosis using medical image analysis relies on deep\nlearning, often requiring large labeled datasets for supervised model training.\nDiseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and\ncostly annotations on a single-cell level. Multiple Instance Learning (MIL)\naddresses weakly labeled scenarios but necessitates powerful encoders typically\ntrained with labeled data. In this study, we explore Self-Supervised Learning\n(SSL) as a pre-training approach for MIL-based AML subtype classification from\nblood smears, removing the need for labeled data during encoder training. We\ninvestigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and\ncompare their performance against supervised pre-training. Our findings show\nthat SSL-pretrained encoders achieve comparable performance, showcasing the\npotential of SSL in MIL. This breakthrough offers a cost-effective and\ndata-efficient solution, propelling the field of AI-based disease diagnosis.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "DINO"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05325v1",
    "title": "Fine-tuning a Multiple Instance Learning Feature Extractor with Masked\n  Context Modelling and Knowledge Distillation",
    "url": "http://arxiv.org/abs/2403.05325v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05325v1",
    "authors": [
      "Juan I. Pisula",
      "Katarzyna Bozek"
    ],
    "date": "2024-03-08",
    "summary": "The first step in Multiple Instance Learning (MIL) algorithms for Whole Slide\nImage (WSI) classification consists of tiling the input image into smaller\npatches and computing their feature vectors produced by a pre-trained feature\nextractor model. Feature extractor models that were pre-trained with\nsupervision on ImageNet have proven to transfer well to this domain, however,\nthis pre-training task does not take into account that visual information in\nneighboring patches is highly correlated. Based on this observation, we propose\nto increase downstream MIL classification by fine-tuning the feature extractor\nmodel using \\textit{Masked Context Modelling with Knowledge Distillation}. In\nthis task, the feature extractor model is fine-tuned by predicting masked\npatches in a bigger context window. Since reconstructing the input image would\nrequire a powerful image generation model, and our goal is not to generate\nrealistically looking image patches, we predict instead the feature vectors\nproduced by a larger teacher network. A single epoch of the proposed task\nsuffices to increase the downstream performance of the feature-extractor model\nwhen used in a MIL scenario, even capable of outperforming the downstream\nperformance of the teacher model, while being considerably smaller and\nrequiring a fraction of its compute.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05300v5",
    "title": "Unity by Diversity: Improved Representation Learning in Multimodal VAEs",
    "url": "http://arxiv.org/abs/2403.05300v5",
    "pdf_url": "http://arxiv.org/pdf/2403.05300v5",
    "authors": [
      "Thomas M. Sutter",
      "Yang Meng",
      "Andrea Agostini",
      "Daphn\u00e9 Chopard",
      "Norbert Fortin",
      "Julia E. Vogt",
      "Babak Shahbaba",
      "Stephan Mandt"
    ],
    "date": "2024-03-08",
    "summary": "Variational Autoencoders for multimodal data hold promise for many tasks in\ndata analysis, such as representation learning, conditional generation, and\nimputation. Current architectures either share the encoder output, decoder\ninput, or both across modalities to learn a shared representation. Such\narchitectures impose hard constraints on the model. In this work, we show that\na better latent representation can be obtained by replacing these hard\nconstraints with a soft constraint. We propose a new mixture-of-experts prior,\nsoftly guiding each modality's latent representation towards a shared aggregate\nposterior. This approach results in a superior latent representation and allows\neach encoding to preserve information better from its uncompressed original\nfeatures. In extensive experiments on multiple benchmark datasets and two\nchallenging real-world datasets, we show improved learned latent\nrepresentations and imputation of missing data modalities compared to existing\nmethods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05297v3",
    "title": "PEEB: Part-based Image Classifiers with an Explainable and Editable\n  Language Bottleneck",
    "url": "http://arxiv.org/abs/2403.05297v3",
    "pdf_url": "http://arxiv.org/pdf/2403.05297v3",
    "authors": [
      "Thang M. Pham",
      "Peijie Chen",
      "Tin Nguyen",
      "Seunghyun Yoon",
      "Trung Bui",
      "Anh Totti Nguyen"
    ],
    "date": "2024-03-08",
    "summary": "CLIP-based classifiers rely on the prompt containing a {class name} that is\nknown to the text encoder. Therefore, they perform poorly on new classes or the\nclasses whose names rarely appear on the Internet (e.g., scientific names of\nbirds). For fine-grained classification, we propose PEEB - an explainable and\neditable classifier to (1) express the class name into a set of text\ndescriptors that describe the visual parts of that class; and (2) match the\nembeddings of the detected parts to their textual descriptors in each class to\ncompute a logit score for classification. In a zero-shot setting where the\nclass names are unknown, PEEB outperforms CLIP by a huge margin (~10x in top-1\naccuracy). Compared to part-based classifiers, PEEB is not only the\nstate-of-the-art (SOTA) on the supervised-learning setting (88.80% and 92.20%\naccuracy on CUB-200 and Dogs-120, respectively) but also the first to enable\nusers to edit the text descriptors to form a new classifier without any\nre-training. Compared to concept bottleneck models, PEEB is also the SOTA in\nboth zero-shot and supervised-learning settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05266v3",
    "title": "ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models",
    "url": "http://arxiv.org/abs/2403.05266v3",
    "pdf_url": "http://arxiv.org/pdf/2403.05266v3",
    "authors": [
      "Jio Oh",
      "Soyeon Kim",
      "Junseok Seo",
      "Jindong Wang",
      "Ruochen Xu",
      "Xing Xie",
      "Steven Euijong Whang"
    ],
    "date": "2024-03-08",
    "summary": "Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05256v1",
    "title": "DuDoUniNeXt: Dual-domain unified hybrid model for single and\n  multi-contrast undersampled MRI reconstruction",
    "url": "http://arxiv.org/abs/2403.05256v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05256v1",
    "authors": [
      "Ziqi Gao",
      "Yue Zhang",
      "Xinwen Liu",
      "Kaiyan Li",
      "S. Kevin Zhou"
    ],
    "date": "2024-03-08",
    "summary": "Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to\nincorporate a reference image of auxiliary modality to guide the reconstruction\nprocess of the target modality. Known MC reconstruction methods perform well\nwith a fully sampled reference image, but usually exhibit inferior performance,\ncompared to single-contrast (SC) methods, when the reference image is missing\nor of low quality. To address this issue, we propose DuDoUniNeXt, a unified\ndual-domain MRI reconstruction network that can accommodate to scenarios\ninvolving absent, low-quality, and high-quality reference images. DuDoUniNeXt\nadopts a hybrid backbone that combines CNN and ViT, enabling specific\nadjustment of image domain and k-space reconstruction. Specifically, an\nadaptive coarse-to-fine feature fusion module (AdaC2F) is devised to\ndynamically process the information from reference images of varying qualities.\nBesides, a partially shared shallow feature extractor (PaSS) is proposed, which\nuses shared and distinct parameters to handle consistent and discrepancy\ninformation among contrasts. Experimental results demonstrate that the proposed\nmodel surpasses state-of-the-art SC and MC models significantly. Ablation\nstudies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS,\nand the dual-domain unified learning scheme.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05231v2",
    "title": "Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance",
    "url": "http://arxiv.org/abs/2403.05231v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05231v2",
    "authors": [
      "Liting Lin",
      "Heng Fan",
      "Zhipeng Zhang",
      "Yaowei Wang",
      "Yong Xu",
      "Haibin Ling"
    ],
    "date": "2024-03-08",
    "summary": "Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language\nmodels, we propose LoRAT, a method that unveils the power of large ViT model\nfor tracking within laboratory-level resources. The essence of our work lies in\nadapting LoRA, a technique that fine-tunes a small subset of model parameters\nwithout adding inference latency, to the domain of visual tracking. However,\nunique challenges and potential domain gaps make this transfer not as easy as\nthe first intuition. Firstly, a transformer-based tracker constructs unshared\nposition embedding for template and search image. This poses a challenge for\nthe transfer of LoRA, usually requiring consistency in the design when applied\nto the pre-trained backbone, to downstream tasks. Secondly, the inductive bias\ninherent in convolutional heads diminishes the effectiveness of\nparameter-efficient fine-tuning in tracking models. To overcome these\nlimitations, we first decouple the position embeddings in transformer-based\ntrackers into shared spatial ones and independent type ones. The shared\nembeddings, which describe the absolute coordinates of multi-resolution images\n(namely, the template and search images), are inherited from the pre-trained\nbackbones. In contrast, the independent embeddings indicate the sources of each\ntoken and are learned from scratch. Furthermore, we design an anchor-free head\nsolely based on MLP to adapt PETR, enabling better performance with less\ncomputational overhead. With our design, 1) it becomes practical to train\ntrackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size\nof 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8\nGPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.742 with the L-224\nvariant; 4) we fast the inference speed of the L-224 variant from 52 to 119\nFPS. Code and models are available at https://github.com/LitingLin/LoRAT.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "ViT"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05606v1",
    "title": "A Concept-based Interpretable Model for the Diagnosis of Choroid\n  Neoplasias using Multimodal Data",
    "url": "http://arxiv.org/abs/2403.05606v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05606v1",
    "authors": [
      "Yifan Wu",
      "Yang Liu",
      "Yue Yang",
      "Michael S. Yao",
      "Wenli Yang",
      "Xuehui Shi",
      "Lihong Yang",
      "Dongjun Li",
      "Yueming Liu",
      "James C. Gee",
      "Xuan Yang",
      "Wenbin Wei",
      "Shi Gu"
    ],
    "date": "2024-03-08",
    "summary": "Diagnosing rare diseases presents a common challenge in clinical practice,\nnecessitating the expertise of specialists for accurate identification. The\nadvent of machine learning offers a promising solution, while the development\nof such technologies is hindered by the scarcity of data on rare conditions and\nthe demand for models that are both interpretable and trustworthy in a clinical\ncontext. Interpretable AI, with its capacity for human-readable outputs, can\nfacilitate validation by clinicians and contribute to medical education. In the\ncurrent work, we focus on choroid neoplasias, the most prevalent form of eye\ncancer in adults, albeit rare with 5.1 per million. We built the so-far largest\ndataset consisting of 750 patients, incorporating three distinct imaging\nmodalities collected from 2004 to 2022. Our work introduces a concept-based\ninterpretable model that distinguishes between three types of choroidal tumors,\nintegrating insights from domain experts via radiological reports. Remarkably,\nthis model not only achieves an F1 score of 0.91, rivaling that of black-box\nmodels, but also boosts the diagnostic accuracy of junior doctors by 42%. This\nstudy highlights the significant potential of interpretable machine learning in\nimproving the diagnosis of rare diseases, laying a groundwork for future\nbreakthroughs in medical AI that could tackle a wider array of complex health\nscenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05093v1",
    "title": "Spectrum Translation for Refinement of Image Generation (STIG) Based on\n  Contrastive Learning and Spectral Filter Profile",
    "url": "http://arxiv.org/abs/2403.05093v1",
    "pdf_url": "http://arxiv.org/pdf/2403.05093v1",
    "authors": [
      "Seokjun Lee",
      "Seung-Won Jung",
      "Hyunseok Seo"
    ],
    "date": "2024-03-08",
    "summary": "Currently, image generation and synthesis have remarkably progressed with\ngenerative models. Despite photo-realistic results, intrinsic discrepancies are\nstill observed in the frequency domain. The spectral discrepancy appeared not\nonly in generative adversarial networks but in diffusion models. In this study,\nwe propose a framework to effectively mitigate the disparity in frequency\ndomain of the generated images to improve generative performance of both GAN\nand diffusion models. This is realized by spectrum translation for the\nrefinement of image generation (STIG) based on contrastive learning. We adopt\ntheoretical logic of frequency components in various generative networks. The\nkey idea, here, is to refine the spectrum of the generated image via the\nconcept of image-to-image translation and contrastive learning in terms of\ndigital signal processing. We evaluate our framework across eight fake image\ndatasets and various cutting-edge models to demonstrate the effectiveness of\nSTIG. Our framework outperforms other cutting-edges showing significant\ndecreases in FID and log frequency distance of spectrum. We further emphasize\nthat STIG improves image quality by decreasing the spectral anomaly.\nAdditionally, validation results present that the frequency-based deepfake\ndetector confuses more in the case where fake spectrums are manipulated by\nSTIG.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05060v3",
    "title": "Multimodal Infusion Tuning for Large Models",
    "url": "http://arxiv.org/abs/2403.05060v3",
    "pdf_url": "http://arxiv.org/pdf/2403.05060v3",
    "authors": [
      "Hao Sun",
      "Yu Song",
      "Xinyao Yu",
      "Jiaqing Liu",
      "Yen-Wei Chen",
      "Lanfen Lin"
    ],
    "date": "2024-03-08",
    "summary": "Recent advancements in large-scale models have showcased remarkable\ngeneralization capabilities in various tasks. However, integrating multimodal\nprocessing into these models presents a significant challenge, as it often\ncomes with a high computational burden. To address this challenge, we introduce\na new parameter-efficient multimodal tuning strategy for large models in this\npaper, referred to as Multimodal Infusion Tuning (MiT). MiT leverages decoupled\nself-attention mechanisms within large language models to effectively integrate\ninformation from diverse modalities such as images and acoustics. In MiT, we\nalso design a novel adaptive rescaling strategy at the attention head level,\nwhich optimizes the representation of infused multimodal features. Notably, all\nfoundation models are kept frozen during the tuning process to reduce the\ncomputational burden and only 2.5\\% parameters are tunable. We conduct\nexperiments across a range of multimodal tasks, including image-related tasks\nlike referring segmentation and non-image tasks such as sentiment analysis. Our\nresults showcase that MiT achieves state-of-the-art performance in multimodal\nunderstanding while significantly reducing computational overhead(10\\% of\nprevious methods). Moreover, our tuned model exhibits robust reasoning\nabilities even in complex scenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05025v3",
    "title": "Debiased Multimodal Understanding for Human Language Sequences",
    "url": "http://arxiv.org/abs/2403.05025v3",
    "pdf_url": "http://arxiv.org/pdf/2403.05025v3",
    "authors": [
      "Zhi Xu",
      "Dingkang Yang",
      "Mingcheng Li",
      "Yuzheng Wang",
      "Zhaoyu Chen",
      "Jiawei Chen",
      "Jinjie Wei",
      "Lihua Zhang"
    ],
    "date": "2024-03-08",
    "summary": "Human multimodal language understanding (MLU) is an indispensable component\nof expression analysis (e.g., sentiment or humor) from heterogeneous\nmodalities, including visual postures, linguistic contents, and acoustic\nbehaviours. Existing works invariably focus on designing sophisticated\nstructures or fusion strategies to achieve impressive improvements.\nUnfortunately, they all suffer from the subject variation problem due to data\ndistribution discrepancies among subjects. Concretely, MLU models are easily\nmisled by distinct subjects with different expression customs and\ncharacteristics in the training data to learn subject-specific spurious\ncorrelations, limiting performance and generalizability across new subjects.\nMotivated by this observation, we introduce a recapitulative causal graph to\nformulate the MLU procedure and analyze the confounding effect of subjects.\nThen, we propose SuCI, a simple yet effective causal intervention module to\ndisentangle the impact of subjects acting as unobserved confounders and achieve\nmodel training via true causal effects. As a plug-and-play component, SuCI can\nbe widely applied to most methods that seek unbiased predictions. Comprehensive\nexperiments on several MLU benchmarks clearly show the effectiveness of the\nproposed module.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.05023v2",
    "title": "Towards Multimodal Sentiment Analysis Debiasing via Bias Purification",
    "url": "http://arxiv.org/abs/2403.05023v2",
    "pdf_url": "http://arxiv.org/pdf/2403.05023v2",
    "authors": [
      "Dingkang Yang",
      "Mingcheng Li",
      "Dongling Xiao",
      "Yang Liu",
      "Kun Yang",
      "Zhaoyu Chen",
      "Yuzheng Wang",
      "Peng Zhai",
      "Ke Li",
      "Lihua Zhang"
    ],
    "date": "2024-03-08",
    "summary": "Multimodal Sentiment Analysis (MSA) aims to understand human intentions by\nintegrating emotion-related clues from diverse modalities, such as visual,\nlanguage, and audio. Unfortunately, the current MSA task invariably suffers\nfrom unplanned dataset biases, particularly multimodal utterance-level label\nbias and word-level context bias. These harmful biases potentially mislead\nmodels to focus on statistical shortcuts and spurious correlations, causing\nsevere performance bottlenecks. To alleviate these issues, we present a\nMultimodal Counterfactual Inference Sentiment (MCIS) analysis framework based\non causality rather than conventional likelihood. Concretely, we first\nformulate a causal graph to discover harmful biases from already-trained\nvanilla models. In the inference phase, given a factual multimodal input, MCIS\nimagines two counterfactual scenarios to purify and mitigate these biases.\nThen, MCIS can make unbiased decisions from biased observations by comparing\nfactual and counterfactual outcomes. We conduct extensive experiments on\nseveral standard MSA benchmarks. Qualitative and quantitative results show the\neffectiveness of the proposed framework.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04997v1",
    "title": "DiffChat: Learning to Chat with Text-to-Image Synthesis Models for\n  Interactive Image Creation",
    "url": "http://arxiv.org/abs/2403.04997v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04997v1",
    "authors": [
      "Jiapeng Wang",
      "Chengyu Wang",
      "Tingfeng Cao",
      "Jun Huang",
      "Lianwen Jin"
    ],
    "date": "2024-03-08",
    "summary": "We present DiffChat, a novel method to align Large Language Models (LLMs) to\n\"chat\" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable\nDiffusion) for interactive image creation. Given a raw prompt/image and a\nuser-specified instruction, DiffChat can effectively make appropriate\nmodifications and generate the target prompt, which can be leveraged to create\nthe target image of high quality. To achieve this, we first collect an\ninstruction-following prompt engineering dataset named InstructPE for the\nsupervised training of DiffChat. Next, we propose a reinforcement learning\nframework with the feedback of three core criteria for image creation, i.e.,\naesthetics, user preference, and content integrity. It involves an action-space\ndynamic modification technique to obtain more relevant positive samples and\nharder negative samples during the off-policy sampling. Content integrity is\nalso introduced into the value estimation function for further improvement of\nproduced images. Our method can exhibit superior performance than baseline\nmodels and strong competitors based on both automatic and human evaluations,\nwhich fully demonstrates its effectiveness.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "text-to-image"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04965v2",
    "title": "StereoDiffusion: Training-Free Stereo Image Generation Using Latent\n  Diffusion Models",
    "url": "http://arxiv.org/abs/2403.04965v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04965v2",
    "authors": [
      "Lezhong Wang",
      "Jeppe Revall Frisvad",
      "Mark Bo Jensen",
      "Siavash Arjomand Bigdeli"
    ],
    "date": "2024-03-08",
    "summary": "The demand for stereo images increases as manufacturers launch more XR\ndevices. To meet this demand, we introduce StereoDiffusion, a method that,\nunlike traditional inpainting pipelines, is trainning free, remarkably\nstraightforward to use, and it seamlessly integrates into the original Stable\nDiffusion model. Our method modifies the latent variable to provide an\nend-to-end, lightweight capability for fast generation of stereo image pairs,\nwithout the need for fine-tuning model weights or any post-processing of\nimages. Using the original input to generate a left image and estimate a\ndisparity map for it, we generate the latent vector for the right image through\nStereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking\nDenoise and Self-Attention Layers Modification methods to align the right-side\nimage with the left-side image. Moreover, our proposed method maintains a high\nstandard of image quality throughout the stereo generation process, achieving\nstate-of-the-art scores in various quantitative evaluations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.2,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04945v3",
    "title": "MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language\n  Models for Report Generation",
    "url": "http://arxiv.org/abs/2403.04945v3",
    "pdf_url": "http://arxiv.org/pdf/2403.04945v3",
    "authors": [
      "Zhongwei Wan",
      "Che Liu",
      "Xin Wang",
      "Chaofan Tao",
      "Hui Shen",
      "Zhenwu Peng",
      "Jie Fu",
      "Rossella Arcucci",
      "Huaxiu Yao",
      "Mi Zhang"
    ],
    "date": "2024-03-07",
    "summary": "Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for\nmonitoring cardiac conditions and is crucial in assisting clinicians. Recent\nstudies have concentrated on classifying cardiac conditions using ECG data but\nhave overlooked ECG report generation, which is time-consuming and requires\nclinical expertise. To automate ECG report generation and ensure its\nversatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework,\nthe first attempt to tackle ECG report generation with LLMs and multimodal\ninstructions. To facilitate future research, we establish a benchmark to\nevaluate MEIT with various LLMs backbones across two large-scale ECG datasets.\nOur approach uniquely aligns the representations of the ECG signal and the\nreport, and we conduct extensive experiments to benchmark MEIT with nine\nopen-source LLMs using more than 800,000 ECG reports. MEIT's results underscore\nthe superior performance of instruction-tuned LLMs, showcasing their\nproficiency in quality report generation, zero-shot capabilities, and\nresilience to signal perturbation. These findings emphasize the efficacy of our\nMEIT framework and its potential for real-world clinical application.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04940v1",
    "title": "A spatiotemporal style transfer algorithm for dynamic visual stimulus\n  generation",
    "url": "http://arxiv.org/abs/2403.04940v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04940v1",
    "authors": [
      "Antonino Greco",
      "Markus Siegel"
    ],
    "date": "2024-03-07",
    "summary": "Understanding how visual information is encoded in biological and artificial\nsystems often requires vision scientists to generate appropriate stimuli to\ntest specific hypotheses. Although deep neural network models have\nrevolutionized the field of image generation with methods such as image style\ntransfer, available methods for video generation are scarce. Here, we introduce\nthe Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus\ngeneration framework that allows powerful manipulation and synthesis of video\nstimuli for vision research. It is based on a two-stream deep neural network\nmodel that factorizes spatial and temporal features to generate dynamic visual\nstimuli whose model layer activations are matched to those of input videos. As\nan example, we show that our algorithm enables the generation of model\nmetamers, dynamic stimuli whose layer activations within our two-stream model\nare matched to those of natural videos. We show that these generated stimuli\nmatch the low-level spatiotemporal features of their natural counterparts but\nlack their high-level semantic features, making it a powerful paradigm to study\nobject recognition. Late layer activations in deep vision models exhibited a\nlower similarity between natural and metameric stimuli compared to early\nlayers, confirming the lack of high-level information in the generated stimuli.\nFinally, we use our generated stimuli to probe the representational\ncapabilities of predictive coding deep networks. These results showcase\npotential applications of our algorithm as a versatile tool for dynamic\nstimulus generation in vision science.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "image generation"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04924v1",
    "title": "$\\text{R}^2$-Bench: Benchmarking the Robustness of Referring Perception\n  Models under Perturbations",
    "url": "http://arxiv.org/abs/2403.04924v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04924v1",
    "authors": [
      "Xiang Li",
      "Kai Qiu",
      "Jinglu Wang",
      "Xiaohao Xu",
      "Rita Singh",
      "Kashu Yamazak",
      "Hao Chen",
      "Xiaonan Huang",
      "Bhiksha Raj"
    ],
    "date": "2024-03-07",
    "summary": "Referring perception, which aims at grounding visual objects with multimodal\nreferring guidance, is essential for bridging the gap between humans, who\nprovide instructions, and the environment where intelligent systems perceive.\nDespite progress in this field, the robustness of referring perception models\n(RPMs) against disruptive perturbations is not well explored. This work\nthoroughly assesses the resilience of RPMs against various perturbations in\nboth general and specific contexts. Recognizing the complex nature of referring\nperception tasks, we present a comprehensive taxonomy of perturbations, and\nthen develop a versatile toolbox for synthesizing and evaluating the effects of\ncomposite disturbances. Employing this toolbox, we construct\n$\\text{R}^2$-Bench, a benchmark for assessing the Robustness of Referring\nperception models under noisy conditions across five key tasks. Moreover, we\npropose the $\\text{R}^2$-Agent, an LLM-based agent that simplifies and\nautomates model evaluation via natural language instructions. Our investigation\nuncovers the vulnerabilities of current RPMs to various perturbations and\nprovides tools for assessing model robustness, potentially promoting the safe\nand resilient integration of intelligent systems into complex real-world\nscenarios.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04866v1",
    "title": "A Modular End-to-End Multimodal Learning Method for Structured and\n  Unstructured Data",
    "url": "http://arxiv.org/abs/2403.04866v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04866v1",
    "authors": [
      "Marco D Alessandro",
      "Enrique Calabr\u00e9s",
      "Mikel Elkano"
    ],
    "date": "2024-03-07",
    "summary": "Multimodal learning is a rapidly growing research field that has\nrevolutionized multitasking and generative modeling in AI. While much of the\nresearch has focused on dealing with unstructured data (e.g., language, images,\naudio, or video), structured data (e.g., tabular data, time series, or signals)\nhas received less attention. However, many industry-relevant use cases involve\nor can be benefited from both types of data. In this work, we propose a\nmodular, end-to-end multimodal learning method called MAGNUM, which can\nnatively handle both structured and unstructured data. MAGNUM is flexible\nenough to employ any specialized unimodal module to extract, compress, and fuse\ninformation from all available modalities.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04822v2",
    "title": "UniTable: Towards a Unified Framework for Table Recognition via\n  Self-Supervised Pretraining",
    "url": "http://arxiv.org/abs/2403.04822v2",
    "pdf_url": "http://arxiv.org/pdf/2403.04822v2",
    "authors": [
      "ShengYun Peng",
      "Aishwarya Chakravarthy",
      "Seongmin Lee",
      "Xiaojing Wang",
      "Rajarajeswari Balasubramaniyan",
      "Duen Horng Chau"
    ],
    "date": "2024-03-07",
    "summary": "Tables convey factual and quantitative data with implicit conventions created\nby humans that are often challenging for machines to parse. Prior work on table\nrecognition (TR) has mainly centered around complex task-specific combinations\nof available inputs and tools. We present UniTable, a training framework that\nunifies both the training paradigm and training objective of TR. Its training\nparadigm combines the simplicity of purely pixel-level inputs with the\neffectiveness and scalability empowered by self-supervised pretraining from\ndiverse unannotated tabular images. Our framework unifies the training\nobjectives of all three TR tasks - extracting table structure, cell content,\nand cell bounding box - into a unified task-agnostic training objective:\nlanguage modeling. Extensive quantitative and qualitative analyses highlight\nUniTable's state-of-the-art (SOTA) performance on four of the largest TR\ndatasets. UniTable's table parsing capability has surpassed both existing TR\nmethods and general large vision-language models, e.g., GPT-4o, GPT-4-turbo\nwith vision, and LLaVA. Our code is publicly available at\nhttps://github.com/poloclub/unitable, featuring a Jupyter Notebook that\nincludes the complete inference pipeline, fine-tuned across multiple TR\ndatasets, supporting all three TR tasks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04453v3",
    "title": "Efficient Off-Policy Learning for High-Dimensional Action Spaces",
    "url": "http://arxiv.org/abs/2403.04453v3",
    "pdf_url": "http://arxiv.org/pdf/2403.04453v3",
    "authors": [
      "Fabian Otto",
      "Philipp Becker",
      "Ngo Anh Vien",
      "Gerhard Neumann"
    ],
    "date": "2024-03-07",
    "summary": "Existing off-policy reinforcement learning algorithms often rely on an\nexplicit state-action-value function representation, which can be problematic\nin high-dimensional action spaces due to the curse of dimensionality. This\nreliance results in data inefficiency as maintaining a state-action-value\nfunction in such spaces is challenging. We present an efficient approach that\nutilizes only a state-value function as the critic for off-policy deep\nreinforcement learning. This approach, which we refer to as Vlearn, effectively\ncircumvents the limitations of existing methods by eliminating the necessity\nfor an explicit state-action-value function. To this end, we leverage a\nweighted importance sampling loss for learning deep value functions from\noff-policy data. While this is common for linear methods, it has not been\ncombined with deep value function networks. This transfer to deep methods is\nnot straightforward and requires novel design choices such as robust policy\nupdates, twin value function networks to avoid an optimization bias, and\nimportance weight clipping. We also present a novel analysis of the variance of\nour estimate compared to commonly used importance sampling estimators such as\nV-trace. Our approach improves sample complexity as well as final performance\nand ensures consistent and robust performance across various benchmark tasks.\nEliminating the state-action-value function in Vlearn facilitates a streamlined\nlearning process, yielding high-return agents.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04309v1",
    "title": "AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection",
    "url": "http://arxiv.org/abs/2403.04309v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04309v1",
    "authors": [
      "Mingyuan Li",
      "Tong Jia",
      "Hao Wang",
      "Bowen Ma",
      "Shuyang Lin",
      "Da Cai",
      "Dongyue Chen"
    ],
    "date": "2024-03-07",
    "summary": "Prohibited item detection in X-ray images is one of the most essential and\nhighly effective methods widely employed in various security inspection\nscenarios. Considering the significant overlapping phenomenon in X-ray\nprohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on\none of the state-of-the-art general object detectors, DINO. Specifically, to\naddress the feature coupling issue caused by overlapping phenomena, we\nintroduce the Category-Specific One-to-One Assignment (CSA) strategy to\nconstrain category-specific object queries in predicting prohibited items of\nfixed categories, which can enhance their ability to extract features specific\nto prohibited items of a particular category from the overlapping\nforeground-background features. To address the edge blurring problem caused by\noverlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which\nimproves the localization accuracy of reference boxes in mid-to-high-level\ndecoder layers and enhances the ability to locate blurry edges of the final\nlayer. Similar to DINO, our AO-DETR provides two different versions with\ndistinct backbones, tailored to meet diverse application requirements.\nExtensive experiments on the PIXray and OPIXray datasets demonstrate that the\nproposed method surpasses the state-of-the-art object detectors, indicating its\npotential applications in the field of prohibited item detection. The source\ncode will be released at https://github.com/Limingyuan001/AO-DETR-test.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "DINO"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04294v1",
    "title": "A$^{3}$lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment\n  for Dynamic Facial Expression Recognition with CLIP",
    "url": "http://arxiv.org/abs/2403.04294v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04294v1",
    "authors": [
      "Zeng Tao",
      "Yan Wang",
      "Junxiong Lin",
      "Haoran Wang",
      "Xinji Mai",
      "Jiawen Yu",
      "Xuan Tong",
      "Ziheng Zhou",
      "Shaoqi Yan",
      "Qing Zhao",
      "Liyuan Han",
      "Wenqiang Zhang"
    ],
    "date": "2024-03-07",
    "summary": "The performance of CLIP in dynamic facial expression recognition (DFER) task\ndoesn't yield exceptional results as observed in other CLIP-based\nclassification tasks. While CLIP's primary objective is to achieve alignment\nbetween images and text in the feature space, DFER poses challenges due to the\nabstract nature of text and the dynamic nature of video, making label\nrepresentation limited and perfect alignment difficult. To address this issue,\nwe have designed A$^{3}$lign-DFER, which introduces a new DFER labeling\nparadigm to comprehensively achieve alignment, thus enhancing CLIP's\nsuitability for the DFER task. Specifically, our A$^{3}$lign-DFER method is\ndesigned with multiple modules that work together to obtain the most suitable\nexpanded-dimensional embeddings for classification and to achieve alignment in\nthree key aspects: affective, dynamic, and bidirectional. We replace the input\nlabel text with a learnable Multi-Dimensional Alignment Token (MAT), enabling\nalignment of text to facial expression video samples in both affective and\ndynamic dimensions. After CLIP feature extraction, we introduce the Joint\nDynamic Alignment Synchronizer (JAS), further facilitating synchronization and\nalignment in the temporal dimension. Additionally, we implement a Bidirectional\nAlignment Training Paradigm (BAP) to ensure gradual and steady training of\nparameters for both modalities. Our insightful and concise A$^{3}$lign-DFER\nmethod achieves state-of-the-art results on multiple DFER datasets, including\nDFEW, FERV39k, and MAFW. Extensive ablation experiments and visualization\nstudies demonstrate the effectiveness of A$^{3}$lign-DFER. The code will be\navailable in the future.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2403.04245v1",
    "title": "A Study of Dropout-Induced Modality Bias on Robustness to Missing Video\n  Frames for Audio-Visual Speech Recognition",
    "url": "http://arxiv.org/abs/2403.04245v1",
    "pdf_url": "http://arxiv.org/pdf/2403.04245v1",
    "authors": [
      "Yusheng Dai",
      "Hang Chen",
      "Jun Du",
      "Ruoyu Wang",
      "Shihao Chen",
      "Jiefeng Ma",
      "Haotian Wang",
      "Chin-Hui Lee"
    ],
    "date": "2024-03-07",
    "summary": "Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to\nbe sensitive to missing video frames, performing even worse than\nsingle-modality models. While applying the dropout technique to the video\nmodality enhances robustness to missing frames, it simultaneously results in a\nperformance loss when dealing with complete data input. In this paper, we\ninvestigate this contrasting phenomenon from the perspective of modality bias\nand reveal that an excessive modality bias on the audio caused by dropout is\nthe underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH)\nto systematically describe the relationship between modality bias and\nrobustness against missing modality in multimodal systems. Building on these\nfindings, we propose a novel Multimodal Distribution Approximation with\nKnowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio\nmodality and to maintain performance and robustness simultaneously. Finally, to\naddress an entirely missing modality, we adopt adapters to dynamically switch\ndecision strategies. The effectiveness of our proposed approach is evaluated\nand validated through a series of comprehensive experiments using the MISP2021\nand MISP2022 datasets. Our code is available at\nhttps://github.com/dalision/ModalBiasAVSR",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03507v1",
    "title": "Mineral segmentation using electron microscope images and spectral\n  sampling through multimodal graph neural networks",
    "url": "http://arxiv.org/abs/2503.03507v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03507v1",
    "authors": [
      "Samuel Repka",
      "Bo\u0159ek Reich",
      "Fedor Zolotarev",
      "Tuomas Eerola",
      "Pavel Zem\u010d\u00edk"
    ],
    "date": "2024-03-05",
    "summary": "We propose a novel Graph Neural Network-based method for segmentation based\non data fusion of multimodal Scanning Electron Microscope (SEM) images. In most\ncases, Backscattered Electron (BSE) images obtained using SEM do not contain\nsufficient information for mineral segmentation. Therefore, imaging is often\ncomplemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS)\nspectral measurements that provide highly accurate information about the\nchemical composition but that are time-consuming to acquire. This motivates the\nuse of sparse spectral data in conjunction with BSE images for mineral\nsegmentation. The unstructured nature of the spectral data makes most\ntraditional image fusion techniques unsuitable for BSE-EDS fusion. We propose\nusing graph neural networks to fuse the two modalities and segment the mineral\nphases simultaneously. Our results demonstrate that providing EDS data for as\nfew as 1% of BSE pixels produces accurate segmentation, enabling rapid analysis\nof mineral samples. The proposed data fusion pipeline is versatile and can be\nadapted to other domains that involve image data and point-wise measurements.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03465v2",
    "title": "DTU-Net: A Multi-Scale Dilated Transformer Network for Nonlinear\n  Hyperspectral Unmixing",
    "url": "http://arxiv.org/abs/2503.03465v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03465v2",
    "authors": [
      "ChenTong Wang",
      "Jincheng Gao",
      "Fei Zhu",
      "Abderrahim Halimi",
      "C\u00e9dric Richard"
    ],
    "date": "2024-03-05",
    "summary": "Transformers have shown significant success in hyperspectral unmixing (HU).\nHowever, challenges remain. While multi-scale and long-range spatial\ncorrelations are essential in unmixing tasks, current Transformer-based\nunmixing networks, built on Vision Transformer (ViT) or Swin-Transformer,\nstruggle to capture them effectively. Additionally, current Transformer-based\nunmixing networks rely on the linear mixing model, which lacks the flexibility\nto accommodate scenarios where nonlinear effects are significant. To address\nthese limitations, we propose a multi-scale Dilated Transformer-based unmixing\nnetwork for nonlinear HU (DTU-Net). The encoder employs two branches. The first\none performs multi-scale spatial feature extraction using Multi-Scale Dilated\nAttention (MSDA) in the Dilated Transformer, which varies dilation rates across\nattention heads to capture long-range and multi-scale spatial correlations. The\nsecond one performs spectral feature extraction utilizing 3D-CNNs with channel\nattention. The outputs from both branches are then fused to integrate\nmulti-scale spatial and spectral information, which is subsequently transformed\nto estimate the abundances. The decoder is designed to accommodate both linear\nand nonlinear mixing scenarios. Its interpretability is enhanced by explicitly\nmodeling the relationships between endmembers, abundances, and nonlinear\ncoefficients in accordance with the polynomial post-nonlinear mixing model\n(PPNMM). Experiments on synthetic and real datasets validate the effectiveness\nof the proposed DTU-Net compared to PPNMM-derived methods and several advanced\nunmixing networks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03335v1",
    "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective\n  Responses to News",
    "url": "http://arxiv.org/abs/2503.03335v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03335v1",
    "authors": [
      "Tiancheng Hu",
      "Nigel Collier"
    ],
    "date": "2024-03-05",
    "summary": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03280v1",
    "title": "BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation",
    "url": "http://arxiv.org/abs/2503.03280v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03280v1",
    "authors": [
      "Hiep Truong Cong",
      "Ajay Kumar Sigatapu",
      "Arindam Das",
      "Yashwanth Sharma",
      "Venkatesh Satagopan",
      "Ganesh Sistu",
      "Ciaran Eising"
    ],
    "date": "2024-03-05",
    "summary": "Accurate motion understanding of the dynamic objects within the scene in\nbird's-eye-view (BEV) is critical to ensure a reliable obstacle avoidance\nsystem and smooth path planning for autonomous vehicles. However, this task has\nreceived relatively limited exploration when compared to object detection and\nsegmentation with only a few recent vision-based approaches presenting\npreliminary findings that significantly deteriorate in low-light, nighttime,\nand adverse weather conditions such as rain. Conversely, LiDAR and radar\nsensors remain almost unaffected in these scenarios, and radar provides key\nvelocity information of the objects. Therefore, we introduce BEVMOSNet, to our\nknowledge, the first end-to-end multimodal fusion leveraging cameras, LiDAR,\nand radar to precisely predict the moving objects in BEV. In addition, we\nperform a deeper analysis to find out the optimal strategy for deformable\ncross-attention-guided sensor fusion for cross-sensor knowledge sharing in BEV.\nWhile evaluating BEVMOSNet on the nuScenes dataset, we show an overall\nimprovement in IoU score of 36.59% compared to the vision-based unimodal\nbaseline BEV-MoSeg (Sigatapu et al., 2023), and 2.35% compared to the\nmultimodel SimpleBEV (Harley et al., 2022), extended for the motion\nsegmentation task, establishing this method as the state-of-the-art in BEV\nmotion segmentation.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03792v1",
    "title": "Rebalanced Multimodal Learning with Data-aware Unimodal Sampling",
    "url": "http://arxiv.org/abs/2503.03792v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03792v1",
    "authors": [
      "Qingyuan Jiang",
      "Zhouyang Chi",
      "Xiao Ma",
      "Qirong Mao",
      "Yang Yang",
      "Jinhui Tang"
    ],
    "date": "2024-03-05",
    "summary": "To address the modality learning degeneration caused by modality imbalance,\nexisting multimodal learning~(MML) approaches primarily attempt to balance the\noptimization process of each modality from the perspective of model learning.\nHowever, almost all existing methods ignore the modality imbalance caused by\nunimodal data sampling, i.e., equal unimodal data sampling often results in\ndiscrepancies in informational content, leading to modality imbalance.\nTherefore, in this paper, we propose a novel MML approach called\n\\underline{D}ata-aware \\underline{U}nimodal \\underline{S}ampling~(\\method),\nwhich aims to dynamically alleviate the modality imbalance caused by sampling.\nSpecifically, we first propose a novel cumulative modality discrepancy to\nmonitor the multimodal learning process. Based on the learning status, we\npropose a heuristic and a reinforcement learning~(RL)-based data-aware unimodal\nsampling approaches to adaptively determine the quantity of sampled data at\neach iteration, thus alleviating the modality imbalance from the perspective of\nsampling. Meanwhile, our method can be seamlessly incorporated into almost all\nexisting multimodal learning approaches as a plugin. Experiments demonstrate\nthat \\method~can achieve the best performance by comparing with diverse\nstate-of-the-art~(SOTA) baselines.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03244v1",
    "title": "Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection\n  in Neonatal Care",
    "url": "http://arxiv.org/abs/2503.03244v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03244v1",
    "authors": [
      "Jorge Garc\u00eda-Torres",
      "\u00d8yvind Meinich-Bache",
      "Sara Brunner",
      "Siren Rettedal",
      "Vilde Kolstad",
      "Kjersti Engan"
    ],
    "date": "2024-03-05",
    "summary": "Around 10% of newborns require some help to initiate breathing, and 5\\% need\nventilation assistance. Accurate Time of Birth (ToB) documentation is essential\nfor optimizing neonatal care, as timely interventions are vital for proper\nresuscitation. However, current clinical methods for recording ToB often rely\non manual processes, which can be prone to inaccuracies. In this study, we\npresent a novel two-stream fusion system that combines the power of image and\nvideo analysis to accurately detect the ToB from thermal recordings in the\ndelivery room and operating theater. By integrating static and dynamic streams,\nour approach captures richer birth-related spatiotemporal features, leading to\nmore robust and precise ToB estimation. We demonstrate that this synergy\nbetween data modalities enhances performance over single-stream approaches. Our\nsystem achieves 95.7% precision and 84.8% recall in detecting birth within\nshort video clips. Additionally, with the help of a score aggregation module,\nit successfully identifies ToB in 100% of test cases, with a median absolute\nerror of 2 seconds and an absolute mean deviation of 4.5 seconds compared to\nmanual annotations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03215v1",
    "title": "COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open\n  Source Intelligence",
    "url": "http://arxiv.org/abs/2503.03215v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03215v1",
    "authors": [
      "Wentao Li",
      "Congcong Wang",
      "Xiaoxiao Cui",
      "Zhi Liu",
      "Wei Guo",
      "Lizhen Cui"
    ],
    "date": "2024-03-05",
    "summary": "Open Source Intelligence (OSINT) requires the integration and reasoning of\ndiverse multimodal data, presenting significant challenges in deriving\nactionable insights. Traditional approaches, including multimodal large\nlanguage models (MLLMs), often struggle to infer complex contextual\nrelationships or deliver comprehensive intelligence from unstructured data\nsources. In this paper, we introduce COSINT-Agent, a knowledge-driven\nmultimodal agent tailored to address the challenges of OSINT in the Chinese\ndomain. COSINT-Agent seamlessly integrates the perceptual capabilities of\nfine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene\nKnowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match\nframework, which bridges COSINT-MLLM and EES-KG, enabling systematic\nextraction, reasoning, and contextualization of multimodal insights. This\nintegration facilitates precise entity recognition, event interpretation, and\ncontext retrieval, effectively transforming raw multimodal data into actionable\nintelligence. Extensive experiments validate the superior performance of\nCOSINT-Agent across core OSINT tasks, including entity recognition, EES\ngeneration, and context matching. These results underscore its potential as a\nrobust and scalable solution for advancing automated multimodal reasoning and\nenhancing the effectiveness of OSINT methodologies.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03190v2",
    "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
    "url": "http://arxiv.org/abs/2503.03190v2",
    "pdf_url": "http://arxiv.org/pdf/2503.03190v2",
    "authors": [
      "Jingzhou Luo",
      "Yang Liu",
      "Weixing Chen",
      "Zhen Li",
      "Yaowei Wang",
      "Guanbin Li",
      "Liang Lin"
    ],
    "date": "2024-03-05",
    "summary": "3D Question Answering (3D QA) requires the model to comprehensively\nunderstand its situated 3D scene described by the text, then reason about its\nsurrounding environment and answer a question under that situation. However,\nexisting methods usually rely on global scene perception from pure 3D point\nclouds and overlook the importance of rich local texture details from\nmulti-view images. Moreover, due to the inherent noise in camera poses and\ncomplex occlusions, there exists significant feature degradation and reduced\nfeature robustness problems when aligning 3D point cloud with multi-view\nimages. In this paper, we propose a Dual-vision Scene Perception Network\n(DSPNet), to comprehensively integrate multi-view and point cloud features to\nimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) module\nprioritizes image views that closely match the semantic content of the text. To\nadaptively fuse back-projected multi-view images with point cloud features, we\ndesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scene\ncomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)\nmodule facilitates robust reasoning by integrating contextual information\nacross visual and linguistic modalities. Experimental results on SQA3D and\nScanQA datasets demonstrate the superiority of our DSPNet. Codes will be\navailable at https://github.com/LZ-CH/DSPNet.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03122v1",
    "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models",
    "url": "http://arxiv.org/abs/2503.03122v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03122v1",
    "authors": [
      "Zichao Li",
      "Xueru Wen",
      "Jie Lou",
      "Yuqiu Ji",
      "Yaojie Lu",
      "Xianpei Han",
      "Debing Zhang",
      "Le Sun"
    ],
    "date": "2024-03-05",
    "summary": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03112v1",
    "title": "A Multimodal Framework for Topic Propagation Classification in Social\n  Networks",
    "url": "http://arxiv.org/abs/2503.03112v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03112v1",
    "authors": [
      "Yuchuan Jiang",
      "Chaolong Jia",
      "Yunyi Qin",
      "Wei Cai",
      "Yongsen Qian"
    ],
    "date": "2024-03-05",
    "summary": "The rapid proliferation of the Internet and the widespread adoption of social\nnetworks have significantly accelerated information dissemination. However,\nthis transformation has introduced complexities in information capture and\nprocessing, posing substantial challenges for researchers and practitioners.\nPredicting the dissemination of topic-related information within social\nnetworks has thus become a critical research focus. This paper proposes a\npredictive model for topic dissemination in social networks by integrating\nmultidimensional features derived from key dissemination characteristics.\nSpecifically, we introduce two novel indicators, user relationship breadth and\nuser authority, into the PageRank algorithm to quantify user influence more\neffectively. Additionally, we employ a Text-CNN model for sentiment\nclassification, extracting sentiment features from textual content. Temporal\nembeddings of nodes are encoded using a Bi-LSTM model to capture temporal\ndynamics. Furthermore, we refine the measurement of user interaction traces\nwith topics, replacing traditional topic view metrics with a more precise\ncommunication characteristics measure. Finally, we integrate the extracted\nmultidimensional features using a Transformer model, significantly enhancing\npredictive performance. Experimental results demonstrate that our proposed\nmodel outperforms traditional machine learning and unimodal deep learning\nmodels in terms of FI-Score, AUC, and Recall, validating its effectiveness in\npredicting topic propagation within social networks.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_03107v1",
    "title": "External Reliable Information-enhanced Multimodal Contrastive Learning\n  for Fake News Detection",
    "url": "http://arxiv.org/abs/2503.03107v1",
    "pdf_url": "http://arxiv.org/pdf/2503.03107v1",
    "authors": [
      "Biwei Cao",
      "Qihang Wu",
      "Jiuxin Cao",
      "Bo Liu",
      "Jie Gui"
    ],
    "date": "2024-03-05",
    "summary": "With the rapid development of the Internet, the information dissemination\nparadigm has changed and the efficiency has been improved greatly. While this\nalso brings the quick spread of fake news and leads to negative impacts on\ncyberspace. Currently, the information presentation formats have evolved\ngradually, with the news formats shifting from texts to multimodal contents. As\na result, detecting multimodal fake news has become one of the research\nhotspots. However, multimodal fake news detection research field still faces\ntwo main challenges: the inability to fully and effectively utilize multimodal\ninformation for detection, and the low credibility or static nature of the\nintroduced external information, which limits dynamic updates. To bridge the\ngaps, we propose ERIC-FND, an external reliable information-enhanced multimodal\ncontrastive learning framework for fake news detection. ERIC-FND strengthens\nthe representation of news contents by entity-enriched external information\nenhancement method. It also enriches the multimodal news information via\nmultimodal semantic interaction method where the multimodal constrative\nlearning is employed to make different modality representations learn from each\nother. Moreover, an adaptive fusion method is taken to integrate the news\nrepresentations from different dimensions for the eventual classification.\nExperiments are done on two commonly used datasets in different languages, X\n(Twitter) and Weibo. Experiment results demonstrate that our proposed model\nERIC-FND outperforms existing state-of-the-art fake news detection methods\nunder the same settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.3,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02950v1",
    "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
    "url": "http://arxiv.org/abs/2503.02950v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02950v1",
    "authors": [
      "Danqing Zhang",
      "Balaji Rama",
      "Jingyi Ni",
      "Shiying He",
      "Fu Zhao",
      "Kunyu Chen",
      "Arnold Chen",
      "Junyu Cao"
    ],
    "date": "2024-03-04",
    "summary": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VLM"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02876v1",
    "title": "SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and\n  Baseline Models",
    "url": "http://arxiv.org/abs/2503.02876v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02876v1",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "date": "2024-03-04",
    "summary": "Advancing AI in computational pathology requires large, high-quality, and\ndiverse datasets, yet existing public datasets are often limited in organ\ndiversity, class coverage, or annotation quality. To bridge this gap, we\nintroduce SPIDER (Supervised Pathology Image-DEscription Repository), the\nlargest publicly available patch-level dataset covering multiple organ types,\nincluding Skin, Colorectal, and Thorax, with comprehensive class coverage for\neach organ. SPIDER provides high-quality annotations verified by expert\npathologists and includes surrounding context patches, which enhance\nclassification performance by providing spatial context.\n  Alongside the dataset, we present baseline models trained on SPIDER using the\nHibou-L foundation model as a feature extractor combined with an\nattention-based classification head. The models achieve state-of-the-art\nperformance across multiple tissue categories and serve as strong benchmarks\nfor future digital pathology research. Beyond patch classification, the model\nenables rapid identification of significant areas, quantitative tissue metrics,\nand establishes a foundation for multimodal approaches.\n  Both the dataset and trained models are publicly available to advance\nresearch, reproducibility, and AI-driven pathology development. Access them at:\nhttps://github.com/HistAI/SPIDER",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02861v1",
    "title": "Evaluation of Architectural Synthesis Using Generative AI",
    "url": "http://arxiv.org/abs/2503.02861v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02861v1",
    "authors": [
      "Jingfei Huang",
      "Alexandros Haridis"
    ],
    "date": "2024-03-04",
    "summary": "Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02853v1",
    "title": "CADDI: An in-Class Activity Detection Dataset using IMU data from\n  low-cost sensors",
    "url": "http://arxiv.org/abs/2503.02853v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02853v1",
    "authors": [
      "Luis Marquez-Carpintero",
      "Sergio Suescun-Ferrandiz",
      "Monica Pina-Navarro",
      "Miguel Cazorla",
      "Francisco Gomez-Donoso"
    ],
    "date": "2024-03-04",
    "summary": "The monitoring and prediction of in-class student activities is of paramount\nimportance for the comprehension of engagement and the enhancement of\npedagogical efficacy. The accurate detection of these activities enables\neducators to modify their lessons in real time, thereby reducing negative\nemotional states and enhancing the overall learning experience. To this end,\nthe use of non-intrusive devices, such as inertial measurement units (IMUs)\nembedded in smartwatches, represents a viable solution. The development of\nreliable predictive systems has been limited by the lack of large, labeled\ndatasets in education. To bridge this gap, we present a novel dataset for\nin-class activity detection using affordable IMU sensors. The dataset comprises\n19 diverse activities, both instantaneous and continuous, performed by 12\nparticipants in typical classroom scenarios. It includes accelerometer,\ngyroscope, rotation vector data, and synchronized stereo images, offering a\ncomprehensive resource for developing multimodal algorithms using sensor and\nvisual data. This dataset represents a key step toward scalable solutions for\nactivity recognition in educational settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02849v1",
    "title": "Multimodal Deep Learning for Subtype Classification in Breast Cancer\n  Using Histopathological Images and Gene Expression Data",
    "url": "http://arxiv.org/abs/2503.02849v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02849v1",
    "authors": [
      "Amin Honarmandi Shandiz"
    ],
    "date": "2024-03-04",
    "summary": "Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02823v1",
    "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
    "url": "http://arxiv.org/abs/2503.02823v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
    "authors": [
      "Matteo Spanio",
      "Massimiliano Zampini",
      "Antonio Rod\u00e0",
      "Franco Pierucci"
    ],
    "date": "2024-03-04",
    "summary": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02800v2",
    "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
    "url": "http://arxiv.org/abs/2503.02800v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02800v2",
    "authors": [
      "Alicia Russell-Gilbert",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Maria Seale",
      "Joseph Jabour",
      "Thomas Arnold",
      "Joshua Church"
    ],
    "date": "2024-03-04",
    "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02798v1",
    "title": "Spike-and-Slab Posterior Sampling in High Dimensions",
    "url": "http://arxiv.org/abs/2503.02798v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02798v1",
    "authors": [
      "Syamantak Kumar",
      "Purnamrita Sarkar",
      "Kevin Tian",
      "Yusong Zhu"
    ],
    "date": "2024-03-04",
    "summary": "Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal\ndistribution used to model uncertainty in variable selection, is considered the\ntheoretical gold standard method for Bayesian sparse linear regression [CPS09,\nRoc18]. However, designing provable algorithms for performing this sampling\ntask is notoriously challenging. Existing posterior samplers for Bayesian\nsparse variable selection tasks either require strong assumptions about the\nsignal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows\nat least linearly in the dimension [MW24], or rely on heuristic approximations\nto the posterior. We give the first provable algorithms for spike-and-slab\nposterior sampling that apply for any SNR, and use a measurement count\nsublinear in the problem dimension. Concretely, assume we are given a\nmeasurement matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ and noisy\nobservations $\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\star + \\mathbf{\\xi}$ of a\nsignal $\\mathbf{\\theta}^\\star$ drawn from a spike-and-slab prior $\\pi$ with a\nGaussian diffuse density and expected sparsity k, where $\\mathbf{\\xi} \\sim\n\\mathcal{N}(\\mathbb{0}_n, \\sigma^2\\mathbf{I}_n)$. We give a polynomial-time\nhigh-accuracy sampler for the posterior $\\pi(\\cdot \\mid \\mathbf{X},\n\\mathbf{y})$, for any SNR $\\sigma^{-1}$ > 0, as long as $n \\geq k^3 \\cdot\n\\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the\nrestricted isometry property. We further give a sampler that runs in\nnear-linear time $\\approx nd$ in the same setting, as long as $n \\geq k^5 \\cdot\n\\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend\nour result to spike-and-slab posterior sampling with Laplace diffuse densities,\nachieving similar guarantees when $\\sigma = O(\\frac{1}{k})$ is bounded.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02781v1",
    "title": "Multimodal AI predicts clinical outcomes of drug combinations from\n  preclinical data",
    "url": "http://arxiv.org/abs/2503.02781v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02781v1",
    "authors": [
      "Yepeng Huang",
      "Xiaorui Su",
      "Varun Ullanat",
      "Ivy Liang",
      "Lindsay Clegg",
      "Damilola Olabode",
      "Nicholas Ho",
      "Bino John",
      "Megan Gibbs",
      "Marinka Zitnik"
    ],
    "date": "2024-03-04",
    "summary": "Predicting clinical outcomes from preclinical data is essential for\nidentifying safe and effective drug combinations. Current models rely on\nstructural or target-based features to identify high-efficacy, low-toxicity\ndrug combinations. However, these approaches fail to incorporate the multimodal\ndata necessary for accurate, clinically-relevant predictions. Here, we\nintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,\ncell viability, and transcriptomic data to predict drug combination effects\nacross 953 clinical outcomes and 21842 compounds, including combinations of\napproved drugs and novel compounds in development. MADRIGAL uses a transformer\nbottleneck module to unify preclinical drug data modalities while handling\nmissing data during training and inference--a major challenge in multimodal\nlearning. It outperforms single-modality methods and state-of-the-art models in\npredicting adverse drug interactions. MADRIGAL performs virtual screening of\nanticancer drug combinations and supports polypharmacy management for type II\ndiabetes and metabolic dysfunction-associated steatohepatitis (MASH). It\nidentifies transporter-mediated drug interactions. MADRIGAL predicts\nresmetirom, the first and only FDA-approved drug for MASH, among therapies with\nthe most favorable safety profile. It supports personalized cancer therapy by\nintegrating genomic profiles from cancer patients. Using primary acute myeloid\nleukemia samples and patient-derived xenograft models, it predicts the efficacy\nof personalized drug combinations. Integrating MADRIGAL with a large language\nmodel allows users to describe clinical outcomes in natural language, improving\nsafety assessment by identifying potential adverse interactions and toxicity\nrisks. MADRIGAL provides a multimodal approach for designing combination\ntherapies with improved predictive accuracy and clinical relevance.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02733v1",
    "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural\n  Video Compression",
    "url": "http://arxiv.org/abs/2503.02733v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02733v1",
    "authors": [
      "Jia Wang",
      "Xinfeng Zhang",
      "Gai Zhang",
      "Jun Zhu",
      "Lv Tang",
      "Li Zhang"
    ],
    "date": "2024-03-04",
    "summary": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "CLIP"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02616v1",
    "title": "Smoothing the Shift: Towards Stable Test-Time Adaptation under Complex\n  Multimodal Noises",
    "url": "http://arxiv.org/abs/2503.02616v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02616v1",
    "authors": [
      "Zirun Guo",
      "Tao Jin"
    ],
    "date": "2024-03-04",
    "summary": "Test-Time Adaptation (TTA) aims to tackle distribution shifts using unlabeled\ntest data without access to the source data. In the context of multimodal data,\nthere are more complex noise patterns than unimodal data such as simultaneous\ncorruptions for multiple modalities and missing modalities. Besides, in\nreal-world applications, corruptions from different distribution shifts are\nalways mixed. Existing TTA methods always fail in such multimodal scenario\nbecause the abrupt distribution shifts will destroy the prior knowledge from\nthe source model, thus leading to performance degradation. To this end, we\nreveal a new challenge named multimodal wild TTA. To address this challenging\nproblem, we propose two novel strategies: sample identification with\ninterquartile range Smoothing and unimodal assistance, and Mutual information\nsharing (SuMi). SuMi smooths the adaptation process by interquartile range\nwhich avoids the abrupt distribution shifts. Then, SuMi fully utilizes the\nunimodal features to select low-entropy samples with rich multimodal\ninformation for optimization. Furthermore, mutual information sharing is\nintroduced to align the information, reduce the discrepancies and enhance the\ninformation utilization across different modalities. Extensive experiments on\ntwo public datasets show the effectiveness and superiority over existing\nmethods under the complex noise patterns in multimodal data. Code is available\nat https://github.com/zrguo/SuMi.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02600v1",
    "title": "Resource-Efficient Affordance Grounding with Complementary Depth and\n  Semantic Prompts",
    "url": "http://arxiv.org/abs/2503.02600v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02600v1",
    "authors": [
      "Yizhou Huang",
      "Fan Yang",
      "Guoliang Zhu",
      "Gen Li",
      "Hao Shi",
      "Yukun Zuo",
      "Wenrui Chen",
      "Zhiyong Li",
      "Kailun Yang"
    ],
    "date": "2024-03-04",
    "summary": "Affordance refers to the functional properties that an agent perceives and\nutilizes from its environment, and is key perceptual information required for\nrobots to perform actions. This information is rich and multimodal in nature.\nExisting multimodal affordance methods face limitations in extracting useful\ninformation, mainly due to simple structural designs, basic fusion methods, and\nlarge model parameters, making it difficult to meet the performance\nrequirements for practical deployment. To address these issues, this paper\nproposes the BiT-Align image-depth-text affordance mapping framework. The\nframework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance\n(TFG) attention selection mechanism. BPM integrates the auxiliary modality\ndepth image directly as a prompt to the primary modality RGB image, embedding\nit into the primary modality encoder without introducing additional encoders.\nThis reduces the model's parameter count and effectively improves functional\nregion localization accuracy. The TFG mechanism guides the selection and\nenhancement of attention heads in the image encoder using textual features,\nimproving the understanding of affordance characteristics. Experimental results\ndemonstrate that the proposed method achieves significant performance\nimprovements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,\ncompared with the current state-of-the-art method, we achieve a 6.0%\nimprovement in the KLD metric, while reducing model parameters by 88.8%,\ndemonstrating practical application values. The source code will be made\npublicly available at https://github.com/DAWDSE/BiT-Align.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02589v2",
    "title": "MCiteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs",
    "url": "http://arxiv.org/abs/2503.02589v2",
    "pdf_url": "http://arxiv.org/pdf/2503.02589v2",
    "authors": [
      "Caiyu Hu",
      "Yikai Zhang",
      "Tinghui Zhu",
      "Yiwei Ye",
      "Yanghua Xiao"
    ],
    "date": "2024-03-04",
    "summary": "Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, overlooking the challenges and opportunities\nof multimodal contexts. To address this gap, we introduce MCiteBench, the first\nbenchmark designed to evaluate and analyze the multimodal citation text\ngeneration ability of MLLMs. Our benchmark comprises data derived from academic\npapers and review-rebuttal interactions, featuring diverse information sources\nand multimodal content. We comprehensively evaluate models from multiple\ndimensions, including citation quality, source reliability, and answer\naccuracy. Through extensive experiments, we observe that MLLMs struggle with\nmultimodal citation text generation. We also conduct deep analyses of models'\nperformance, revealing that the bottleneck lies in attributing the correct\nsources rather than understanding the multimodal content.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02917v1",
    "title": "Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided\n  Prompting of Vision-Language Models",
    "url": "http://arxiv.org/abs/2503.02917v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02917v1",
    "authors": [
      "Deval Mehta",
      "Yiwen Jiang",
      "Catherine L Jan",
      "Mingguang He",
      "Kshitij Jadhav",
      "Zongyuan Ge"
    ],
    "date": "2024-03-04",
    "summary": "Recent advancements in deep learning have shown significant potential for\nclassifying retinal diseases using color fundus images. However, existing works\npredominantly rely exclusively on image data, lack interpretability in their\ndiagnostic decisions, and treat medical professionals primarily as annotators\nfor ground truth labeling. To fill this gap, we implement two key strategies:\nextracting interpretable concepts of retinal diseases using the knowledge base\nof GPT models and incorporating these concepts as a language component in\nprompt-learning to train vision-language (VL) models with both fundus images\nand their associated concepts. Our method not only improves retinal disease\nclassification but also enriches few-shot and zero-shot detection (novel\ndisease detection), while offering the added benefit of concept-based model\ninterpretability. Our extensive evaluation across two diverse retinal fundus\nimage datasets illustrates substantial performance gains in VL-model based\nfew-shot methodologies through our concept integration approach, demonstrating\nan average improvement of approximately 5.8\\% and 2.7\\% mean average precision\nfor 16-shot learning and zero-shot (novel class) detection respectively. Our\nmethod marks a pivotal step towards interpretable and efficient retinal disease\nrecognition for real-world clinical applications.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision-language"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02511v1",
    "title": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place\n  Recognition",
    "url": "http://arxiv.org/abs/2503.02511v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02511v1",
    "authors": [
      "Oliver Grainge",
      "Michael Milford",
      "Indu Bodala",
      "Sarvapali D. Ramchurn",
      "Shoaib Ehsan"
    ],
    "date": "2024-03-04",
    "summary": "Visual Place Recognition (VPR) localizes a query image by matching it against\na database of geo-tagged reference images, making it essential for navigation\nand mapping in robotics. Although Vision Transformer (ViT) solutions deliver\nhigh accuracy, their large models often exceed the memory and compute budgets\nof resource-constrained platforms such as drones and mobile robots. To address\nthis issue, we propose TeTRA, a ternary transformer approach that progressively\nquantizes the ViT backbone to 2-bit precision and binarizes its final embedding\nlayer, offering substantial reductions in model size and latency. A carefully\ndesigned progressive distillation strategy preserves the representational power\nof a full-precision teacher, allowing TeTRA to retain or even surpass the\naccuracy of uncompressed convolutional counterparts, despite using fewer\nresources. Experiments on standard VPR benchmarks demonstrate that TeTRA\nreduces memory consumption by up to 69% compared to efficient baselines, while\nlowering inference latency by 35%, with either no loss or a slight improvement\nin recall@1. These gains enable high-accuracy VPR on power-constrained,\nmemory-limited robotic platforms, making TeTRA an appealing solution for\nreal-world deployment.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02459v1",
    "title": "Exploring Token-Level Augmentation in Vision Transformer for\n  Semi-Supervised Semantic Segmentation",
    "url": "http://arxiv.org/abs/2503.02459v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02459v1",
    "authors": [
      "Dengke Zhang",
      "Quan Tang",
      "Fagui Liu",
      "C. L. Philip Chen",
      "Haiqing Mei"
    ],
    "date": "2024-03-04",
    "summary": "Semi-supervised semantic segmentation has witnessed remarkable advancements\nin recent years. However, existing algorithms are based on convolutional neural\nnetworks and directly applying them to Vision Transformers poses certain\nlimitations due to conceptual disparities. To this end, we propose TokenMix, a\ndata augmentation technique specifically designed for semi-supervised semantic\nsegmentation with Vision Transformers. TokenMix aligns well with the global\nattention mechanism by mixing images at the token level, enhancing learning\ncapability for contexutual information among image patches. We further\nincorporate image augmentation and feature augmentation to promote the\ndiversity of augmentation. Moreover, to enhance consistency regularization, we\npropose a dual-branch framework where each branch applies both image\naugmentation and feature augmentation to the input image. We conduct extensive\nexperiments across multiple benchmark datasets, including Pascal VOC 2012,\nCityscapes, and COCO. Results suggest that the proposed method outperforms\nstate-of-the-art algorithms with notably observed accuracy improvement,\nespecially under the circumstance of limited fine annotations.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02420v1",
    "title": "Exploring Model Quantization in GenAI-based Image Inpainting and\n  Detection of Arable Plants",
    "url": "http://arxiv.org/abs/2503.02420v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02420v1",
    "authors": [
      "Sourav Modak",
      "Ahmet O\u011fuz Salt\u0131k",
      "Anthony Stein"
    ],
    "date": "2024-03-04",
    "summary": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "Stable Diffusion"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02394v3",
    "title": "BHViT: Binarized Hybrid Vision Transformer",
    "url": "http://arxiv.org/abs/2503.02394v3",
    "pdf_url": "http://arxiv.org/pdf/2503.02394v3",
    "authors": [
      "Tian Gao",
      "Zhiyuan Zhang",
      "Yu Zhang",
      "Huajun Liu",
      "Kaijie Yin",
      "Chengzhong Xu",
      "Hui Kong"
    ],
    "date": "2024-03-04",
    "summary": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "vision transformer"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02379v1",
    "title": "Teaching Metric Distance to Autoregressive Multimodal Foundational\n  Models",
    "url": "http://arxiv.org/abs/2503.02379v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02379v1",
    "authors": [
      "Jiwan Chung",
      "Saejin Kim",
      "Yongrae Jo",
      "Jaewoo Park",
      "Dongjun Min",
      "Youngjae Yu"
    ],
    "date": "2024-03-04",
    "summary": "As large language models expand beyond natural language to domains such as\nmathematics, multimodal understanding, and embodied agents, tokens increasingly\nreflect metric relationships rather than purely linguistic meaning. We\nintroduce DIST2Loss, a distance-aware framework designed to train\nautoregressive discrete models by leveraging predefined distance relationships\namong output tokens. At its core, DIST2Loss transforms continuous exponential\nfamily distributions derived from inherent distance metrics into discrete,\ncategorical optimization targets compatible with the models' architectures.\nThis approach enables the models to learn and preserve meaningful distance\nrelationships during token generation while maintaining compatibility with\nexisting architectures. Empirical evaluations show consistent performance gains\nin diverse multimodal applications, including visual grounding, robotic\nmanipulation, generative reward modeling, and image generation using\nvector-quantized features. These improvements are pronounced in cases of\nlimited training data, highlighting DIST2Loss's effectiveness in\nresource-constrained settings.",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "multimodal"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  },
  {
    "id": "2503_02330v1",
    "title": "Exploring Simple Siamese Network for High-Resolution Video Quality\n  Assessment",
    "url": "http://arxiv.org/abs/2503.02330v1",
    "pdf_url": "http://arxiv.org/pdf/2503.02330v1",
    "authors": [
      "Guotao Shen",
      "Ziheng Yan",
      "Xin Jin",
      "Longhai Wu",
      "Jie Chen",
      "Ilhyun Cho",
      "Cheul-Hee Hahm"
    ],
    "date": "2024-03-04",
    "summary": "In the research of video quality assessment (VQA), two-branch network has\nemerged as a promising solution. It decouples VQA with separate technical and\naesthetic branches to measure the perception of low-level distortions and\nhigh-level semantics respectively. However, we argue that while technical and\naesthetic perspectives are complementary, the technical perspective itself\nshould be measured in semantic-aware manner. We hypothesize that existing\ntechnical branch struggles to perceive the semantics of high-resolution videos,\nas it is trained on local mini-patches sampled from videos. This issue can be\nhidden by apparently good results on low-resolution videos, but indeed becomes\ncritical for high-resolution VQA. This work introduces SiamVQA, a simple but\neffective Siamese network for highre-solution VQA. SiamVQA shares weights\nbetween technical and aesthetic branches, enhancing the semantic perception\nability of technical branch to facilitate technical-quality representation\nlearning. Furthermore, it integrates a dual cross-attention layer for fusing\ntechnical and aesthetic features. SiamVQA achieves state-of-the-art accuracy on\nhigh-resolution benchmarks, and competitive results on lower-resolution\nbenchmarks. Codes will be available at: https://github.com/srcn-ivl/SiamVQA",
    "source": "arXiv",
    "categories": [],
    "keywords": [
      "VQA"
    ],
    "attention_score": 0.12,
    "attention_components": {
      "base_score": 1.2,
      "recency_factor": 0.1,
      "source_weight": 1.0,
      "age_months": 12.4,
      "citation_velocity": 0
    }
  }
]