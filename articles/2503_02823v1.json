{
  "id": "2503_02823v1",
  "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
  "url": "http://arxiv.org/abs/2503.02823v1",
  "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
  "authors": [
    "Matteo Spanio",
    "Massimiliano Zampini",
    "Antonio Rod\u00e0",
    "Franco Pierucci"
  ],
  "date": "2024-03-04",
  "summary": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal"
  ],
  "attention_score": 0.12,
  "attention_components": {
    "base_score": 1.2,
    "recency_factor": 0.1,
    "source_weight": 1.0,
    "age_months": 12.4,
    "citation_velocity": 0
  }
}