{
  "id": "2403.04473v2",
  "title": "TextMonkey: An OCR-Free Large Multimodal Model for Understanding\n  Document",
  "url": "http://arxiv.org/abs/2403.04473v2",
  "pdf_url": "http://arxiv.org/pdf/2403.04473v2",
  "authors": [
    "Yuliang Liu",
    "Biao Yang",
    "Qiang Liu",
    "Zhang Li",
    "Zhiyin Ma",
    "Shuo Zhang",
    "Xiang Bai"
  ],
  "date": "2024-03-07",
  "summary": "We present TextMonkey, a large multimodal model (LMM) tailored for\ntext-centric tasks. Our approach introduces enhancement across several\ndimensions: By adopting Shifted Window Attention with zero-initialization, we\nachieve cross-window connectivity at higher input resolutions and stabilize\nearly training; We hypothesize that images may contain redundant tokens, and by\nusing similarity to filter out significant tokens, we can not only streamline\nthe token length but also enhance the model's performance. Moreover, by\nexpanding our model's capabilities to encompass text spotting and grounding,\nand incorporating positional information into responses, we enhance\ninterpretability. It also learns to perform screenshot tasks through\nfinetuning. Evaluation on 12 benchmarks shows notable improvements: 5.2% in\nScene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in\nDocument-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister\nCharity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks\n(comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with\na 10.9\\% increase and sets a new standard on OCRBench, a comprehensive\nbenchmark consisting of 29 OCR-related assessments, with a score of 561,\nsurpassing previous open-sourced large multimodal models for document\nunderstanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal",
    "VQA",
    "ViT"
  ],
  "attention_score": 0.16,
  "attention_components": {
    "base_score": 1.6,
    "recency_factor": 0.1,
    "source_weight": 1.0,
    "age_months": 12.3,
    "citation_velocity": 0
  }
}