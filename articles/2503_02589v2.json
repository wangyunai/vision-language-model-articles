{
  "id": "2503_02589v2",
  "title": "MCiteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs",
  "url": "http://arxiv.org/abs/2503.02589v2",
  "pdf_url": "http://arxiv.org/pdf/2503.02589v2",
  "authors": [
    "Caiyu Hu",
    "Yikai Zhang",
    "Tinghui Zhu",
    "Yiwei Ye",
    "Yanghua Xiao"
  ],
  "date": "2025-03-04",
  "summary": "Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, overlooking the challenges and opportunities\nof multimodal contexts. To address this gap, we introduce MCiteBench, the first\nbenchmark designed to evaluate and analyze the multimodal citation text\ngeneration ability of MLLMs. Our benchmark comprises data derived from academic\npapers and review-rebuttal interactions, featuring diverse information sources\nand multimodal content. We comprehensively evaluate models from multiple\ndimensions, including citation quality, source reliability, and answer\naccuracy. Through extensive experiments, we observe that MLLMs struggle with\nmultimodal citation text generation. We also conduct deep analyses of models'\nperformance, revealing that the bottleneck lies in attributing the correct\nsources rather than understanding the multimodal content.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal"
  ]
}