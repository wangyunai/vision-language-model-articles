{
  "id": "2403.04652v3",
  "title": "Yi: Open Foundation Models by 01.AI",
  "url": "http://arxiv.org/abs/2403.04652v3",
  "pdf_url": "http://arxiv.org/pdf/2403.04652v3",
  "authors": [
    "01. AI",
    ":",
    "Alex Young",
    "Bei Chen",
    "Chao Li",
    "Chengen Huang",
    "Ge Zhang",
    "Guanwei Zhang",
    "Guoyin Wang",
    "Heng Li",
    "Jiangcheng Zhu",
    "Jianqun Chen",
    "Jing Chang",
    "Kaidong Yu",
    "Peng Liu",
    "Qiang Liu",
    "Shawn Yue",
    "Senbin Yang",
    "Shiming Yang",
    "Wen Xie",
    "Wenhao Huang",
    "Xiaohui Hu",
    "Xiaoyi Ren",
    "Xinyao Niu",
    "Pengcheng Nie",
    "Yanpeng Li",
    "Yuchi Xu",
    "Yudong Liu",
    "Yue Wang",
    "Yuxuan Cai",
    "Zhenyu Gu",
    "Zhiyuan Liu",
    "Zonghong Dai"
  ],
  "date": "2024-03-07",
  "summary": "We introduce the Yi model family, a series of language and multimodal models\nthat demonstrate strong multi-dimensional capabilities. The Yi model family is\nbased on 6B and 34B pretrained language models, then we extend them to chat\nmodels, 200K long context models, depth-upscaled models, and vision-language\nmodels. Our base models achieve strong performance on a wide range of\nbenchmarks like MMLU, and our finetuned chat models deliver strong human\npreference rate on major evaluation platforms like AlpacaEval and Chatbot\nArena. Building upon our scalable super-computing infrastructure and the\nclassical transformer architecture, we attribute the performance of Yi models\nprimarily to its data quality resulting from our data-engineering efforts. For\npretraining, we construct 3.1 trillion tokens of English and Chinese corpora\nusing a cascaded data deduplication and quality filtering pipeline. For\nfinetuning, we polish a small scale (less than 10K) instruction dataset over\nmultiple iterations such that every single instance has been verified directly\nby our machine learning engineers. For vision-language, we combine the chat\nlanguage model with a vision transformer encoder and train the model to align\nvisual representations to the semantic space of the language model. We further\nextend the context length to 200K through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. We show that\nextending the depth of the pretrained checkpoint through continual pretraining\nfurther improves performance. We believe that given our current results,\ncontinuing to scale up model parameters using thoroughly optimized data will\nlead to even stronger frontier models.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "multimodal",
    "vision-language",
    "vision transformer"
  ],
  "attention_score": 0.16,
  "attention_components": {
    "base_score": 1.6,
    "recency_factor": 0.1,
    "source_weight": 1.0,
    "age_months": 12.3,
    "citation_velocity": 0
  }
}