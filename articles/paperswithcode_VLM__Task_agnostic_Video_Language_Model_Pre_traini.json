{
  "id": "paperswithcode_VLM__Task_agnostic_Video_Language_Model_Pre_traini",
  "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
  "url": "https://paperswithcode.com/paper/vlm-task-agnostic-video-language-model-pre",
  "authors": [],
  "date": "2024-03-15",
  "summary": "We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks.",
  "source": "Papers With Code",
  "keywords": [
    "VLM"
  ],
  "code_url": null,
  "attention_score": 0.14,
  "attention_components": {
    "base_score": 1.2,
    "recency_factor": 0.1,
    "source_weight": 1.2,
    "age_months": 12.0,
    "citation_velocity": 0
  }
}