{
  "id": "2403.06098v4",
  "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video\n  Diffusion Models",
  "url": "http://arxiv.org/abs/2403.06098v4",
  "pdf_url": "http://arxiv.org/pdf/2403.06098v4",
  "authors": [
    "Wenhao Wang",
    "Yi Yang"
  ],
  "date": "2024-03-10",
  "summary": "The arrival of Sora marks a new era for text-to-video diffusion models,\nbringing significant advancements in video generation and potential\napplications. However, Sora, along with other text-to-video diffusion models,\nis highly reliant on prompts, and there is no publicly available dataset that\nfeatures a study of text-to-video prompts. In this paper, we introduce VidProM,\nthe first large-scale dataset comprising 1.67 Million unique text-to-Video\nPrompts from real users. Additionally, this dataset includes 6.69 million\nvideos generated by four state-of-the-art diffusion models, alongside some\nrelated data. We initially discuss the curation of this large-scale dataset, a\nprocess that is both time-consuming and costly. Subsequently, we underscore the\nneed for a new prompt dataset specifically designed for text-to-video\ngeneration by illustrating how VidProM differs from DiffusionDB, a large-scale\nprompt-gallery dataset for image generation. Our extensive and diverse dataset\nalso opens up many exciting new research areas. For instance, we suggest\nexploring text-to-video prompt engineering, efficient video generation, and\nvideo copy detection for diffusion models to develop better, more efficient,\nand safer models. The project (including the collected dataset VidProM and\nrelated code) is publicly available at https://vidprom.github.io under the\nCC-BY-NC 4.0 License.",
  "source": "arXiv",
  "categories": [],
  "keywords": [
    "text-to-video",
    "Sora",
    "image generation"
  ],
  "attention_score": 0.16,
  "attention_components": {
    "base_score": 1.6,
    "recency_factor": 0.1,
    "source_weight": 1.0,
    "age_months": 12.2,
    "citation_velocity": 0
  }
}